{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:40px;\"><center>Exercise V:<br> GANs\n",
    "</center></h1>\n",
    "\n",
    "## Short summary\n",
    "In this exercise, we will design a generative network to generate the last rgb image given the first image. These folder has **three files**: \n",
    "- **configGAN.py:** this involves definitions of all parameters and data paths\n",
    "- **utilsGAN.py:** includes utility functions required to grab and visualize data \n",
    "- **runGAN.ipynb:** contains the script to design, train and test the network \n",
    "\n",
    "Make sure that before running this script, you created an environment and **installed all required libraries** such \n",
    "as keras.\n",
    "\n",
    "## The data\n",
    "There exists also a subfolder called **data** which contains the traning, validation, and testing data each has both RGB input images together with the corresponding ground truth images.\n",
    "\n",
    "\n",
    "## The exercises\n",
    "As for the previous lab all exercises are found below.\n",
    "\n",
    "\n",
    "## The different 'Cells'\n",
    "This notebook contains several cells with python code, together with the markdown cells (like this one) with only text. Each of the cells with python code has a \"header\" markdown cell with information about the code. The table below provides a short overview of the code cells. \n",
    "\n",
    "| #  |  CellName | CellType | Comment |\n",
    "| :--- | :-------- | :-------- | :------- |\n",
    "| 1 | Init | Needed | Sets up the environment|\n",
    "| 2 | Ex | Exercise 1| A class definition of a network model  |\n",
    "| 3 | Loading | Needed | Loading parameters and initializing the model |\n",
    "| 4 | Stats | Needed | Show data distribution | \n",
    "| 5 | Data | Needed | Generating the data batches |\n",
    "| 6 | Debug | Needed | Debugging the data |\n",
    "| 7 | Device | Needed | Selecting CPU/GPU |\n",
    "| 8 | Init | Needed | Sets up the timer and other neccessary components |\n",
    "| 9 | Training | Exercise 1-2 | Training the model   |\n",
    "| 10 | Testing | Exercise 1-2| Testing the  method   |  \n",
    "\n",
    "\n",
    "In order for you to start with the exercise you need to run all cells. It is important that you do this in the correct order, starting from the top and continuing with the next cells. Later when you have started to work with the notebook it may be easier to use the command \"Run All\" found in the \"Cell\" dropdown menu.\n",
    "\n",
    "## Writing the report\n",
    "\n",
    "There is no need to provide any report. However, implemented network architecuture and observed experimental results must be presented as a short presentation in the last lecture, May 28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) We first start with importing all required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating network model using gpu 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from configGAN import *\n",
    "cfg = flying_objects_config()\n",
    "if cfg.GPU >=0:\n",
    "    print(\"creating network model using gpu \" + str(cfg.GPU))\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(cfg.GPU)\n",
    "elif cfg.GPU >=-1:\n",
    "    print(\"creating network model using cpu \")  \n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from utilsGAN import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "from datetime import datetime\n",
    "import imageio\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "import pprint\n",
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv3D, Conv2D, Conv1D, Convolution2D, Deconvolution2D, Cropping2D, UpSampling2D\n",
    "from keras.layers import Input, Conv2DTranspose, ConvLSTM2D, TimeDistributed\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import Concatenate, concatenate, Reshape\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from keras.layers import Input, merge\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout, LeakyReLU\n",
    "import keras.backend as kb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Here, we have the network model class definition. In this class, the most important functions are **build_generator()** and **build_discriminator()**. As defined in the exercises section, your task is to update the both network architectures defined in these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANModel():\n",
    "    def __init__(self, batch_size=32, inputShape=(64, 64, 3), dropout_prob=0.25): \n",
    "        self.batch_size = batch_size\n",
    "        self.inputShape = inputShape\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # Calculate the shape of patches\n",
    "        patch = int(self.inputShape[0] / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "  \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mse', optimizer=Adam(0.0002, 0.5),metrics=['accuracy'])\n",
    " \n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # Input images and their conditioning images\n",
    "        first_frame = Input(shape=self.inputShape)\n",
    "        last_frame = Input(shape=self.inputShape)\n",
    "\n",
    "        # By conditioning on the first frame generate a fake version of the last frame\n",
    "        fake_last_frame = self.generator(first_frame)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        # Discriminators determines validity of fake and condition first image pairs\n",
    "        valid = self.discriminator([fake_last_frame, first_frame])\n",
    "\n",
    "        self.combined = Model(inputs=[last_frame, first_frame], outputs=[valid, fake_last_frame])\n",
    "        self.combined.compile(loss=['mse', 'mae'], # mean squared and mean absolute errors\n",
    "                              loss_weights=[1, 100],\n",
    "                              optimizer=Adam(0.0002, 0.5))\n",
    "        \n",
    "    def conv2d_block(self, input_tensor, n_filters, kernel_size=3, batchnorm=True, strides=1, moment=None):\n",
    "        # first layer\n",
    "        x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "                   padding=\"same\")(input_tensor)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization(momentum=moment)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        # second layer\n",
    "        x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "                   padding=\"same\")(x)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization(momentum=moment)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x_pooled = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "        \n",
    "        return x, x_pooled\n",
    "\n",
    "    def upSampling2d_block(self, input_tensor, input_tensor_pooled, n_filters, kernel_size=3, batchnorm=True, moment=None):\n",
    "        upSampling = UpSampling2D((2, 2))(input_tensor)\n",
    "        concat = concatenate([upSampling, input_tensor_pooled], axis=3)\n",
    "        up = Conv2D(n_filters, (kernel_size, kernel_size), padding='same')(concat)\n",
    "        if batchnorm:\n",
    "            up = BatchNormalization(momentum=moment)(up)\n",
    "        up = Activation('relu')(up)\n",
    "        up = Conv2D(n_filters, (kernel_size, kernel_size), padding='same')(up)\n",
    "        if batchnorm:\n",
    "            up = BatchNormalization(momentum=moment)(up)\n",
    "        up = Activation('relu')(up)\n",
    "        \n",
    "        return up\n",
    "\n",
    "    def build_generator(self):\n",
    "        inputs = Input(shape=self.inputShape)\n",
    "        \n",
    "        batch_norm = True\n",
    "        \n",
    "        x1, x_pooled1 = self.conv2d_block(inputs, 16, batchnorm=batch_norm, moment=0.8)\n",
    "        x2, x_pooled2 = self.conv2d_block(x_pooled1, 32, batchnorm=batch_norm, moment=0.8)\n",
    "        \n",
    "        mid = Conv2D(64, (3, 3), padding='same')(x_pooled2)\n",
    "        #if batch_norm:\n",
    "        #    mid = BatchNormalization()(mid)\n",
    "        mid = Activation('relu')(mid)\n",
    "        \n",
    "        up1 = self.upSampling2d_block(mid, x2, 32, batchnorm=batch_norm, moment=0.8)\n",
    "        up2 = self.upSampling2d_block(up1, x1, 16, batchnorm=batch_norm, moment=0.8)\n",
    "        \n",
    "        \n",
    "#         conv1= Conv2D(filters=32,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(inputs)\n",
    "#         conv1 = BatchNormalization(momentum=0.8)(conv1)\n",
    "#         conv1= Conv2D(filters=32,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv1)\n",
    "#         conv1 = BatchNormalization(momentum=0.8)(conv1)\n",
    "#         pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        \n",
    "#         conv2= Conv2D(filters=64,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(pool1)\n",
    "#         conv2 = BatchNormalization(momentum=0.8)(conv2)\n",
    "#         conv2= Conv2D(filters=64,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv2)\n",
    "#         conv2 = BatchNormalization(momentum=0.8)(conv2)\n",
    "#         pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        \n",
    "#         conv3= Conv2D(filters=128,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(pool2)\n",
    "#         conv3 = BatchNormalization(momentum=0.8)(conv3)\n",
    "#         conv3= Conv2D(filters=128,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv3)\n",
    "#         conv3 = BatchNormalization(momentum=0.8)(conv3)\n",
    "#         pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "        \n",
    "#         conv4= Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(pool3)\n",
    "#         conv4 = BatchNormalization(momentum=0.8)(conv4)\n",
    "#         conv4= Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv4)\n",
    "#         conv4 = BatchNormalization(momentum=0.8)(conv4)\n",
    "#         drop4 = Dropout(0.5)(conv4)\n",
    "        '''pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "        \n",
    "        conv5= Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(pool4)\n",
    "        conv5= Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv5)\n",
    "        #pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        drop5 = Dropout(0.5)(conv5)\n",
    "        \n",
    "        \n",
    "        up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "        up6 = Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(up6)\n",
    "        up6 = Concatenate(axis=3)([drop4, up6])\n",
    "        conv6 = Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(up6)\n",
    "        conv6 = Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv6)'''\n",
    "\n",
    "#         up7 = UpSampling2D(size=(2, 2))(conv4)\n",
    "#         up7 = Conv2D(filters=256,kernel_size=2,activation='relu',padding='same',kernel_initializer='he_normal')(up7)\n",
    "#         up7 = BatchNormalization(momentum=0.8)(up7)\n",
    "#         up7 = Concatenate(axis=3)([conv3, up7])\n",
    "#         conv7 = Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(up7)\n",
    "#         conv7 = BatchNormalization(momentum=0.8)(conv7)\n",
    "#         conv7 = Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv7)\n",
    "#         conv7 = BatchNormalization(momentum=0.8)(conv7)\n",
    "\n",
    "#         up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "#         up8 = Conv2D(filters=128,kernel_size=2,activation='relu',padding='same',kernel_initializer='he_normal')(up8)\n",
    "#         up8 = BatchNormalization(momentum=0.8)(up8)\n",
    "#         up8 = Concatenate(axis=3)([conv2, up8])\n",
    "#         conv8 = Conv2D(filters=128,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(up8)\n",
    "#         conv8 = BatchNormalization(momentum=0.8)(conv8)\n",
    "#         conv8 = Conv2D(filters=128,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "#         up9 = UpSampling2D(size=(2, 2))(conv8)\n",
    "#         up9 = Conv2D(filters=64,kernel_size=2,activation='relu',padding='same',kernel_initializer='he_normal')(up9)\n",
    "#         up9 = BatchNormalization(momentum=0.8)(up9)\n",
    "#         up9 = Concatenate(axis=3)([conv1, up9])\n",
    "#         conv9 = Conv2D(filters=64,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(up9)\n",
    "#         conv9 = BatchNormalization(momentum=0.8)(conv9)\n",
    "#         conv9 = Conv2D(filters=64,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv9)\n",
    "#         conv9 = BatchNormalization(momentum=0.8)(conv9)\n",
    "#         conv9 = Conv2D(filters=32,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv9)\n",
    "#         conv9 = BatchNormalization(momentum=0.8)(conv9)\n",
    "        \n",
    "        nbr_img_channels = self.inputShape[2]\n",
    "\n",
    "        outputs = Conv2D(nbr_img_channels, (1, 1), activation='sigmoid')(up2)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs, name='Generator')\n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self):\n",
    "  \n",
    "        last_img = Input(shape=self.inputShape)\n",
    "        first_img = Input(shape=self.inputShape)\n",
    "\n",
    "        # Concatenate image and conditioning image by channels to produce input\n",
    "        combined_imgs = Concatenate(axis=-1)([last_img, first_img])\n",
    "  \n",
    "        d1 = Conv2D(32, (3, 3), strides=2, padding='same')(combined_imgs) \n",
    "        d1 = Activation('relu')(d1) \n",
    "        d2 = Conv2D(64, (3, 3), strides=2, padding='same')(d1)\n",
    "        d2 = Activation('relu')(d2) \n",
    "        d3 = Conv2D(128, (3, 3), strides=2, padding='same')(d2)\n",
    "        d3 = Activation('relu')(d3) \n",
    "         \n",
    "        validity = Conv2D(1, (3, 3), strides=2, padding='same')(d3)\n",
    "\n",
    "        model = Model([last_img, first_img], validity)\n",
    "        model.summary()\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) We import the network **hyperparameters** and build a simple network by calling the class introduced in the previous step. Please note that to change the hyperparameters, you just need to change the values in the file called **configPredictor.py.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 64, 64, 6)    0           input_10[0][0]                   \n",
      "                                                                 input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 32)   1760        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 32, 32, 32)   0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 64)   18496       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 64)   0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 128)    73856       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 128)    0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 4, 4, 1)      1153        activation_20[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 95,265\n",
      "Trainable params: 95,265\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"Generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 64, 64, 16)   448         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 64, 64, 16)   64          conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 64, 64, 16)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 64, 64, 16)   2320        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 64, 64, 16)   64          conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 64, 64, 16)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 32)   128         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 32, 32, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 32, 32, 32)   9248        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 32)   128         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 32, 32, 32)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 16, 16, 64)   18496       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 64)   0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 64)   0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 96)   0           up_sampling2d_2[0][0]            \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 32, 32, 32)   27680       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 32)   128         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 32, 32, 32)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 32, 32, 32)   9248        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 32)   128         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 32, 32, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 64, 64, 32)   0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64, 64, 48)   0           up_sampling2d_3[0][0]            \n",
      "                                                                 activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 64, 64, 16)   6928        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 64, 64, 16)   64          conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 64, 64, 16)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 64, 64, 16)   2320        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 64, 64, 16)   64          conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 64, 64, 16)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 64, 64, 3)    51          activation_29[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 82,147\n",
      "Trainable params: 81,763\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_shape = (cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH, cfg.IMAGE_CHANNEL)\n",
    "modelObj = GANModel(batch_size=cfg.BATCH_SIZE, inputShape=image_shape,\n",
    "                                 dropout_prob=cfg.DROPOUT_PROB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) We call the utility function **show_statistics** to display the data distribution. This is just for debugging purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "##################### Training Data Statistics #####################\n",
      "######################################################################\n",
      "total image number \t 10817\n",
      "total class number \t 3\n",
      "class triangle \t 3703 images\n",
      "class square \t 3488 images\n",
      "class circular \t 3626 images\n",
      "######################################################################\n",
      "\n",
      "######################################################################\n",
      "##################### Validation Data Statistics #####################\n",
      "######################################################################\n",
      "total image number \t 2241\n",
      "total class number \t 3\n",
      "class triangle \t 745 images\n",
      "class circular \t 713 images\n",
      "class square \t 783 images\n",
      "######################################################################\n",
      "\n",
      "######################################################################\n",
      "##################### Testing Data Statistics #####################\n",
      "######################################################################\n",
      "total image number \t 2220\n",
      "total class number \t 3\n",
      "class triangle \t 733 images\n",
      "class circular \t 722 images\n",
      "class square \t 765 images\n",
      "######################################################################\n"
     ]
    }
   ],
   "source": [
    "#### show how the data looks like\n",
    "show_statistics(cfg.training_data_dir, fineGrained=False, title=\" Training Data Statistics \")\n",
    "show_statistics(cfg.validation_data_dir, fineGrained=False, title=\" Validation Data Statistics \")\n",
    "show_statistics(cfg.testing_data_dir, fineGrained=False, title=\" Testing Data Statistics \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) We now create batch generators to get small batches from the entire dataset. There is no need to change these functions as they already return **normalized inputs as batches**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batch generators are created!\n"
     ]
    }
   ],
   "source": [
    "nbr_train_data = get_dataset_size(cfg.training_data_dir)\n",
    "nbr_valid_data = get_dataset_size(cfg.validation_data_dir)\n",
    "nbr_test_data = get_dataset_size(cfg.testing_data_dir)\n",
    "train_batch_generator = generate_lastframepredictor_batches(cfg.training_data_dir, image_shape, cfg.BATCH_SIZE)\n",
    "valid_batch_generator = generate_lastframepredictor_batches(cfg.validation_data_dir, image_shape, cfg.BATCH_SIZE)\n",
    "test_batch_generator = generate_lastframepredictor_batches(cfg.testing_data_dir, image_shape, cfg.BATCH_SIZE)\n",
    "print(\"Data batch generators are created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) We can visualize how the data looks like for debugging purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (30, 64, 64, 3) float32 0.0 1.0\n",
      "train_y (30, 64, 64, 3) float32 0.0 1.0\n",
      "{'BATCH_SIZE': 30,\n",
      " 'DATA_AUGMENTATION': True,\n",
      " 'DEBUG_MODE': True,\n",
      " 'DROPOUT_PROB': 0.5,\n",
      " 'GPU': 0,\n",
      " 'IMAGE_CHANNEL': 3,\n",
      " 'IMAGE_HEIGHT': 64,\n",
      " 'IMAGE_WIDTH': 64,\n",
      " 'LEARNING_RATE': 0.01,\n",
      " 'LR_DECAY_FACTOR': 0.1,\n",
      " 'NUM_EPOCHS': 5,\n",
      " 'PRINT_EVERY': 20,\n",
      " 'SAVE_EVERY': 1,\n",
      " 'SEQUENCE_LENGTH': 10,\n",
      " 'testing_data_dir': '../data/FlyingObjectDataset_10K/testing',\n",
      " 'training_data_dir': '../data/FlyingObjectDataset_10K/training',\n",
      " 'validation_data_dir': '../data/FlyingObjectDataset_10K/validation'}\n"
     ]
    }
   ],
   "source": [
    "if cfg.DEBUG_MODE:\n",
    "    t_x, t_y = next(train_batch_generator)\n",
    "    print('train_x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\n",
    "    print('train_y', t_y.shape, t_y.dtype, t_y.min(), t_y.max()) \n",
    "    #plot_sample_lastframepredictor_data_with_groundtruth(t_x, t_y, t_y)\n",
    "    pprint.pprint (cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Start timer and init matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "# Adversarial loss ground truths\n",
    "valid = np.ones((cfg.BATCH_SIZE,) + modelObj.disc_patch)\n",
    "fake = np.zeros((cfg.BATCH_SIZE,) + modelObj.disc_patch)\n",
    "# log file\n",
    "output_log_dir = \"./logs/{}\".format(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "if not os.path.exists(output_log_dir):\n",
    "    os.makedirs(output_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) We can now feed the training and validation data to the network. This will train the network for **some epochs**. Note that the epoch number is also predefined in the file called **configGAN.py.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 64, 64, 3)\n",
      "0.44544736\n",
      "[Epoch 0/5] [Batch 0/360] [D loss: 0.555349] [G loss: 49.481636] time: 0:00:06.611614\n",
      "(30, 64, 64, 3)\n",
      "0.46809027\n",
      "[Epoch 0/5] [Batch 1/360] [D loss: 0.453507] [G loss: 48.088661] time: 0:00:06.744995\n",
      "(30, 64, 64, 3)\n",
      "0.49139598\n",
      "[Epoch 0/5] [Batch 2/360] [D loss: 0.371885] [G loss: 47.338261] time: 0:00:06.886890\n",
      "(30, 64, 64, 3)\n",
      "0.50718755\n",
      "[Epoch 0/5] [Batch 3/360] [D loss: 0.307387] [G loss: 46.426208] time: 0:00:07.019932\n",
      "(30, 64, 64, 3)\n",
      "0.5237621\n",
      "[Epoch 0/5] [Batch 4/360] [D loss: 0.254211] [G loss: 46.103062] time: 0:00:07.152967\n",
      "(30, 64, 64, 3)\n",
      "0.52076083\n",
      "[Epoch 0/5] [Batch 5/360] [D loss: 0.203362] [G loss: 45.455284] time: 0:00:07.287117\n",
      "(30, 64, 64, 3)\n",
      "0.51470953\n",
      "[Epoch 0/5] [Batch 6/360] [D loss: 0.160362] [G loss: 44.849819] time: 0:00:07.418163\n",
      "(30, 64, 64, 3)\n",
      "0.5204425\n",
      "[Epoch 0/5] [Batch 7/360] [D loss: 0.134224] [G loss: 44.269215] time: 0:00:07.549129\n",
      "(30, 64, 64, 3)\n",
      "0.51688695\n",
      "[Epoch 0/5] [Batch 8/360] [D loss: 0.113219] [G loss: 43.791031] time: 0:00:07.674210\n",
      "(30, 64, 64, 3)\n",
      "0.52522224\n",
      "[Epoch 0/5] [Batch 9/360] [D loss: 0.093514] [G loss: 43.777084] time: 0:00:07.804669\n",
      "(30, 64, 64, 3)\n",
      "0.52869767\n",
      "[Epoch 0/5] [Batch 10/360] [D loss: 0.075400] [G loss: 43.184635] time: 0:00:07.932583\n",
      "(30, 64, 64, 3)\n",
      "0.54263836\n",
      "[Epoch 0/5] [Batch 11/360] [D loss: 0.069753] [G loss: 42.894444] time: 0:00:08.059485\n",
      "(30, 64, 64, 3)\n",
      "0.50158256\n",
      "[Epoch 0/5] [Batch 12/360] [D loss: 0.058962] [G loss: 42.221722] time: 0:00:08.188530\n",
      "(30, 64, 64, 3)\n",
      "0.55986387\n",
      "[Epoch 0/5] [Batch 13/360] [D loss: 0.048436] [G loss: 42.385818] time: 0:00:08.348738\n",
      "(30, 64, 64, 3)\n",
      "0.5538298\n",
      "[Epoch 0/5] [Batch 14/360] [D loss: 0.041625] [G loss: 42.281445] time: 0:00:08.472302\n",
      "(30, 64, 64, 3)\n",
      "0.5133415\n",
      "[Epoch 0/5] [Batch 15/360] [D loss: 0.037349] [G loss: 41.411777] time: 0:00:08.593915\n",
      "(30, 64, 64, 3)\n",
      "0.5299948\n",
      "[Epoch 0/5] [Batch 16/360] [D loss: 0.034731] [G loss: 40.998795] time: 0:00:08.719125\n",
      "(30, 64, 64, 3)\n",
      "0.5572775\n",
      "[Epoch 0/5] [Batch 17/360] [D loss: 0.027384] [G loss: 41.034069] time: 0:00:08.881222\n",
      "(30, 64, 64, 3)\n",
      "0.5214772\n",
      "[Epoch 0/5] [Batch 18/360] [D loss: 0.022915] [G loss: 41.275555] time: 0:00:09.006213\n",
      "(30, 64, 64, 3)\n",
      "0.4695855\n",
      "[Epoch 0/5] [Batch 19/360] [D loss: 0.026163] [G loss: 39.736652] time: 0:00:09.133661\n",
      "(30, 64, 64, 3)\n",
      "0.5759414\n",
      "[Epoch 0/5] [Batch 20/360] [D loss: 0.020525] [G loss: 39.479538] time: 0:00:09.260563\n",
      "(30, 64, 64, 3)\n",
      "0.52791077\n",
      "[Epoch 0/5] [Batch 21/360] [D loss: 0.018600] [G loss: 39.598511] time: 0:00:09.390876\n",
      "(30, 64, 64, 3)\n",
      "0.5385935\n",
      "[Epoch 0/5] [Batch 22/360] [D loss: 0.019611] [G loss: 38.852802] time: 0:00:09.543329\n",
      "(30, 64, 64, 3)\n",
      "0.55290747\n",
      "[Epoch 0/5] [Batch 23/360] [D loss: 0.021309] [G loss: 38.898037] time: 0:00:09.675805\n",
      "(30, 64, 64, 3)\n",
      "0.5587854\n",
      "[Epoch 0/5] [Batch 24/360] [D loss: 0.019633] [G loss: 38.300861] time: 0:00:09.806090\n",
      "(30, 64, 64, 3)\n",
      "0.54250675\n",
      "[Epoch 0/5] [Batch 25/360] [D loss: 0.016501] [G loss: 38.282097] time: 0:00:09.936647\n",
      "(30, 64, 64, 3)\n",
      "0.5625003\n",
      "[Epoch 0/5] [Batch 26/360] [D loss: 0.016401] [G loss: 38.260616] time: 0:00:10.061347\n",
      "(30, 64, 64, 3)\n",
      "0.50336915\n",
      "[Epoch 0/5] [Batch 27/360] [D loss: 0.014097] [G loss: 38.749611] time: 0:00:10.224439\n",
      "(30, 64, 64, 3)\n",
      "0.56710696\n",
      "[Epoch 0/5] [Batch 28/360] [D loss: 0.016145] [G loss: 37.638256] time: 0:00:10.352024\n",
      "(30, 64, 64, 3)\n",
      "0.6062205\n",
      "[Epoch 0/5] [Batch 29/360] [D loss: 0.013882] [G loss: 37.292183] time: 0:00:10.485690\n",
      "(30, 64, 64, 3)\n",
      "0.60097027\n",
      "[Epoch 0/5] [Batch 30/360] [D loss: 0.014957] [G loss: 37.148960] time: 0:00:10.612496\n",
      "(30, 64, 64, 3)\n",
      "0.5218099\n",
      "[Epoch 0/5] [Batch 31/360] [D loss: 0.013380] [G loss: 36.935902] time: 0:00:10.735661\n",
      "(30, 64, 64, 3)\n",
      "0.61477536\n",
      "[Epoch 0/5] [Batch 32/360] [D loss: 0.015133] [G loss: 36.857231] time: 0:00:10.886973\n",
      "(30, 64, 64, 3)\n",
      "0.6130227\n",
      "[Epoch 0/5] [Batch 33/360] [D loss: 0.012239] [G loss: 36.503620] time: 0:00:11.009024\n",
      "(30, 64, 64, 3)\n",
      "0.5722025\n",
      "[Epoch 0/5] [Batch 34/360] [D loss: 0.011243] [G loss: 36.647011] time: 0:00:11.152153\n",
      "(30, 64, 64, 3)\n",
      "0.54652435\n",
      "[Epoch 0/5] [Batch 35/360] [D loss: 0.011003] [G loss: 36.531235] time: 0:00:11.279145\n",
      "(30, 64, 64, 3)\n",
      "0.60745263\n",
      "[Epoch 0/5] [Batch 36/360] [D loss: 0.010665] [G loss: 36.788128] time: 0:00:11.402242\n",
      "(30, 64, 64, 3)\n",
      "0.59798664\n",
      "[Epoch 0/5] [Batch 37/360] [D loss: 0.013077] [G loss: 36.278797] time: 0:00:11.524477\n",
      "(30, 64, 64, 3)\n",
      "0.566922\n",
      "[Epoch 0/5] [Batch 38/360] [D loss: 0.014427] [G loss: 36.349773] time: 0:00:11.648277\n",
      "(30, 64, 64, 3)\n",
      "0.6209712\n",
      "[Epoch 0/5] [Batch 39/360] [D loss: 0.012508] [G loss: 35.389267] time: 0:00:11.773802\n",
      "(30, 64, 64, 3)\n",
      "0.60331434\n",
      "[Epoch 0/5] [Batch 40/360] [D loss: 0.017138] [G loss: 35.735088] time: 0:00:11.938069\n",
      "(30, 64, 64, 3)\n",
      "0.62469155\n",
      "[Epoch 0/5] [Batch 41/360] [D loss: 0.014556] [G loss: 35.982388] time: 0:00:12.059781\n",
      "(30, 64, 64, 3)\n",
      "0.6267829\n",
      "[Epoch 0/5] [Batch 42/360] [D loss: 0.023490] [G loss: 35.528236] time: 0:00:12.187128\n",
      "(30, 64, 64, 3)\n",
      "0.587029\n",
      "[Epoch 0/5] [Batch 43/360] [D loss: 0.011327] [G loss: 35.309132] time: 0:00:12.318353\n",
      "(30, 64, 64, 3)\n",
      "0.61394477\n",
      "[Epoch 0/5] [Batch 44/360] [D loss: 0.013648] [G loss: 35.390385] time: 0:00:12.446835\n",
      "(30, 64, 64, 3)\n",
      "0.5934649\n",
      "[Epoch 0/5] [Batch 45/360] [D loss: 0.009900] [G loss: 35.305378] time: 0:00:12.569338\n",
      "(30, 64, 64, 3)\n",
      "0.61524194\n",
      "[Epoch 0/5] [Batch 46/360] [D loss: 0.012625] [G loss: 35.409756] time: 0:00:12.698004\n",
      "(30, 64, 64, 3)\n",
      "0.61050755\n",
      "[Epoch 0/5] [Batch 47/360] [D loss: 0.008357] [G loss: 35.296219] time: 0:00:12.830792\n",
      "(30, 64, 64, 3)\n",
      "0.6078747\n",
      "[Epoch 0/5] [Batch 48/360] [D loss: 0.010409] [G loss: 34.601082] time: 0:00:12.954235\n",
      "(30, 64, 64, 3)\n",
      "0.649954\n",
      "[Epoch 0/5] [Batch 49/360] [D loss: 0.009149] [G loss: 34.584282] time: 0:00:13.077905\n",
      "(30, 64, 64, 3)\n",
      "0.61935\n",
      "[Epoch 0/5] [Batch 50/360] [D loss: 0.011332] [G loss: 34.149551] time: 0:00:13.202070\n",
      "(30, 64, 64, 3)\n",
      "0.6069785\n",
      "[Epoch 0/5] [Batch 51/360] [D loss: 0.009806] [G loss: 34.734917] time: 0:00:13.324207\n",
      "(30, 64, 64, 3)\n",
      "0.6244667\n",
      "[Epoch 0/5] [Batch 52/360] [D loss: 0.011307] [G loss: 34.184929] time: 0:00:13.462117\n",
      "(30, 64, 64, 3)\n",
      "0.64413774\n",
      "[Epoch 0/5] [Batch 53/360] [D loss: 0.008254] [G loss: 34.805016] time: 0:00:13.619138\n",
      "(30, 64, 64, 3)\n",
      "0.6322605\n",
      "[Epoch 0/5] [Batch 54/360] [D loss: 0.011907] [G loss: 34.465340] time: 0:00:13.769897\n",
      "(30, 64, 64, 3)\n",
      "0.5998803\n",
      "[Epoch 0/5] [Batch 55/360] [D loss: 0.006547] [G loss: 34.607784] time: 0:00:13.898702\n",
      "(30, 64, 64, 3)\n",
      "0.62724084\n",
      "[Epoch 0/5] [Batch 56/360] [D loss: 0.010637] [G loss: 34.033489] time: 0:00:14.029701\n",
      "(30, 64, 64, 3)\n",
      "0.6267646\n",
      "[Epoch 0/5] [Batch 57/360] [D loss: 0.009004] [G loss: 34.271477] time: 0:00:14.163701\n",
      "(30, 64, 64, 3)\n",
      "0.5665791\n",
      "[Epoch 0/5] [Batch 58/360] [D loss: 0.010985] [G loss: 34.316982] time: 0:00:14.298620\n",
      "(30, 64, 64, 3)\n",
      "0.6012261\n",
      "[Epoch 0/5] [Batch 59/360] [D loss: 0.007134] [G loss: 34.351646] time: 0:00:14.427145\n",
      "(30, 64, 64, 3)\n",
      "0.6108598\n",
      "[Epoch 0/5] [Batch 60/360] [D loss: 0.009784] [G loss: 33.855907] time: 0:00:14.558005\n",
      "(30, 64, 64, 3)\n",
      "0.6255266\n",
      "[Epoch 0/5] [Batch 61/360] [D loss: 0.007531] [G loss: 33.744274] time: 0:00:14.688576\n",
      "(30, 64, 64, 3)\n",
      "0.61169994\n",
      "[Epoch 0/5] [Batch 62/360] [D loss: 0.011663] [G loss: 33.300747] time: 0:00:14.816196\n",
      "(30, 64, 64, 3)\n",
      "0.62012213\n",
      "[Epoch 0/5] [Batch 63/360] [D loss: 0.007448] [G loss: 34.084183] time: 0:00:14.939742\n",
      "(30, 64, 64, 3)\n",
      "0.65144545\n",
      "[Epoch 0/5] [Batch 64/360] [D loss: 0.010912] [G loss: 33.463097] time: 0:00:15.071604\n",
      "(30, 64, 64, 3)\n",
      "0.63857955\n",
      "[Epoch 0/5] [Batch 65/360] [D loss: 0.008411] [G loss: 34.040951] time: 0:00:15.203264\n",
      "(30, 64, 64, 3)\n",
      "0.6353853\n",
      "[Epoch 0/5] [Batch 66/360] [D loss: 0.012159] [G loss: 33.416309] time: 0:00:15.331798\n",
      "(30, 64, 64, 3)\n",
      "0.64107674\n",
      "[Epoch 0/5] [Batch 67/360] [D loss: 0.009721] [G loss: 33.401829] time: 0:00:15.465557\n",
      "(30, 64, 64, 3)\n",
      "0.5737281\n",
      "[Epoch 0/5] [Batch 68/360] [D loss: 0.010696] [G loss: 33.216564] time: 0:00:15.598136\n",
      "(30, 64, 64, 3)\n",
      "0.6220699\n",
      "[Epoch 0/5] [Batch 69/360] [D loss: 0.007754] [G loss: 33.354122] time: 0:00:15.727065\n",
      "(30, 64, 64, 3)\n",
      "0.63809633\n",
      "[Epoch 0/5] [Batch 70/360] [D loss: 0.009582] [G loss: 32.799011] time: 0:00:15.866469\n",
      "(30, 64, 64, 3)\n",
      "0.61711\n",
      "[Epoch 0/5] [Batch 71/360] [D loss: 0.006337] [G loss: 32.784271] time: 0:00:15.998633\n",
      "(30, 64, 64, 3)\n",
      "0.5915624\n",
      "[Epoch 0/5] [Batch 72/360] [D loss: 0.006370] [G loss: 33.335865] time: 0:00:16.124465\n",
      "(30, 64, 64, 3)\n",
      "0.5805229\n",
      "[Epoch 0/5] [Batch 73/360] [D loss: 0.005964] [G loss: 33.397114] time: 0:00:16.256433\n",
      "(30, 64, 64, 3)\n",
      "0.6499759\n",
      "[Epoch 0/5] [Batch 74/360] [D loss: 0.006944] [G loss: 33.391037] time: 0:00:16.388472\n",
      "(30, 64, 64, 3)\n",
      "0.6123071\n",
      "[Epoch 0/5] [Batch 75/360] [D loss: 0.005067] [G loss: 32.793732] time: 0:00:16.516617\n",
      "(30, 64, 64, 3)\n",
      "0.6242854\n",
      "[Epoch 0/5] [Batch 76/360] [D loss: 0.007700] [G loss: 33.013237] time: 0:00:16.645536\n",
      "(30, 64, 64, 3)\n",
      "0.6375575\n",
      "[Epoch 0/5] [Batch 77/360] [D loss: 0.005275] [G loss: 32.501511] time: 0:00:16.776230\n",
      "(30, 64, 64, 3)\n",
      "0.5955314\n",
      "[Epoch 0/5] [Batch 78/360] [D loss: 0.004946] [G loss: 32.889931] time: 0:00:16.908970\n",
      "(30, 64, 64, 3)\n",
      "0.62374705\n",
      "[Epoch 0/5] [Batch 79/360] [D loss: 0.004419] [G loss: 32.767826] time: 0:00:17.042459\n",
      "(30, 64, 64, 3)\n",
      "0.62851524\n",
      "[Epoch 0/5] [Batch 80/360] [D loss: 0.004489] [G loss: 32.681892] time: 0:00:17.175792\n",
      "(30, 64, 64, 3)\n",
      "0.61644775\n",
      "[Epoch 0/5] [Batch 81/360] [D loss: 0.005478] [G loss: 32.315872] time: 0:00:17.316376\n",
      "(30, 64, 64, 3)\n",
      "0.64044946\n",
      "[Epoch 0/5] [Batch 82/360] [D loss: 0.004104] [G loss: 32.529984] time: 0:00:17.454701\n",
      "(30, 64, 64, 3)\n",
      "0.62289137\n",
      "[Epoch 0/5] [Batch 83/360] [D loss: 0.004639] [G loss: 32.452324] time: 0:00:17.612119\n",
      "(30, 64, 64, 3)\n",
      "0.62599254\n",
      "[Epoch 0/5] [Batch 84/360] [D loss: 0.005050] [G loss: 32.290401] time: 0:00:17.743229\n",
      "(30, 64, 64, 3)\n",
      "0.6320345\n",
      "[Epoch 0/5] [Batch 85/360] [D loss: 0.008659] [G loss: 31.925587] time: 0:00:17.876610\n",
      "(30, 64, 64, 3)\n",
      "0.65800685\n",
      "[Epoch 0/5] [Batch 86/360] [D loss: 0.004346] [G loss: 32.000042] time: 0:00:18.014825\n",
      "(30, 64, 64, 3)\n",
      "0.63795334\n",
      "[Epoch 0/5] [Batch 87/360] [D loss: 0.006567] [G loss: 32.018070] time: 0:00:18.146183\n",
      "(30, 64, 64, 3)\n",
      "0.62154865\n",
      "[Epoch 0/5] [Batch 88/360] [D loss: 0.008820] [G loss: 32.462467] time: 0:00:18.285856\n",
      "(30, 64, 64, 3)\n",
      "0.6380666\n",
      "[Epoch 0/5] [Batch 89/360] [D loss: 0.014489] [G loss: 32.164257] time: 0:00:18.418702\n",
      "(30, 64, 64, 3)\n",
      "0.64838225\n",
      "[Epoch 0/5] [Batch 90/360] [D loss: 0.008981] [G loss: 31.960592] time: 0:00:18.551509\n",
      "(30, 64, 64, 3)\n",
      "0.6589313\n",
      "[Epoch 0/5] [Batch 91/360] [D loss: 0.016710] [G loss: 32.303631] time: 0:00:18.676817\n",
      "(30, 64, 64, 3)\n",
      "0.62230915\n",
      "[Epoch 0/5] [Batch 92/360] [D loss: 0.009695] [G loss: 32.154510] time: 0:00:18.803119\n",
      "(30, 64, 64, 3)\n",
      "0.6493842\n",
      "[Epoch 0/5] [Batch 93/360] [D loss: 0.017551] [G loss: 31.953505] time: 0:00:18.960916\n",
      "(30, 64, 64, 3)\n",
      "0.64113593\n",
      "[Epoch 0/5] [Batch 94/360] [D loss: 0.010243] [G loss: 32.039234] time: 0:00:19.091441\n",
      "(30, 64, 64, 3)\n",
      "0.65815705\n",
      "[Epoch 0/5] [Batch 95/360] [D loss: 0.018676] [G loss: 31.343153] time: 0:00:19.218416\n",
      "(30, 64, 64, 3)\n",
      "0.6929953\n",
      "[Epoch 0/5] [Batch 96/360] [D loss: 0.010401] [G loss: 31.924582] time: 0:00:19.343315\n",
      "(30, 64, 64, 3)\n",
      "0.5871331\n",
      "[Epoch 0/5] [Batch 97/360] [D loss: 0.012407] [G loss: 31.942768] time: 0:00:19.477894\n",
      "(30, 64, 64, 3)\n",
      "0.6669386\n",
      "[Epoch 0/5] [Batch 98/360] [D loss: 0.007641] [G loss: 32.014015] time: 0:00:19.607917\n",
      "(30, 64, 64, 3)\n",
      "0.6404512\n",
      "[Epoch 0/5] [Batch 99/360] [D loss: 0.011512] [G loss: 31.706829] time: 0:00:19.741557\n",
      "(30, 64, 64, 3)\n",
      "0.62981415\n",
      "[Epoch 0/5] [Batch 100/360] [D loss: 0.005874] [G loss: 31.839520] time: 0:00:19.868831\n",
      "(30, 64, 64, 3)\n",
      "0.6837316\n",
      "[Epoch 0/5] [Batch 101/360] [D loss: 0.007292] [G loss: 31.692842] time: 0:00:20.005145\n",
      "(30, 64, 64, 3)\n",
      "0.668675\n",
      "[Epoch 0/5] [Batch 102/360] [D loss: 0.004572] [G loss: 31.334673] time: 0:00:20.135228\n",
      "(30, 64, 64, 3)\n",
      "0.64526045\n",
      "[Epoch 0/5] [Batch 103/360] [D loss: 0.004701] [G loss: 30.846504] time: 0:00:20.265414\n",
      "(30, 64, 64, 3)\n",
      "0.63783574\n",
      "[Epoch 0/5] [Batch 104/360] [D loss: 0.003573] [G loss: 31.570087] time: 0:00:20.398793\n",
      "(30, 64, 64, 3)\n",
      "0.6366511\n",
      "[Epoch 0/5] [Batch 105/360] [D loss: 0.006477] [G loss: 30.936312] time: 0:00:20.529346\n",
      "(30, 64, 64, 3)\n",
      "0.64831644\n",
      "[Epoch 0/5] [Batch 106/360] [D loss: 0.004490] [G loss: 31.879372] time: 0:00:20.659425\n",
      "(30, 64, 64, 3)\n",
      "0.64857394\n",
      "[Epoch 0/5] [Batch 107/360] [D loss: 0.006465] [G loss: 31.378502] time: 0:00:20.787174\n",
      "(30, 64, 64, 3)\n",
      "0.6154221\n",
      "[Epoch 0/5] [Batch 108/360] [D loss: 0.004126] [G loss: 31.036741] time: 0:00:20.911998\n",
      "(30, 64, 64, 3)\n",
      "0.6521999\n",
      "[Epoch 0/5] [Batch 109/360] [D loss: 0.003880] [G loss: 31.194332] time: 0:00:21.037910\n",
      "(30, 64, 64, 3)\n",
      "0.64945495\n",
      "[Epoch 0/5] [Batch 110/360] [D loss: 0.003899] [G loss: 31.080935] time: 0:00:21.164662\n",
      "(30, 64, 64, 3)\n",
      "0.6980648\n",
      "[Epoch 0/5] [Batch 111/360] [D loss: 0.004606] [G loss: 30.404707] time: 0:00:21.288663\n",
      "(30, 64, 64, 3)\n",
      "0.6892989\n",
      "[Epoch 0/5] [Batch 112/360] [D loss: 0.003829] [G loss: 30.965443] time: 0:00:21.410624\n",
      "(30, 64, 64, 3)\n",
      "0.62964046\n",
      "[Epoch 0/5] [Batch 113/360] [D loss: 0.005301] [G loss: 30.180077] time: 0:00:21.534225\n",
      "(30, 64, 64, 3)\n",
      "0.67847675\n",
      "[Epoch 0/5] [Batch 114/360] [D loss: 0.003528] [G loss: 31.018459] time: 0:00:21.660497\n",
      "(30, 64, 64, 3)\n",
      "0.6425131\n",
      "[Epoch 0/5] [Batch 115/360] [D loss: 0.004793] [G loss: 30.418013] time: 0:00:21.797505\n",
      "(30, 64, 64, 3)\n",
      "0.63795197\n",
      "[Epoch 0/5] [Batch 116/360] [D loss: 0.006613] [G loss: 30.774874] time: 0:00:21.956534\n",
      "(30, 64, 64, 3)\n",
      "0.68403226\n",
      "[Epoch 0/5] [Batch 117/360] [D loss: 0.006380] [G loss: 30.678312] time: 0:00:22.081946\n",
      "(30, 64, 64, 3)\n",
      "0.65177435\n",
      "[Epoch 0/5] [Batch 118/360] [D loss: 0.004727] [G loss: 30.403574] time: 0:00:22.204513\n",
      "(30, 64, 64, 3)\n",
      "0.6616816\n",
      "[Epoch 0/5] [Batch 119/360] [D loss: 0.005286] [G loss: 30.679544] time: 0:00:22.328950\n",
      "(30, 64, 64, 3)\n",
      "0.66000915\n",
      "[Epoch 0/5] [Batch 120/360] [D loss: 0.004278] [G loss: 30.609497] time: 0:00:22.453945\n",
      "(30, 64, 64, 3)\n",
      "0.6852429\n",
      "[Epoch 0/5] [Batch 121/360] [D loss: 0.005122] [G loss: 30.645849] time: 0:00:22.575458\n",
      "(30, 64, 64, 3)\n",
      "0.63568413\n",
      "[Epoch 0/5] [Batch 122/360] [D loss: 0.003954] [G loss: 30.533680] time: 0:00:22.716119\n",
      "(30, 64, 64, 3)\n",
      "0.63938093\n",
      "[Epoch 0/5] [Batch 123/360] [D loss: 0.004617] [G loss: 30.656115] time: 0:00:22.870336\n",
      "(30, 64, 64, 3)\n",
      "0.6796856\n",
      "[Epoch 0/5] [Batch 124/360] [D loss: 0.003433] [G loss: 30.203794] time: 0:00:22.999311\n",
      "(30, 64, 64, 3)\n",
      "0.6123603\n",
      "[Epoch 0/5] [Batch 125/360] [D loss: 0.004524] [G loss: 30.162050] time: 0:00:23.130206\n",
      "(30, 64, 64, 3)\n",
      "0.7021257\n",
      "[Epoch 0/5] [Batch 126/360] [D loss: 0.003878] [G loss: 30.457312] time: 0:00:23.254402\n",
      "(30, 64, 64, 3)\n",
      "0.65075237\n",
      "[Epoch 0/5] [Batch 127/360] [D loss: 0.006714] [G loss: 30.382168] time: 0:00:23.387017\n",
      "(30, 64, 64, 3)\n",
      "0.61289257\n",
      "[Epoch 0/5] [Batch 128/360] [D loss: 0.004952] [G loss: 30.469393] time: 0:00:23.537524\n",
      "(30, 64, 64, 3)\n",
      "0.67954046\n",
      "[Epoch 0/5] [Batch 129/360] [D loss: 0.007103] [G loss: 30.097254] time: 0:00:23.665903\n",
      "(30, 64, 64, 3)\n",
      "0.6408701\n",
      "[Epoch 0/5] [Batch 130/360] [D loss: 0.006802] [G loss: 30.155540] time: 0:00:23.794446\n",
      "(30, 64, 64, 3)\n",
      "0.6771094\n",
      "[Epoch 0/5] [Batch 131/360] [D loss: 0.009469] [G loss: 29.617201] time: 0:00:23.937164\n",
      "(30, 64, 64, 3)\n",
      "0.63980997\n",
      "[Epoch 0/5] [Batch 132/360] [D loss: 0.005951] [G loss: 29.639351] time: 0:00:24.057092\n",
      "(30, 64, 64, 3)\n",
      "0.6543161\n",
      "[Epoch 0/5] [Batch 133/360] [D loss: 0.006199] [G loss: 30.217583] time: 0:00:24.186439\n",
      "(30, 64, 64, 3)\n",
      "0.6580785\n",
      "[Epoch 0/5] [Batch 134/360] [D loss: 0.005218] [G loss: 30.302608] time: 0:00:24.309007\n",
      "(30, 64, 64, 3)\n",
      "0.63995427\n",
      "[Epoch 0/5] [Batch 135/360] [D loss: 0.007980] [G loss: 29.929255] time: 0:00:24.430389\n",
      "(30, 64, 64, 3)\n",
      "0.6515636\n",
      "[Epoch 0/5] [Batch 136/360] [D loss: 0.003698] [G loss: 29.935175] time: 0:00:24.555790\n",
      "(30, 64, 64, 3)\n",
      "0.6835678\n",
      "[Epoch 0/5] [Batch 137/360] [D loss: 0.005575] [G loss: 30.074080] time: 0:00:24.688543\n",
      "(30, 64, 64, 3)\n",
      "0.6904579\n",
      "[Epoch 0/5] [Batch 138/360] [D loss: 0.004171] [G loss: 29.722010] time: 0:00:24.813820\n",
      "(30, 64, 64, 3)\n",
      "0.6355286\n",
      "[Epoch 0/5] [Batch 139/360] [D loss: 0.004557] [G loss: 29.848747] time: 0:00:24.937522\n",
      "(30, 64, 64, 3)\n",
      "0.66576034\n",
      "[Epoch 0/5] [Batch 140/360] [D loss: 0.003259] [G loss: 30.122454] time: 0:00:25.058547\n",
      "(30, 64, 64, 3)\n",
      "0.6635792\n",
      "[Epoch 0/5] [Batch 141/360] [D loss: 0.003197] [G loss: 29.547842] time: 0:00:25.182247\n",
      "(30, 64, 64, 3)\n",
      "0.6606571\n",
      "[Epoch 0/5] [Batch 142/360] [D loss: 0.003382] [G loss: 29.480753] time: 0:00:25.341225\n",
      "(30, 64, 64, 3)\n",
      "0.6731073\n",
      "[Epoch 0/5] [Batch 143/360] [D loss: 0.003720] [G loss: 29.251810] time: 0:00:25.480791\n",
      "(30, 64, 64, 3)\n",
      "0.67229193\n",
      "[Epoch 0/5] [Batch 144/360] [D loss: 0.002579] [G loss: 29.473818] time: 0:00:25.602599\n",
      "(30, 64, 64, 3)\n",
      "0.67324924\n",
      "[Epoch 0/5] [Batch 145/360] [D loss: 0.004480] [G loss: 29.011181] time: 0:00:25.725045\n",
      "(30, 64, 64, 3)\n",
      "0.66556996\n",
      "[Epoch 0/5] [Batch 146/360] [D loss: 0.002150] [G loss: 29.321079] time: 0:00:25.850644\n",
      "(30, 64, 64, 3)\n",
      "0.6167668\n",
      "[Epoch 0/5] [Batch 147/360] [D loss: 0.004320] [G loss: 29.449636] time: 0:00:25.977694\n",
      "(30, 64, 64, 3)\n",
      "0.6562957\n",
      "[Epoch 0/5] [Batch 148/360] [D loss: 0.002682] [G loss: 29.113604] time: 0:00:26.106477\n",
      "(30, 64, 64, 3)\n",
      "0.6536475\n",
      "[Epoch 0/5] [Batch 149/360] [D loss: 0.002413] [G loss: 28.968109] time: 0:00:26.240253\n",
      "(30, 64, 64, 3)\n",
      "0.6955598\n",
      "[Epoch 0/5] [Batch 150/360] [D loss: 0.002605] [G loss: 29.230198] time: 0:00:26.368295\n",
      "(30, 64, 64, 3)\n",
      "0.6394922\n",
      "[Epoch 0/5] [Batch 151/360] [D loss: 0.002315] [G loss: 29.915808] time: 0:00:26.505889\n",
      "(30, 64, 64, 3)\n",
      "0.6569279\n",
      "[Epoch 0/5] [Batch 152/360] [D loss: 0.002728] [G loss: 29.064245] time: 0:00:26.634884\n",
      "(30, 64, 64, 3)\n",
      "0.67839783\n",
      "[Epoch 0/5] [Batch 153/360] [D loss: 0.002334] [G loss: 29.349030] time: 0:00:26.772892\n",
      "(30, 64, 64, 3)\n",
      "0.70434064\n",
      "[Epoch 0/5] [Batch 154/360] [D loss: 0.002913] [G loss: 29.708998] time: 0:00:26.902140\n",
      "(30, 64, 64, 3)\n",
      "0.6765339\n",
      "[Epoch 0/5] [Batch 155/360] [D loss: 0.002451] [G loss: 28.852509] time: 0:00:27.035374\n",
      "(30, 64, 64, 3)\n",
      "0.6537207\n",
      "[Epoch 0/5] [Batch 156/360] [D loss: 0.002749] [G loss: 29.087713] time: 0:00:27.185889\n",
      "(30, 64, 64, 3)\n",
      "0.688725\n",
      "[Epoch 0/5] [Batch 157/360] [D loss: 0.002848] [G loss: 29.180084] time: 0:00:27.314608\n",
      "(30, 64, 64, 3)\n",
      "0.68128896\n",
      "[Epoch 0/5] [Batch 158/360] [D loss: 0.002614] [G loss: 29.144415] time: 0:00:27.441600\n",
      "(30, 64, 64, 3)\n",
      "0.6286\n",
      "[Epoch 0/5] [Batch 159/360] [D loss: 0.002566] [G loss: 29.166782] time: 0:00:27.566370\n",
      "(30, 64, 64, 3)\n",
      "0.66348237\n",
      "[Epoch 0/5] [Batch 160/360] [D loss: 0.002763] [G loss: 28.847569] time: 0:00:27.696008\n",
      "(30, 64, 64, 3)\n",
      "0.68493795\n",
      "[Epoch 0/5] [Batch 161/360] [D loss: 0.003071] [G loss: 29.369825] time: 0:00:27.827674\n",
      "(30, 64, 64, 3)\n",
      "0.65234995\n",
      "[Epoch 0/5] [Batch 162/360] [D loss: 0.002211] [G loss: 29.046831] time: 0:00:27.951480\n",
      "(30, 64, 64, 3)\n",
      "0.68354696\n",
      "[Epoch 0/5] [Batch 163/360] [D loss: 0.004083] [G loss: 28.744762] time: 0:00:28.081464\n",
      "(30, 64, 64, 3)\n",
      "0.67251277\n",
      "[Epoch 0/5] [Batch 164/360] [D loss: 0.003659] [G loss: 28.845318] time: 0:00:28.218590\n",
      "(30, 64, 64, 3)\n",
      "0.66898704\n",
      "[Epoch 0/5] [Batch 165/360] [D loss: 0.003403] [G loss: 28.741627] time: 0:00:28.384563\n",
      "(30, 64, 64, 3)\n",
      "0.70297736\n",
      "[Epoch 0/5] [Batch 166/360] [D loss: 0.002870] [G loss: 28.771048] time: 0:00:28.510929\n",
      "(30, 64, 64, 3)\n",
      "0.66346186\n",
      "[Epoch 0/5] [Batch 167/360] [D loss: 0.003102] [G loss: 28.516060] time: 0:00:28.635400\n",
      "(30, 64, 64, 3)\n",
      "0.68053645\n",
      "[Epoch 0/5] [Batch 168/360] [D loss: 0.002838] [G loss: 28.589899] time: 0:00:28.769407\n",
      "(30, 64, 64, 3)\n",
      "0.65382123\n",
      "[Epoch 0/5] [Batch 169/360] [D loss: 0.004221] [G loss: 28.341276] time: 0:00:28.934367\n",
      "(30, 64, 64, 3)\n",
      "0.69349843\n",
      "[Epoch 0/5] [Batch 170/360] [D loss: 0.003809] [G loss: 28.592541] time: 0:00:29.067788\n",
      "(30, 64, 64, 3)\n",
      "0.6573345\n",
      "[Epoch 0/5] [Batch 171/360] [D loss: 0.003855] [G loss: 29.064899] time: 0:00:29.194966\n",
      "(30, 64, 64, 3)\n",
      "0.6462523\n",
      "[Epoch 0/5] [Batch 172/360] [D loss: 0.002895] [G loss: 28.442554] time: 0:00:29.323794\n",
      "(30, 64, 64, 3)\n",
      "0.68882084\n",
      "[Epoch 0/5] [Batch 173/360] [D loss: 0.005070] [G loss: 28.289194] time: 0:00:29.454921\n",
      "(30, 64, 64, 3)\n",
      "0.6895239\n",
      "[Epoch 0/5] [Batch 174/360] [D loss: 0.005098] [G loss: 28.972639] time: 0:00:29.585915\n",
      "(30, 64, 64, 3)\n",
      "0.67871124\n",
      "[Epoch 0/5] [Batch 175/360] [D loss: 0.011785] [G loss: 28.404572] time: 0:00:29.716818\n",
      "(30, 64, 64, 3)\n",
      "0.6590157\n",
      "[Epoch 0/5] [Batch 176/360] [D loss: 0.010217] [G loss: 28.729883] time: 0:00:29.843849\n",
      "(30, 64, 64, 3)\n",
      "0.68459016\n",
      "[Epoch 0/5] [Batch 177/360] [D loss: 0.022317] [G loss: 28.000772] time: 0:00:29.970408\n",
      "(30, 64, 64, 3)\n",
      "0.64409214\n",
      "[Epoch 0/5] [Batch 178/360] [D loss: 0.009011] [G loss: 29.033329] time: 0:00:30.099484\n",
      "(30, 64, 64, 3)\n",
      "0.69930243\n",
      "[Epoch 0/5] [Batch 179/360] [D loss: 0.026117] [G loss: 28.136650] time: 0:00:30.228208\n",
      "(30, 64, 64, 3)\n",
      "0.6487934\n",
      "[Epoch 0/5] [Batch 180/360] [D loss: 0.014513] [G loss: 28.460974] time: 0:00:30.362115\n",
      "(30, 64, 64, 3)\n",
      "0.6572393\n",
      "[Epoch 0/5] [Batch 181/360] [D loss: 0.024323] [G loss: 28.020252] time: 0:00:30.486528\n",
      "(30, 64, 64, 3)\n",
      "0.6817158\n",
      "[Epoch 0/5] [Batch 182/360] [D loss: 0.011316] [G loss: 28.284346] time: 0:00:30.612795\n",
      "(30, 64, 64, 3)\n",
      "0.6700559\n",
      "[Epoch 0/5] [Batch 183/360] [D loss: 0.011291] [G loss: 28.372305] time: 0:00:30.767684\n",
      "(30, 64, 64, 3)\n",
      "0.689362\n",
      "[Epoch 0/5] [Batch 184/360] [D loss: 0.004945] [G loss: 28.429235] time: 0:00:30.902658\n",
      "(30, 64, 64, 3)\n",
      "0.65972275\n",
      "[Epoch 0/5] [Batch 185/360] [D loss: 0.005505] [G loss: 28.262415] time: 0:00:31.024536\n",
      "(30, 64, 64, 3)\n",
      "0.71694\n",
      "[Epoch 0/5] [Batch 186/360] [D loss: 0.002687] [G loss: 28.409477] time: 0:00:31.147333\n",
      "(30, 64, 64, 3)\n",
      "0.6861773\n",
      "[Epoch 0/5] [Batch 187/360] [D loss: 0.003165] [G loss: 28.057770] time: 0:00:31.273137\n",
      "(30, 64, 64, 3)\n",
      "0.6923016\n",
      "[Epoch 0/5] [Batch 188/360] [D loss: 0.002359] [G loss: 28.102789] time: 0:00:31.396168\n",
      "(30, 64, 64, 3)\n",
      "0.67611235\n",
      "[Epoch 0/5] [Batch 189/360] [D loss: 0.001917] [G loss: 28.051294] time: 0:00:31.525130\n",
      "(30, 64, 64, 3)\n",
      "0.649574\n",
      "[Epoch 0/5] [Batch 190/360] [D loss: 0.001649] [G loss: 27.760088] time: 0:00:31.648872\n",
      "(30, 64, 64, 3)\n",
      "0.70333976\n",
      "[Epoch 0/5] [Batch 191/360] [D loss: 0.002348] [G loss: 28.227926] time: 0:00:31.775539\n",
      "(30, 64, 64, 3)\n",
      "0.6575182\n",
      "[Epoch 0/5] [Batch 192/360] [D loss: 0.001811] [G loss: 27.978016] time: 0:00:31.899439\n",
      "(30, 64, 64, 3)\n",
      "0.712138\n",
      "[Epoch 0/5] [Batch 193/360] [D loss: 0.001624] [G loss: 27.652273] time: 0:00:32.025221\n",
      "(30, 64, 64, 3)\n",
      "0.6759575\n",
      "[Epoch 0/5] [Batch 194/360] [D loss: 0.001526] [G loss: 27.886309] time: 0:00:32.148917\n",
      "(30, 64, 64, 3)\n",
      "0.6867443\n",
      "[Epoch 0/5] [Batch 195/360] [D loss: 0.001893] [G loss: 27.450596] time: 0:00:32.275731\n",
      "(30, 64, 64, 3)\n",
      "0.664589\n",
      "[Epoch 0/5] [Batch 196/360] [D loss: 0.001781] [G loss: 27.884777] time: 0:00:32.413882\n",
      "(30, 64, 64, 3)\n",
      "0.69270235\n",
      "[Epoch 0/5] [Batch 197/360] [D loss: 0.001566] [G loss: 28.128593] time: 0:00:32.547656\n",
      "(30, 64, 64, 3)\n",
      "0.68404245\n",
      "[Epoch 0/5] [Batch 198/360] [D loss: 0.001453] [G loss: 27.871635] time: 0:00:32.673576\n",
      "(30, 64, 64, 3)\n",
      "0.6781082\n",
      "[Epoch 0/5] [Batch 199/360] [D loss: 0.001704] [G loss: 27.586771] time: 0:00:32.811368\n",
      "(30, 64, 64, 3)\n",
      "0.6690449\n",
      "[Epoch 0/5] [Batch 200/360] [D loss: 0.001727] [G loss: 28.243036] time: 0:00:32.935644\n",
      "(30, 64, 64, 3)\n",
      "0.65871614\n",
      "[Epoch 0/5] [Batch 201/360] [D loss: 0.001803] [G loss: 27.605053] time: 0:00:33.074215\n",
      "(30, 64, 64, 3)\n",
      "0.69884807\n",
      "[Epoch 0/5] [Batch 202/360] [D loss: 0.002408] [G loss: 27.904844] time: 0:00:33.196190\n",
      "(30, 64, 64, 3)\n",
      "0.693132\n",
      "[Epoch 0/5] [Batch 203/360] [D loss: 0.002581] [G loss: 27.840466] time: 0:00:33.321594\n",
      "(30, 64, 64, 3)\n",
      "0.66879344\n",
      "[Epoch 0/5] [Batch 204/360] [D loss: 0.001632] [G loss: 27.696703] time: 0:00:33.454667\n",
      "(30, 64, 64, 3)\n",
      "0.669679\n",
      "[Epoch 0/5] [Batch 205/360] [D loss: 0.001918] [G loss: 27.108564] time: 0:00:33.605305\n",
      "(30, 64, 64, 3)\n",
      "0.6766095\n",
      "[Epoch 0/5] [Batch 206/360] [D loss: 0.001440] [G loss: 27.230986] time: 0:00:33.738881\n",
      "(30, 64, 64, 3)\n",
      "0.65686435\n",
      "[Epoch 0/5] [Batch 207/360] [D loss: 0.001492] [G loss: 27.508030] time: 0:00:33.867006\n",
      "(30, 64, 64, 3)\n",
      "0.70292443\n",
      "[Epoch 0/5] [Batch 208/360] [D loss: 0.001803] [G loss: 27.499882] time: 0:00:33.995527\n",
      "(30, 64, 64, 3)\n",
      "0.67259854\n",
      "[Epoch 0/5] [Batch 209/360] [D loss: 0.003747] [G loss: 27.035748] time: 0:00:34.123760\n",
      "(30, 64, 64, 3)\n",
      "0.6659836\n",
      "[Epoch 0/5] [Batch 210/360] [D loss: 0.002275] [G loss: 27.720741] time: 0:00:34.356338\n",
      "(30, 64, 64, 3)\n",
      "0.6791475\n",
      "[Epoch 0/5] [Batch 211/360] [D loss: 0.002179] [G loss: 27.302238] time: 0:00:34.487154\n",
      "(30, 64, 64, 3)\n",
      "0.6493449\n",
      "[Epoch 0/5] [Batch 212/360] [D loss: 0.002174] [G loss: 27.355766] time: 0:00:34.617972\n",
      "(30, 64, 64, 3)\n",
      "0.6935079\n",
      "[Epoch 0/5] [Batch 213/360] [D loss: 0.002827] [G loss: 27.052153] time: 0:00:34.745304\n",
      "(30, 64, 64, 3)\n",
      "0.6709039\n",
      "[Epoch 0/5] [Batch 214/360] [D loss: 0.002662] [G loss: 27.147551] time: 0:00:34.875859\n",
      "(30, 64, 64, 3)\n",
      "0.6720214\n",
      "[Epoch 0/5] [Batch 215/360] [D loss: 0.002453] [G loss: 27.321093] time: 0:00:35.031287\n",
      "(30, 64, 64, 3)\n",
      "0.6898815\n",
      "[Epoch 0/5] [Batch 216/360] [D loss: 0.002118] [G loss: 26.920683] time: 0:00:35.152354\n",
      "(30, 64, 64, 3)\n",
      "0.6521747\n",
      "[Epoch 0/5] [Batch 217/360] [D loss: 0.002452] [G loss: 27.552992] time: 0:00:35.278526\n",
      "(30, 64, 64, 3)\n",
      "0.7265353\n",
      "[Epoch 0/5] [Batch 218/360] [D loss: 0.001728] [G loss: 27.099657] time: 0:00:35.404710\n",
      "(30, 64, 64, 3)\n",
      "0.6643386\n",
      "[Epoch 0/5] [Batch 219/360] [D loss: 0.002811] [G loss: 26.660940] time: 0:00:35.538256\n",
      "(30, 64, 64, 3)\n",
      "0.67138857\n",
      "[Epoch 0/5] [Batch 220/360] [D loss: 0.002305] [G loss: 27.191927] time: 0:00:35.670013\n",
      "(30, 64, 64, 3)\n",
      "0.68937856\n",
      "[Epoch 0/5] [Batch 221/360] [D loss: 0.001836] [G loss: 27.247166] time: 0:00:35.796925\n",
      "(30, 64, 64, 3)\n",
      "0.7032791\n",
      "[Epoch 0/5] [Batch 222/360] [D loss: 0.002305] [G loss: 26.441463] time: 0:00:35.925314\n",
      "(30, 64, 64, 3)\n",
      "0.67489195\n",
      "[Epoch 0/5] [Batch 223/360] [D loss: 0.002286] [G loss: 26.723188] time: 0:00:36.051964\n",
      "(30, 64, 64, 3)\n",
      "0.68287164\n",
      "[Epoch 0/5] [Batch 224/360] [D loss: 0.002305] [G loss: 27.085663] time: 0:00:36.183041\n",
      "(30, 64, 64, 3)\n",
      "0.7149697\n",
      "[Epoch 0/5] [Batch 225/360] [D loss: 0.004256] [G loss: 27.145430] time: 0:00:36.316272\n",
      "(30, 64, 64, 3)\n",
      "0.7170544\n",
      "[Epoch 0/5] [Batch 226/360] [D loss: 0.003472] [G loss: 27.248152] time: 0:00:36.448791\n",
      "(30, 64, 64, 3)\n",
      "0.7325792\n",
      "[Epoch 0/5] [Batch 227/360] [D loss: 0.005853] [G loss: 26.971466] time: 0:00:36.579661\n",
      "(30, 64, 64, 3)\n",
      "0.68411463\n",
      "[Epoch 0/5] [Batch 228/360] [D loss: 0.003223] [G loss: 26.608824] time: 0:00:36.714407\n",
      "(30, 64, 64, 3)\n",
      "0.66090304\n",
      "[Epoch 0/5] [Batch 229/360] [D loss: 0.004050] [G loss: 26.714851] time: 0:00:36.890457\n",
      "(30, 64, 64, 3)\n",
      "0.68166846\n",
      "[Epoch 0/5] [Batch 230/360] [D loss: 0.003202] [G loss: 26.839085] time: 0:00:37.020575\n",
      "(30, 64, 64, 3)\n",
      "0.70851535\n",
      "[Epoch 0/5] [Batch 231/360] [D loss: 0.004260] [G loss: 26.791662] time: 0:00:37.158869\n",
      "(30, 64, 64, 3)\n",
      "0.6884885\n",
      "[Epoch 0/5] [Batch 232/360] [D loss: 0.003702] [G loss: 27.089565] time: 0:00:37.299308\n",
      "(30, 64, 64, 3)\n",
      "0.69059306\n",
      "[Epoch 0/5] [Batch 233/360] [D loss: 0.007789] [G loss: 26.611057] time: 0:00:37.430608\n",
      "(30, 64, 64, 3)\n",
      "0.7064968\n",
      "[Epoch 0/5] [Batch 234/360] [D loss: 0.005119] [G loss: 26.677036] time: 0:00:37.556624\n",
      "(30, 64, 64, 3)\n",
      "0.6960604\n",
      "[Epoch 0/5] [Batch 235/360] [D loss: 0.010613] [G loss: 26.218475] time: 0:00:37.683092\n",
      "(30, 64, 64, 3)\n",
      "0.64310247\n",
      "[Epoch 0/5] [Batch 236/360] [D loss: 0.006316] [G loss: 26.129604] time: 0:00:37.810590\n",
      "(30, 64, 64, 3)\n",
      "0.65199697\n",
      "[Epoch 0/5] [Batch 237/360] [D loss: 0.007238] [G loss: 26.270006] time: 0:00:37.946361\n",
      "(30, 64, 64, 3)\n",
      "0.685548\n",
      "[Epoch 0/5] [Batch 238/360] [D loss: 0.004951] [G loss: 26.080990] time: 0:00:38.075855\n",
      "(30, 64, 64, 3)\n",
      "0.67106724\n",
      "[Epoch 0/5] [Batch 239/360] [D loss: 0.005538] [G loss: 26.747105] time: 0:00:38.205435\n",
      "(30, 64, 64, 3)\n",
      "0.69601566\n",
      "[Epoch 0/5] [Batch 240/360] [D loss: 0.004394] [G loss: 26.815273] time: 0:00:38.336515\n",
      "(30, 64, 64, 3)\n",
      "0.64207786\n",
      "[Epoch 0/5] [Batch 241/360] [D loss: 0.010952] [G loss: 26.352278] time: 0:00:38.470999\n",
      "(30, 64, 64, 3)\n",
      "0.6704739\n",
      "[Epoch 0/5] [Batch 242/360] [D loss: 0.006618] [G loss: 26.571714] time: 0:00:38.620866\n",
      "(30, 64, 64, 3)\n",
      "0.71914196\n",
      "[Epoch 0/5] [Batch 243/360] [D loss: 0.010498] [G loss: 26.280903] time: 0:00:38.753145\n",
      "(30, 64, 64, 3)\n",
      "0.7326676\n",
      "[Epoch 0/5] [Batch 244/360] [D loss: 0.007352] [G loss: 26.653940] time: 0:00:38.879418\n",
      "(30, 64, 64, 3)\n",
      "0.6614087\n",
      "[Epoch 0/5] [Batch 245/360] [D loss: 0.010594] [G loss: 26.565599] time: 0:00:39.016702\n",
      "(30, 64, 64, 3)\n",
      "0.66500205\n",
      "[Epoch 0/5] [Batch 246/360] [D loss: 0.004362] [G loss: 26.330070] time: 0:00:39.148317\n",
      "(30, 64, 64, 3)\n",
      "0.6758445\n",
      "[Epoch 0/5] [Batch 247/360] [D loss: 0.004641] [G loss: 26.019524] time: 0:00:39.279874\n",
      "(30, 64, 64, 3)\n",
      "0.6733265\n",
      "[Epoch 0/5] [Batch 248/360] [D loss: 0.002452] [G loss: 26.718870] time: 0:00:39.413016\n",
      "(30, 64, 64, 3)\n",
      "0.6361552\n",
      "[Epoch 0/5] [Batch 249/360] [D loss: 0.002432] [G loss: 26.320375] time: 0:00:39.551141\n",
      "(30, 64, 64, 3)\n",
      "0.67511874\n",
      "[Epoch 0/5] [Batch 250/360] [D loss: 0.001513] [G loss: 26.178862] time: 0:00:39.693176\n",
      "(30, 64, 64, 3)\n",
      "0.6870857\n",
      "[Epoch 0/5] [Batch 251/360] [D loss: 0.002210] [G loss: 26.535381] time: 0:00:39.828549\n",
      "(30, 64, 64, 3)\n",
      "0.715425\n",
      "[Epoch 0/5] [Batch 252/360] [D loss: 0.002173] [G loss: 26.189837] time: 0:00:39.958839\n",
      "(30, 64, 64, 3)\n",
      "0.7201202\n",
      "[Epoch 0/5] [Batch 253/360] [D loss: 0.002744] [G loss: 25.703070] time: 0:00:40.092973\n",
      "(30, 64, 64, 3)\n",
      "0.7097957\n",
      "[Epoch 0/5] [Batch 254/360] [D loss: 0.001832] [G loss: 26.026678] time: 0:00:40.228262\n",
      "(30, 64, 64, 3)\n",
      "0.7068245\n",
      "[Epoch 0/5] [Batch 255/360] [D loss: 0.001828] [G loss: 25.939085] time: 0:00:40.367232\n",
      "(30, 64, 64, 3)\n",
      "0.7021942\n",
      "[Epoch 0/5] [Batch 256/360] [D loss: 0.002696] [G loss: 26.089235] time: 0:00:40.500787\n",
      "(30, 64, 64, 3)\n",
      "0.6561371\n",
      "[Epoch 0/5] [Batch 257/360] [D loss: 0.002956] [G loss: 25.571663] time: 0:00:40.633153\n",
      "(30, 64, 64, 3)\n",
      "0.66966695\n",
      "[Epoch 0/5] [Batch 258/360] [D loss: 0.001512] [G loss: 26.019350] time: 0:00:40.761330\n",
      "(30, 64, 64, 3)\n",
      "0.6974125\n",
      "[Epoch 0/5] [Batch 259/360] [D loss: 0.001577] [G loss: 26.410908] time: 0:00:40.893876\n",
      "(30, 64, 64, 3)\n",
      "0.68089485\n",
      "[Epoch 0/5] [Batch 260/360] [D loss: 0.001925] [G loss: 25.874224] time: 0:00:41.021567\n",
      "(30, 64, 64, 3)\n",
      "0.7053769\n",
      "[Epoch 0/5] [Batch 261/360] [D loss: 0.002296] [G loss: 25.731163] time: 0:00:41.159125\n",
      "(30, 64, 64, 3)\n",
      "0.71613055\n",
      "[Epoch 0/5] [Batch 262/360] [D loss: 0.001589] [G loss: 25.885624] time: 0:00:41.289430\n",
      "(30, 64, 64, 3)\n",
      "0.7003407\n",
      "[Epoch 0/5] [Batch 263/360] [D loss: 0.001916] [G loss: 25.837215] time: 0:00:41.428455\n",
      "(30, 64, 64, 3)\n",
      "0.7354889\n",
      "[Epoch 0/5] [Batch 264/360] [D loss: 0.001842] [G loss: 26.017757] time: 0:00:41.569845\n",
      "(30, 64, 64, 3)\n",
      "0.6878588\n",
      "[Epoch 0/5] [Batch 265/360] [D loss: 0.001461] [G loss: 25.833107] time: 0:00:41.701288\n",
      "(30, 64, 64, 3)\n",
      "0.695107\n",
      "[Epoch 0/5] [Batch 266/360] [D loss: 0.001823] [G loss: 25.753462] time: 0:00:41.831073\n",
      "(30, 64, 64, 3)\n",
      "0.66562265\n",
      "[Epoch 0/5] [Batch 267/360] [D loss: 0.002670] [G loss: 25.310575] time: 0:00:41.959393\n",
      "(30, 64, 64, 3)\n",
      "0.6850181\n",
      "[Epoch 0/5] [Batch 268/360] [D loss: 0.001651] [G loss: 26.043951] time: 0:00:42.094157\n",
      "(30, 64, 64, 3)\n",
      "0.65130764\n",
      "[Epoch 0/5] [Batch 269/360] [D loss: 0.001905] [G loss: 25.574154] time: 0:00:42.232178\n",
      "(30, 64, 64, 3)\n",
      "0.7158692\n",
      "[Epoch 0/5] [Batch 270/360] [D loss: 0.001638] [G loss: 25.631016] time: 0:00:42.405197\n",
      "(30, 64, 64, 3)\n",
      "0.67202646\n",
      "[Epoch 0/5] [Batch 271/360] [D loss: 0.001832] [G loss: 25.938332] time: 0:00:42.538369\n",
      "(30, 64, 64, 3)\n",
      "0.69111204\n",
      "[Epoch 0/5] [Batch 272/360] [D loss: 0.001891] [G loss: 25.453623] time: 0:00:42.669325\n",
      "(30, 64, 64, 3)\n",
      "0.707283\n",
      "[Epoch 0/5] [Batch 273/360] [D loss: 0.002717] [G loss: 25.611147] time: 0:00:42.802713\n",
      "(30, 64, 64, 3)\n",
      "0.7124109\n",
      "[Epoch 0/5] [Batch 274/360] [D loss: 0.001757] [G loss: 25.829847] time: 0:00:42.936778\n",
      "(30, 64, 64, 3)\n",
      "0.73657733\n",
      "[Epoch 0/5] [Batch 275/360] [D loss: 0.004826] [G loss: 25.066790] time: 0:00:43.069046\n",
      "(30, 64, 64, 3)\n",
      "0.68262386\n",
      "[Epoch 0/5] [Batch 276/360] [D loss: 0.002267] [G loss: 25.273783] time: 0:00:43.196009\n",
      "(30, 64, 64, 3)\n",
      "0.7069599\n",
      "[Epoch 0/5] [Batch 277/360] [D loss: 0.002511] [G loss: 25.539173] time: 0:00:43.328264\n",
      "(30, 64, 64, 3)\n",
      "0.6982934\n",
      "[Epoch 0/5] [Batch 278/360] [D loss: 0.001946] [G loss: 25.085615] time: 0:00:43.459206\n",
      "(30, 64, 64, 3)\n",
      "0.6893485\n",
      "[Epoch 0/5] [Batch 279/360] [D loss: 0.002038] [G loss: 25.412066] time: 0:00:43.589429\n",
      "(30, 64, 64, 3)\n",
      "0.7167797\n",
      "[Epoch 0/5] [Batch 280/360] [D loss: 0.002059] [G loss: 25.738220] time: 0:00:43.716311\n",
      "(30, 64, 64, 3)\n",
      "0.72393245\n",
      "[Epoch 0/5] [Batch 281/360] [D loss: 0.002036] [G loss: 25.036682] time: 0:00:43.852274\n",
      "(30, 64, 64, 3)\n",
      "0.718918\n",
      "[Epoch 0/5] [Batch 282/360] [D loss: 0.001284] [G loss: 25.208172] time: 0:00:43.982884\n",
      "(30, 64, 64, 3)\n",
      "0.7240405\n",
      "[Epoch 0/5] [Batch 283/360] [D loss: 0.001336] [G loss: 25.467987] time: 0:00:44.112902\n",
      "(30, 64, 64, 3)\n",
      "0.71289897\n",
      "[Epoch 0/5] [Batch 284/360] [D loss: 0.002186] [G loss: 25.322598] time: 0:00:44.241192\n",
      "(30, 64, 64, 3)\n",
      "0.7306916\n",
      "[Epoch 0/5] [Batch 285/360] [D loss: 0.001956] [G loss: 25.164909] time: 0:00:44.367227\n",
      "(30, 64, 64, 3)\n",
      "0.70360726\n",
      "[Epoch 0/5] [Batch 286/360] [D loss: 0.001827] [G loss: 25.210787] time: 0:00:44.518153\n",
      "(30, 64, 64, 3)\n",
      "0.70428544\n",
      "[Epoch 0/5] [Batch 287/360] [D loss: 0.002030] [G loss: 25.191961] time: 0:00:44.645789\n",
      "(30, 64, 64, 3)\n",
      "0.72394013\n",
      "[Epoch 0/5] [Batch 288/360] [D loss: 0.002176] [G loss: 25.403059] time: 0:00:44.772260\n",
      "(30, 64, 64, 3)\n",
      "0.744126\n",
      "[Epoch 0/5] [Batch 289/360] [D loss: 0.003420] [G loss: 25.454388] time: 0:00:44.896654\n",
      "(30, 64, 64, 3)\n",
      "0.70688087\n",
      "[Epoch 0/5] [Batch 290/360] [D loss: 0.003120] [G loss: 25.157694] time: 0:00:45.046780\n",
      "(30, 64, 64, 3)\n",
      "0.72838753\n",
      "[Epoch 0/5] [Batch 291/360] [D loss: 0.003983] [G loss: 25.504999] time: 0:00:45.180735\n",
      "(30, 64, 64, 3)\n",
      "0.72481704\n",
      "[Epoch 0/5] [Batch 292/360] [D loss: 0.003190] [G loss: 24.901680] time: 0:00:45.313454\n",
      "(30, 64, 64, 3)\n",
      "0.72901845\n",
      "[Epoch 0/5] [Batch 293/360] [D loss: 0.003955] [G loss: 25.019442] time: 0:00:45.438812\n",
      "(30, 64, 64, 3)\n",
      "0.7086952\n",
      "[Epoch 0/5] [Batch 294/360] [D loss: 0.002131] [G loss: 25.235991] time: 0:00:45.568717\n",
      "(30, 64, 64, 3)\n",
      "0.6865371\n",
      "[Epoch 0/5] [Batch 295/360] [D loss: 0.004783] [G loss: 25.293880] time: 0:00:45.703965\n",
      "(30, 64, 64, 3)\n",
      "0.72449017\n",
      "[Epoch 0/5] [Batch 296/360] [D loss: 0.004712] [G loss: 24.998211] time: 0:00:45.832144\n",
      "(30, 64, 64, 3)\n",
      "0.7258518\n",
      "[Epoch 0/5] [Batch 297/360] [D loss: 0.009090] [G loss: 24.476355] time: 0:00:45.962283\n",
      "(30, 64, 64, 3)\n",
      "0.7164772\n",
      "[Epoch 0/5] [Batch 298/360] [D loss: 0.005907] [G loss: 25.088987] time: 0:00:46.097720\n",
      "(30, 64, 64, 3)\n",
      "0.70886534\n",
      "[Epoch 0/5] [Batch 299/360] [D loss: 0.010650] [G loss: 24.799709] time: 0:00:46.231880\n",
      "(30, 64, 64, 3)\n",
      "0.7046127\n",
      "[Epoch 0/5] [Batch 300/360] [D loss: 0.007766] [G loss: 25.088747] time: 0:00:46.365966\n",
      "(30, 64, 64, 3)\n",
      "0.70676804\n",
      "[Epoch 0/5] [Batch 301/360] [D loss: 0.017639] [G loss: 24.901449] time: 0:00:46.491969\n",
      "(30, 64, 64, 3)\n",
      "0.70679903\n",
      "[Epoch 0/5] [Batch 302/360] [D loss: 0.007869] [G loss: 25.080662] time: 0:00:46.617442\n",
      "(30, 64, 64, 3)\n",
      "0.71770865\n",
      "[Epoch 0/5] [Batch 303/360] [D loss: 0.018798] [G loss: 24.374203] time: 0:00:46.758332\n",
      "(30, 64, 64, 3)\n",
      "0.71598345\n",
      "[Epoch 0/5] [Batch 304/360] [D loss: 0.009869] [G loss: 25.026852] time: 0:00:46.894508\n",
      "(30, 64, 64, 3)\n",
      "0.7055697\n",
      "[Epoch 0/5] [Batch 305/360] [D loss: 0.016777] [G loss: 24.764965] time: 0:00:47.021989\n",
      "(30, 64, 64, 3)\n",
      "0.7101117\n",
      "[Epoch 0/5] [Batch 306/360] [D loss: 0.008449] [G loss: 24.527494] time: 0:00:47.153302\n",
      "(30, 64, 64, 3)\n",
      "0.74330777\n",
      "[Epoch 0/5] [Batch 307/360] [D loss: 0.012036] [G loss: 24.593294] time: 0:00:47.282682\n",
      "(30, 64, 64, 3)\n",
      "0.72203\n",
      "[Epoch 0/5] [Batch 308/360] [D loss: 0.004796] [G loss: 24.641226] time: 0:00:47.413620\n",
      "(30, 64, 64, 3)\n",
      "0.70996237\n",
      "[Epoch 0/5] [Batch 309/360] [D loss: 0.009059] [G loss: 24.485924] time: 0:00:47.549344\n",
      "(30, 64, 64, 3)\n",
      "0.70345044\n",
      "[Epoch 0/5] [Batch 310/360] [D loss: 0.005091] [G loss: 24.752630] time: 0:00:47.684624\n",
      "(30, 64, 64, 3)\n",
      "0.71414405\n",
      "[Epoch 0/5] [Batch 311/360] [D loss: 0.002779] [G loss: 25.053354] time: 0:00:47.830657\n",
      "(30, 64, 64, 3)\n",
      "0.7292946\n",
      "[Epoch 0/5] [Batch 312/360] [D loss: 0.002173] [G loss: 24.953339] time: 0:00:47.959199\n",
      "(30, 64, 64, 3)\n",
      "0.7051842\n",
      "[Epoch 0/5] [Batch 313/360] [D loss: 0.003907] [G loss: 24.255909] time: 0:00:48.093082\n",
      "(30, 64, 64, 3)\n",
      "0.7303748\n",
      "[Epoch 0/5] [Batch 314/360] [D loss: 0.002202] [G loss: 24.419649] time: 0:00:48.221889\n",
      "(30, 64, 64, 3)\n",
      "0.6964944\n",
      "[Epoch 0/5] [Batch 315/360] [D loss: 0.002611] [G loss: 24.099033] time: 0:00:48.359927\n",
      "(30, 64, 64, 3)\n",
      "0.7257759\n",
      "[Epoch 0/5] [Batch 316/360] [D loss: 0.002535] [G loss: 24.354191] time: 0:00:48.491611\n",
      "(30, 64, 64, 3)\n",
      "0.7099884\n",
      "[Epoch 0/5] [Batch 317/360] [D loss: 0.003383] [G loss: 24.444143] time: 0:00:48.619686\n",
      "(30, 64, 64, 3)\n",
      "0.7135279\n",
      "[Epoch 0/5] [Batch 318/360] [D loss: 0.001512] [G loss: 24.294445] time: 0:00:48.759160\n",
      "(30, 64, 64, 3)\n",
      "0.67858934\n",
      "[Epoch 0/5] [Batch 319/360] [D loss: 0.002911] [G loss: 24.273268] time: 0:00:48.888674\n",
      "(30, 64, 64, 3)\n",
      "0.70050025\n",
      "[Epoch 0/5] [Batch 320/360] [D loss: 0.002071] [G loss: 25.025211] time: 0:00:49.016698\n",
      "(30, 64, 64, 3)\n",
      "0.72271115\n",
      "[Epoch 0/5] [Batch 321/360] [D loss: 0.001775] [G loss: 24.613901] time: 0:00:49.138470\n",
      "(30, 64, 64, 3)\n",
      "0.7052415\n",
      "[Epoch 0/5] [Batch 322/360] [D loss: 0.002193] [G loss: 24.266020] time: 0:00:49.276662\n",
      "(30, 64, 64, 3)\n",
      "0.7004748\n",
      "[Epoch 0/5] [Batch 323/360] [D loss: 0.002292] [G loss: 23.881689] time: 0:00:49.409057\n",
      "(30, 64, 64, 3)\n",
      "0.71263045\n",
      "[Epoch 0/5] [Batch 324/360] [D loss: 0.002278] [G loss: 24.212488] time: 0:00:49.537607\n",
      "(30, 64, 64, 3)\n",
      "0.70239717\n",
      "[Epoch 0/5] [Batch 325/360] [D loss: 0.002816] [G loss: 23.943600] time: 0:00:49.669047\n",
      "(30, 64, 64, 3)\n",
      "0.719485\n",
      "[Epoch 0/5] [Batch 326/360] [D loss: 0.001627] [G loss: 23.897020] time: 0:00:49.795265\n",
      "(30, 64, 64, 3)\n",
      "0.7268839\n",
      "[Epoch 0/5] [Batch 327/360] [D loss: 0.001892] [G loss: 24.321039] time: 0:00:49.927734\n",
      "(30, 64, 64, 3)\n",
      "0.6939437\n",
      "[Epoch 0/5] [Batch 328/360] [D loss: 0.001215] [G loss: 24.307465] time: 0:00:50.058781\n",
      "(30, 64, 64, 3)\n",
      "0.7276413\n",
      "[Epoch 0/5] [Batch 329/360] [D loss: 0.002194] [G loss: 23.964033] time: 0:00:50.220229\n",
      "(30, 64, 64, 3)\n",
      "0.70400304\n",
      "[Epoch 0/5] [Batch 330/360] [D loss: 0.002272] [G loss: 23.798519] time: 0:00:50.378277\n",
      "(30, 64, 64, 3)\n",
      "0.66377354\n",
      "[Epoch 0/5] [Batch 331/360] [D loss: 0.001922] [G loss: 23.820587] time: 0:00:50.510768\n",
      "(30, 64, 64, 3)\n",
      "0.69149375\n",
      "[Epoch 0/5] [Batch 332/360] [D loss: 0.001348] [G loss: 23.834454] time: 0:00:50.639722\n",
      "(30, 64, 64, 3)\n",
      "0.72138834\n",
      "[Epoch 0/5] [Batch 333/360] [D loss: 0.002238] [G loss: 23.939924] time: 0:00:50.769438\n",
      "(30, 64, 64, 3)\n",
      "0.7696498\n",
      "[Epoch 0/5] [Batch 334/360] [D loss: 0.001279] [G loss: 24.489241] time: 0:00:50.893728\n",
      "(30, 64, 64, 3)\n",
      "0.7205966\n",
      "[Epoch 0/5] [Batch 335/360] [D loss: 0.001427] [G loss: 23.650358] time: 0:00:51.020010\n",
      "(30, 64, 64, 3)\n",
      "0.7083687\n",
      "[Epoch 0/5] [Batch 336/360] [D loss: 0.000940] [G loss: 24.199984] time: 0:00:51.150472\n",
      "(30, 64, 64, 3)\n",
      "0.70403415\n",
      "[Epoch 0/5] [Batch 337/360] [D loss: 0.001442] [G loss: 23.844116] time: 0:00:51.274212\n",
      "(30, 64, 64, 3)\n",
      "0.7310721\n",
      "[Epoch 0/5] [Batch 338/360] [D loss: 0.001380] [G loss: 23.817148] time: 0:00:51.404035\n",
      "(30, 64, 64, 3)\n",
      "0.6556476\n",
      "[Epoch 0/5] [Batch 339/360] [D loss: 0.001153] [G loss: 23.944580] time: 0:00:51.548252\n",
      "(30, 64, 64, 3)\n",
      "0.71197724\n",
      "[Epoch 0/5] [Batch 340/360] [D loss: 0.000960] [G loss: 23.461756] time: 0:00:51.677875\n",
      "(30, 64, 64, 3)\n",
      "0.7084667\n",
      "[Epoch 0/5] [Batch 341/360] [D loss: 0.001626] [G loss: 23.540018] time: 0:00:51.810365\n",
      "(30, 64, 64, 3)\n",
      "0.7228956\n",
      "[Epoch 0/5] [Batch 342/360] [D loss: 0.001229] [G loss: 23.527863] time: 0:00:51.942759\n",
      "(30, 64, 64, 3)\n",
      "0.7154735\n",
      "[Epoch 0/5] [Batch 343/360] [D loss: 0.001571] [G loss: 23.991917] time: 0:00:52.069501\n",
      "(30, 64, 64, 3)\n",
      "0.7168322\n",
      "[Epoch 0/5] [Batch 344/360] [D loss: 0.001713] [G loss: 23.466200] time: 0:00:52.194249\n",
      "(30, 64, 64, 3)\n",
      "0.72785014\n",
      "[Epoch 0/5] [Batch 345/360] [D loss: 0.001095] [G loss: 23.659399] time: 0:00:52.329766\n",
      "(30, 64, 64, 3)\n",
      "0.7387871\n",
      "[Epoch 0/5] [Batch 346/360] [D loss: 0.002046] [G loss: 23.532375] time: 0:00:52.465579\n",
      "(30, 64, 64, 3)\n",
      "0.7467351\n",
      "[Epoch 0/5] [Batch 347/360] [D loss: 0.002571] [G loss: 23.965172] time: 0:00:52.630693\n",
      "(30, 64, 64, 3)\n",
      "0.7206952\n",
      "[Epoch 0/5] [Batch 348/360] [D loss: 0.001329] [G loss: 23.665560] time: 0:00:52.761004\n",
      "(30, 64, 64, 3)\n",
      "0.7262569\n",
      "[Epoch 0/5] [Batch 349/360] [D loss: 0.001357] [G loss: 23.567949] time: 0:00:52.895994\n",
      "(30, 64, 64, 3)\n",
      "0.69617957\n",
      "[Epoch 0/5] [Batch 350/360] [D loss: 0.001554] [G loss: 23.057203] time: 0:00:53.021722\n",
      "(30, 64, 64, 3)\n",
      "0.707634\n",
      "[Epoch 0/5] [Batch 351/360] [D loss: 0.001863] [G loss: 23.068733] time: 0:00:53.186212\n",
      "(30, 64, 64, 3)\n",
      "0.73472214\n",
      "[Epoch 0/5] [Batch 352/360] [D loss: 0.001308] [G loss: 23.722893] time: 0:00:53.317353\n",
      "(30, 64, 64, 3)\n",
      "0.754272\n",
      "[Epoch 0/5] [Batch 353/360] [D loss: 0.002193] [G loss: 23.414579] time: 0:00:53.447647\n",
      "(30, 64, 64, 3)\n",
      "0.71572894\n",
      "[Epoch 0/5] [Batch 354/360] [D loss: 0.001958] [G loss: 23.189812] time: 0:00:53.578384\n",
      "(30, 64, 64, 3)\n",
      "0.7080576\n",
      "[Epoch 0/5] [Batch 355/360] [D loss: 0.001856] [G loss: 23.401985] time: 0:00:53.709620\n",
      "(30, 64, 64, 3)\n",
      "0.70181894\n",
      "[Epoch 0/5] [Batch 356/360] [D loss: 0.002388] [G loss: 23.547197] time: 0:00:53.842960\n",
      "(30, 64, 64, 3)\n",
      "0.7022936\n",
      "[Epoch 0/5] [Batch 357/360] [D loss: 0.002137] [G loss: 23.212044] time: 0:00:53.974809\n",
      "(30, 64, 64, 3)\n",
      "0.72224474\n",
      "[Epoch 0/5] [Batch 358/360] [D loss: 0.001444] [G loss: 23.160158] time: 0:00:54.106389\n",
      "(30, 64, 64, 3)\n",
      "0.7315131\n",
      "[Epoch 1/5] [Batch 0/360] [D loss: 0.002525] [G loss: 23.318569] time: 0:00:54.248129\n",
      "(30, 64, 64, 3)\n",
      "0.7226637\n",
      "[Epoch 1/5] [Batch 1/360] [D loss: 0.002743] [G loss: 23.193804] time: 0:00:54.367831\n",
      "(30, 64, 64, 3)\n",
      "0.7233766\n",
      "[Epoch 1/5] [Batch 2/360] [D loss: 0.004004] [G loss: 23.062492] time: 0:00:54.488900\n",
      "(30, 64, 64, 3)\n",
      "0.6794693\n",
      "[Epoch 1/5] [Batch 3/360] [D loss: 0.003171] [G loss: 22.698195] time: 0:00:54.618608\n",
      "(30, 64, 64, 3)\n",
      "0.6728421\n",
      "[Epoch 1/5] [Batch 4/360] [D loss: 0.004389] [G loss: 23.470329] time: 0:00:54.742462\n",
      "(30, 64, 64, 3)\n",
      "0.7199219\n",
      "[Epoch 1/5] [Batch 5/360] [D loss: 0.002489] [G loss: 23.623394] time: 0:00:54.862454\n",
      "(30, 64, 64, 3)\n",
      "0.7193219\n",
      "[Epoch 1/5] [Batch 6/360] [D loss: 0.005020] [G loss: 22.924044] time: 0:00:54.990463\n",
      "(30, 64, 64, 3)\n",
      "0.70415807\n",
      "[Epoch 1/5] [Batch 7/360] [D loss: 0.004089] [G loss: 22.729683] time: 0:00:55.105341\n",
      "(30, 64, 64, 3)\n",
      "0.7053654\n",
      "[Epoch 1/5] [Batch 8/360] [D loss: 0.005603] [G loss: 22.801521] time: 0:00:55.224742\n",
      "(30, 64, 64, 3)\n",
      "0.7429457\n",
      "[Epoch 1/5] [Batch 9/360] [D loss: 0.003101] [G loss: 22.615879] time: 0:00:55.346895\n",
      "(30, 64, 64, 3)\n",
      "0.6872628\n",
      "[Epoch 1/5] [Batch 10/360] [D loss: 0.003076] [G loss: 23.353279] time: 0:00:55.467220\n",
      "(30, 64, 64, 3)\n",
      "0.7068414\n",
      "[Epoch 1/5] [Batch 11/360] [D loss: 0.002333] [G loss: 22.693674] time: 0:00:55.589169\n",
      "(30, 64, 64, 3)\n",
      "0.7036666\n",
      "[Epoch 1/5] [Batch 12/360] [D loss: 0.005175] [G loss: 23.123627] time: 0:00:55.714542\n",
      "(30, 64, 64, 3)\n",
      "0.7246861\n",
      "[Epoch 1/5] [Batch 13/360] [D loss: 0.002578] [G loss: 23.162889] time: 0:00:55.833882\n",
      "(30, 64, 64, 3)\n",
      "0.70818776\n",
      "[Epoch 1/5] [Batch 14/360] [D loss: 0.002877] [G loss: 22.928869] time: 0:00:55.956412\n",
      "(30, 64, 64, 3)\n",
      "0.68489695\n",
      "[Epoch 1/5] [Batch 15/360] [D loss: 0.002941] [G loss: 22.756397] time: 0:00:56.073676\n",
      "(30, 64, 64, 3)\n",
      "0.74940324\n",
      "[Epoch 1/5] [Batch 16/360] [D loss: 0.005525] [G loss: 22.400419] time: 0:00:56.196627\n",
      "(30, 64, 64, 3)\n",
      "0.7673858\n",
      "[Epoch 1/5] [Batch 17/360] [D loss: 0.003351] [G loss: 23.149101] time: 0:00:56.316495\n",
      "(30, 64, 64, 3)\n",
      "0.7114832\n",
      "[Epoch 1/5] [Batch 18/360] [D loss: 0.004692] [G loss: 22.521444] time: 0:00:56.440451\n",
      "(30, 64, 64, 3)\n",
      "0.7268477\n",
      "[Epoch 1/5] [Batch 19/360] [D loss: 0.003770] [G loss: 22.670275] time: 0:00:56.557211\n",
      "(30, 64, 64, 3)\n",
      "0.70326376\n",
      "[Epoch 1/5] [Batch 20/360] [D loss: 0.006854] [G loss: 22.623030] time: 0:00:56.672382\n",
      "(30, 64, 64, 3)\n",
      "0.74574375\n",
      "[Epoch 1/5] [Batch 21/360] [D loss: 0.004480] [G loss: 23.176867] time: 0:00:56.785582\n",
      "(30, 64, 64, 3)\n",
      "0.7010272\n",
      "[Epoch 1/5] [Batch 22/360] [D loss: 0.007795] [G loss: 22.575569] time: 0:00:56.910465\n",
      "(30, 64, 64, 3)\n",
      "0.7289254\n",
      "[Epoch 1/5] [Batch 23/360] [D loss: 0.003782] [G loss: 22.268946] time: 0:00:57.029226\n",
      "(30, 64, 64, 3)\n",
      "0.765394\n",
      "[Epoch 1/5] [Batch 24/360] [D loss: 0.006341] [G loss: 22.730953] time: 0:00:57.156972\n",
      "(30, 64, 64, 3)\n",
      "0.7522902\n",
      "[Epoch 1/5] [Batch 25/360] [D loss: 0.004861] [G loss: 22.632214] time: 0:00:57.274692\n",
      "(30, 64, 64, 3)\n",
      "0.7410793\n",
      "[Epoch 1/5] [Batch 26/360] [D loss: 0.006460] [G loss: 22.402527] time: 0:00:57.396833\n",
      "(30, 64, 64, 3)\n",
      "0.73232985\n",
      "[Epoch 1/5] [Batch 27/360] [D loss: 0.005515] [G loss: 22.631981] time: 0:00:57.514086\n",
      "(30, 64, 64, 3)\n",
      "0.73079044\n",
      "[Epoch 1/5] [Batch 28/360] [D loss: 0.011734] [G loss: 22.355410] time: 0:00:57.638537\n",
      "(30, 64, 64, 3)\n",
      "0.733545\n",
      "[Epoch 1/5] [Batch 29/360] [D loss: 0.007508] [G loss: 22.241877] time: 0:00:57.758890\n",
      "(30, 64, 64, 3)\n",
      "0.7422915\n",
      "[Epoch 1/5] [Batch 30/360] [D loss: 0.017599] [G loss: 21.789732] time: 0:00:57.883174\n",
      "(30, 64, 64, 3)\n",
      "0.75438434\n",
      "[Epoch 1/5] [Batch 31/360] [D loss: 0.007237] [G loss: 22.182253] time: 0:00:58.008347\n",
      "(30, 64, 64, 3)\n",
      "0.75162715\n",
      "[Epoch 1/5] [Batch 32/360] [D loss: 0.012953] [G loss: 22.258930] time: 0:00:58.130982\n",
      "(30, 64, 64, 3)\n",
      "0.744615\n",
      "[Epoch 1/5] [Batch 33/360] [D loss: 0.009612] [G loss: 22.407032] time: 0:00:58.250578\n",
      "(30, 64, 64, 3)\n",
      "0.73008436\n",
      "[Epoch 1/5] [Batch 34/360] [D loss: 0.023314] [G loss: 21.819735] time: 0:00:58.370055\n",
      "(30, 64, 64, 3)\n",
      "0.74238735\n",
      "[Epoch 1/5] [Batch 35/360] [D loss: 0.009736] [G loss: 22.598951] time: 0:00:58.490400\n",
      "(30, 64, 64, 3)\n",
      "0.7449525\n",
      "[Epoch 1/5] [Batch 36/360] [D loss: 0.019615] [G loss: 22.138079] time: 0:00:58.608000\n",
      "(30, 64, 64, 3)\n",
      "0.75853586\n",
      "[Epoch 1/5] [Batch 37/360] [D loss: 0.010600] [G loss: 22.204430] time: 0:00:58.719214\n",
      "(30, 64, 64, 3)\n",
      "0.7326688\n",
      "[Epoch 1/5] [Batch 38/360] [D loss: 0.015436] [G loss: 22.262531] time: 0:00:58.836658\n",
      "(30, 64, 64, 3)\n",
      "0.7110382\n",
      "[Epoch 1/5] [Batch 39/360] [D loss: 0.003978] [G loss: 22.335424] time: 0:00:58.951251\n",
      "(30, 64, 64, 3)\n",
      "0.70712596\n",
      "[Epoch 1/5] [Batch 40/360] [D loss: 0.004081] [G loss: 22.019445] time: 0:00:59.072877\n",
      "(30, 64, 64, 3)\n",
      "0.718715\n",
      "[Epoch 1/5] [Batch 41/360] [D loss: 0.001620] [G loss: 21.775372] time: 0:00:59.191207\n",
      "(30, 64, 64, 3)\n",
      "0.72929955\n",
      "[Epoch 1/5] [Batch 42/360] [D loss: 0.001763] [G loss: 22.444309] time: 0:00:59.308475\n",
      "(30, 64, 64, 3)\n",
      "0.759898\n",
      "[Epoch 1/5] [Batch 43/360] [D loss: 0.001534] [G loss: 21.861467] time: 0:00:59.429197\n",
      "(30, 64, 64, 3)\n",
      "0.6767767\n",
      "[Epoch 1/5] [Batch 44/360] [D loss: 0.001297] [G loss: 22.423927] time: 0:00:59.555726\n",
      "(30, 64, 64, 3)\n",
      "0.743079\n",
      "[Epoch 1/5] [Batch 45/360] [D loss: 0.001347] [G loss: 21.922138] time: 0:00:59.676259\n",
      "(30, 64, 64, 3)\n",
      "0.7376943\n",
      "[Epoch 1/5] [Batch 46/360] [D loss: 0.000929] [G loss: 22.352617] time: 0:00:59.794171\n",
      "(30, 64, 64, 3)\n",
      "0.74729186\n",
      "[Epoch 1/5] [Batch 47/360] [D loss: 0.001108] [G loss: 21.917154] time: 0:00:59.914034\n",
      "(30, 64, 64, 3)\n",
      "0.73387647\n",
      "[Epoch 1/5] [Batch 48/360] [D loss: 0.001064] [G loss: 22.220873] time: 0:01:00.035725\n",
      "(30, 64, 64, 3)\n",
      "0.7371009\n",
      "[Epoch 1/5] [Batch 49/360] [D loss: 0.001490] [G loss: 21.966946] time: 0:01:00.160551\n",
      "(30, 64, 64, 3)\n",
      "0.7180422\n",
      "[Epoch 1/5] [Batch 50/360] [D loss: 0.001046] [G loss: 21.846447] time: 0:01:00.278269\n",
      "(30, 64, 64, 3)\n",
      "0.7176623\n",
      "[Epoch 1/5] [Batch 51/360] [D loss: 0.001791] [G loss: 21.723515] time: 0:01:00.402787\n",
      "(30, 64, 64, 3)\n",
      "0.7405715\n",
      "[Epoch 1/5] [Batch 52/360] [D loss: 0.001080] [G loss: 22.420931] time: 0:01:00.519437\n",
      "(30, 64, 64, 3)\n",
      "0.72248626\n",
      "[Epoch 1/5] [Batch 53/360] [D loss: 0.001988] [G loss: 21.763577] time: 0:01:00.642777\n",
      "(30, 64, 64, 3)\n",
      "0.7346046\n",
      "[Epoch 1/5] [Batch 54/360] [D loss: 0.000990] [G loss: 22.201803] time: 0:01:00.768664\n",
      "(30, 64, 64, 3)\n",
      "0.7453964\n",
      "[Epoch 1/5] [Batch 55/360] [D loss: 0.002233] [G loss: 21.723995] time: 0:01:00.887043\n",
      "(30, 64, 64, 3)\n",
      "0.69703704\n",
      "[Epoch 1/5] [Batch 56/360] [D loss: 0.001756] [G loss: 22.087175] time: 0:01:01.010911\n",
      "(30, 64, 64, 3)\n",
      "0.7752614\n",
      "[Epoch 1/5] [Batch 57/360] [D loss: 0.001063] [G loss: 21.534403] time: 0:01:01.127937\n",
      "(30, 64, 64, 3)\n",
      "0.7551498\n",
      "[Epoch 1/5] [Batch 58/360] [D loss: 0.001379] [G loss: 21.642248] time: 0:01:01.254796\n",
      "(30, 64, 64, 3)\n",
      "0.7308216\n",
      "[Epoch 1/5] [Batch 59/360] [D loss: 0.001500] [G loss: 21.466240] time: 0:01:01.375256\n",
      "(30, 64, 64, 3)\n",
      "0.73172903\n",
      "[Epoch 1/5] [Batch 60/360] [D loss: 0.001961] [G loss: 21.708879] time: 0:01:01.493531\n",
      "(30, 64, 64, 3)\n",
      "0.69180435\n",
      "[Epoch 1/5] [Batch 61/360] [D loss: 0.001875] [G loss: 21.642113] time: 0:01:01.612542\n",
      "(30, 64, 64, 3)\n",
      "0.7220209\n",
      "[Epoch 1/5] [Batch 62/360] [D loss: 0.001487] [G loss: 21.619665] time: 0:01:01.735916\n",
      "(30, 64, 64, 3)\n",
      "0.7524337\n",
      "[Epoch 1/5] [Batch 63/360] [D loss: 0.000943] [G loss: 21.146984] time: 0:01:01.856341\n",
      "(30, 64, 64, 3)\n",
      "0.7447011\n",
      "[Epoch 1/5] [Batch 64/360] [D loss: 0.001433] [G loss: 21.490129] time: 0:01:01.976237\n",
      "(30, 64, 64, 3)\n",
      "0.71378464\n",
      "[Epoch 1/5] [Batch 65/360] [D loss: 0.001816] [G loss: 21.064873] time: 0:01:02.096180\n",
      "(30, 64, 64, 3)\n",
      "0.72014266\n",
      "[Epoch 1/5] [Batch 66/360] [D loss: 0.001902] [G loss: 21.331018] time: 0:01:02.217684\n",
      "(30, 64, 64, 3)\n",
      "0.6988599\n",
      "[Epoch 1/5] [Batch 67/360] [D loss: 0.001593] [G loss: 21.601383] time: 0:01:02.338735\n",
      "(30, 64, 64, 3)\n",
      "0.74222165\n",
      "[Epoch 1/5] [Batch 68/360] [D loss: 0.001753] [G loss: 21.413897] time: 0:01:02.453973\n",
      "(30, 64, 64, 3)\n",
      "0.7616842\n",
      "[Epoch 1/5] [Batch 69/360] [D loss: 0.001702] [G loss: 21.697260] time: 0:01:02.572873\n",
      "(30, 64, 64, 3)\n",
      "0.73094195\n",
      "[Epoch 1/5] [Batch 70/360] [D loss: 0.001450] [G loss: 20.665575] time: 0:01:02.697482\n",
      "(30, 64, 64, 3)\n",
      "0.77224857\n",
      "[Epoch 1/5] [Batch 71/360] [D loss: 0.001454] [G loss: 21.581320] time: 0:01:02.827350\n",
      "(30, 64, 64, 3)\n",
      "0.71916693\n",
      "[Epoch 1/5] [Batch 72/360] [D loss: 0.001130] [G loss: 21.350056] time: 0:01:02.954438\n",
      "(30, 64, 64, 3)\n",
      "0.7362337\n",
      "[Epoch 1/5] [Batch 73/360] [D loss: 0.001657] [G loss: 21.319967] time: 0:01:03.077826\n",
      "(30, 64, 64, 3)\n",
      "0.71275216\n",
      "[Epoch 1/5] [Batch 74/360] [D loss: 0.001608] [G loss: 21.664883] time: 0:01:03.196458\n",
      "(30, 64, 64, 3)\n",
      "0.76445514\n",
      "[Epoch 1/5] [Batch 75/360] [D loss: 0.002000] [G loss: 21.356428] time: 0:01:03.313492\n",
      "(30, 64, 64, 3)\n",
      "0.7257295\n",
      "[Epoch 1/5] [Batch 76/360] [D loss: 0.001408] [G loss: 21.495728] time: 0:01:03.436485\n",
      "(30, 64, 64, 3)\n",
      "0.76782465\n",
      "[Epoch 1/5] [Batch 77/360] [D loss: 0.002470] [G loss: 21.437788] time: 0:01:03.550797\n",
      "(30, 64, 64, 3)\n",
      "0.74215555\n",
      "[Epoch 1/5] [Batch 78/360] [D loss: 0.002467] [G loss: 21.217091] time: 0:01:03.664524\n",
      "(30, 64, 64, 3)\n",
      "0.7201322\n",
      "[Epoch 1/5] [Batch 79/360] [D loss: 0.003308] [G loss: 21.168488] time: 0:01:03.776584\n",
      "(30, 64, 64, 3)\n",
      "0.7252474\n",
      "[Epoch 1/5] [Batch 80/360] [D loss: 0.002334] [G loss: 20.940044] time: 0:01:03.892611\n",
      "(30, 64, 64, 3)\n",
      "0.77636427\n",
      "[Epoch 1/5] [Batch 81/360] [D loss: 0.003247] [G loss: 21.234617] time: 0:01:04.015122\n",
      "(30, 64, 64, 3)\n",
      "0.7504199\n",
      "[Epoch 1/5] [Batch 82/360] [D loss: 0.003270] [G loss: 20.916046] time: 0:01:04.136627\n",
      "(30, 64, 64, 3)\n",
      "0.74832135\n",
      "[Epoch 1/5] [Batch 83/360] [D loss: 0.005341] [G loss: 20.973459] time: 0:01:04.261006\n",
      "(30, 64, 64, 3)\n",
      "0.7270951\n",
      "[Epoch 1/5] [Batch 84/360] [D loss: 0.003413] [G loss: 21.039080] time: 0:01:04.378401\n",
      "(30, 64, 64, 3)\n",
      "0.7083094\n",
      "[Epoch 1/5] [Batch 85/360] [D loss: 0.004934] [G loss: 21.064602] time: 0:01:04.503555\n",
      "(30, 64, 64, 3)\n",
      "0.74661773\n",
      "[Epoch 1/5] [Batch 86/360] [D loss: 0.003458] [G loss: 21.150288] time: 0:01:04.629065\n",
      "(30, 64, 64, 3)\n",
      "0.73679787\n",
      "[Epoch 1/5] [Batch 87/360] [D loss: 0.005590] [G loss: 21.422651] time: 0:01:04.748724\n",
      "(30, 64, 64, 3)\n",
      "0.76860285\n",
      "[Epoch 1/5] [Batch 88/360] [D loss: 0.003312] [G loss: 21.172747] time: 0:01:04.881383\n",
      "(30, 64, 64, 3)\n",
      "0.733118\n",
      "[Epoch 1/5] [Batch 89/360] [D loss: 0.004142] [G loss: 21.226017] time: 0:01:05.003045\n",
      "(30, 64, 64, 3)\n",
      "0.7430544\n",
      "[Epoch 1/5] [Batch 90/360] [D loss: 0.003549] [G loss: 21.323175] time: 0:01:05.122105\n",
      "(30, 64, 64, 3)\n",
      "0.760482\n",
      "[Epoch 1/5] [Batch 91/360] [D loss: 0.006624] [G loss: 20.580109] time: 0:01:05.235714\n",
      "(30, 64, 64, 3)\n",
      "0.7670521\n",
      "[Epoch 1/5] [Batch 92/360] [D loss: 0.003578] [G loss: 21.179575] time: 0:01:05.356520\n",
      "(30, 64, 64, 3)\n",
      "0.69221324\n",
      "[Epoch 1/5] [Batch 93/360] [D loss: 0.004963] [G loss: 20.667553] time: 0:01:05.473406\n",
      "(30, 64, 64, 3)\n",
      "0.73534966\n",
      "[Epoch 1/5] [Batch 94/360] [D loss: 0.002483] [G loss: 21.004324] time: 0:01:05.594325\n",
      "(30, 64, 64, 3)\n",
      "0.7496336\n",
      "[Epoch 1/5] [Batch 95/360] [D loss: 0.003757] [G loss: 21.287586] time: 0:01:05.715449\n",
      "(30, 64, 64, 3)\n",
      "0.7714939\n",
      "[Epoch 1/5] [Batch 96/360] [D loss: 0.002026] [G loss: 20.809584] time: 0:01:05.837254\n",
      "(30, 64, 64, 3)\n",
      "0.7811585\n",
      "[Epoch 1/5] [Batch 97/360] [D loss: 0.003668] [G loss: 20.873816] time: 0:01:05.954459\n",
      "(30, 64, 64, 3)\n",
      "0.77341795\n",
      "[Epoch 1/5] [Batch 98/360] [D loss: 0.001803] [G loss: 20.959288] time: 0:01:06.080384\n",
      "(30, 64, 64, 3)\n",
      "0.7027634\n",
      "[Epoch 1/5] [Batch 99/360] [D loss: 0.002563] [G loss: 20.662943] time: 0:01:06.201120\n",
      "(30, 64, 64, 3)\n",
      "0.7420833\n",
      "[Epoch 1/5] [Batch 100/360] [D loss: 0.001909] [G loss: 20.700197] time: 0:01:06.328377\n",
      "(30, 64, 64, 3)\n",
      "0.7760503\n",
      "[Epoch 1/5] [Batch 101/360] [D loss: 0.003929] [G loss: 20.794079] time: 0:01:06.443963\n",
      "(30, 64, 64, 3)\n",
      "0.78928465\n",
      "[Epoch 1/5] [Batch 102/360] [D loss: 0.002797] [G loss: 20.921743] time: 0:01:06.565429\n",
      "(30, 64, 64, 3)\n",
      "0.73563147\n",
      "[Epoch 1/5] [Batch 103/360] [D loss: 0.003859] [G loss: 20.136730] time: 0:01:06.689933\n",
      "(30, 64, 64, 3)\n",
      "0.7147484\n",
      "[Epoch 1/5] [Batch 104/360] [D loss: 0.002294] [G loss: 21.002476] time: 0:01:06.804964\n",
      "(30, 64, 64, 3)\n",
      "0.75500745\n",
      "[Epoch 1/5] [Batch 105/360] [D loss: 0.003409] [G loss: 20.352802] time: 0:01:06.925126\n",
      "(30, 64, 64, 3)\n",
      "0.74769706\n",
      "[Epoch 1/5] [Batch 106/360] [D loss: 0.003015] [G loss: 20.858923] time: 0:01:07.049203\n",
      "(30, 64, 64, 3)\n",
      "0.7465787\n",
      "[Epoch 1/5] [Batch 107/360] [D loss: 0.004377] [G loss: 20.667894] time: 0:01:07.173945\n",
      "(30, 64, 64, 3)\n",
      "0.7816792\n",
      "[Epoch 1/5] [Batch 108/360] [D loss: 0.002570] [G loss: 20.364897] time: 0:01:07.291288\n",
      "(30, 64, 64, 3)\n",
      "0.72692084\n",
      "[Epoch 1/5] [Batch 109/360] [D loss: 0.004088] [G loss: 20.278505] time: 0:01:07.412488\n",
      "(30, 64, 64, 3)\n",
      "0.7797966\n",
      "[Epoch 1/5] [Batch 110/360] [D loss: 0.002531] [G loss: 20.814116] time: 0:01:07.533403\n",
      "(30, 64, 64, 3)\n",
      "0.7410142\n",
      "[Epoch 1/5] [Batch 111/360] [D loss: 0.002581] [G loss: 21.172451] time: 0:01:07.650554\n",
      "(30, 64, 64, 3)\n",
      "0.7282503\n",
      "[Epoch 1/5] [Batch 112/360] [D loss: 0.002273] [G loss: 20.482822] time: 0:01:07.775585\n",
      "(30, 64, 64, 3)\n",
      "0.77926296\n",
      "[Epoch 1/5] [Batch 113/360] [D loss: 0.004784] [G loss: 20.195951] time: 0:01:07.896148\n",
      "(30, 64, 64, 3)\n",
      "0.7278552\n",
      "[Epoch 1/5] [Batch 114/360] [D loss: 0.003270] [G loss: 20.112938] time: 0:01:08.016663\n",
      "(30, 64, 64, 3)\n",
      "0.7429382\n",
      "[Epoch 1/5] [Batch 115/360] [D loss: 0.003848] [G loss: 20.145374] time: 0:01:08.134483\n",
      "(30, 64, 64, 3)\n",
      "0.71868724\n",
      "[Epoch 1/5] [Batch 116/360] [D loss: 0.002706] [G loss: 20.760582] time: 0:01:08.252500\n",
      "(30, 64, 64, 3)\n",
      "0.7546484\n",
      "[Epoch 1/5] [Batch 117/360] [D loss: 0.004070] [G loss: 20.431631] time: 0:01:08.367031\n",
      "(30, 64, 64, 3)\n",
      "0.7411967\n",
      "[Epoch 1/5] [Batch 118/360] [D loss: 0.001900] [G loss: 20.597685] time: 0:01:08.485267\n",
      "(30, 64, 64, 3)\n",
      "0.76577884\n",
      "[Epoch 1/5] [Batch 119/360] [D loss: 0.003244] [G loss: 20.378042] time: 0:01:08.602554\n",
      "(30, 64, 64, 3)\n",
      "0.73184377\n",
      "[Epoch 1/5] [Batch 120/360] [D loss: 0.002351] [G loss: 20.504587] time: 0:01:08.725169\n",
      "(30, 64, 64, 3)\n",
      "0.78046083\n",
      "[Epoch 1/5] [Batch 121/360] [D loss: 0.003395] [G loss: 20.191896] time: 0:01:08.849127\n",
      "(30, 64, 64, 3)\n",
      "0.77343893\n",
      "[Epoch 1/5] [Batch 122/360] [D loss: 0.002876] [G loss: 20.624128] time: 0:01:08.967004\n",
      "(30, 64, 64, 3)\n",
      "0.7540744\n",
      "[Epoch 1/5] [Batch 123/360] [D loss: 0.003697] [G loss: 20.321251] time: 0:01:09.087422\n",
      "(30, 64, 64, 3)\n",
      "0.7465331\n",
      "[Epoch 1/5] [Batch 124/360] [D loss: 0.003259] [G loss: 20.195532] time: 0:01:09.207445\n",
      "(30, 64, 64, 3)\n",
      "0.7573581\n",
      "[Epoch 1/5] [Batch 125/360] [D loss: 0.004274] [G loss: 20.146482] time: 0:01:09.333926\n",
      "(30, 64, 64, 3)\n",
      "0.7758592\n",
      "[Epoch 1/5] [Batch 126/360] [D loss: 0.004137] [G loss: 20.369617] time: 0:01:09.449276\n",
      "(30, 64, 64, 3)\n",
      "0.71426153\n",
      "[Epoch 1/5] [Batch 127/360] [D loss: 0.006881] [G loss: 20.058033] time: 0:01:09.564175\n",
      "(30, 64, 64, 3)\n",
      "0.7446959\n",
      "[Epoch 1/5] [Batch 128/360] [D loss: 0.002713] [G loss: 19.727736] time: 0:01:09.679822\n",
      "(30, 64, 64, 3)\n",
      "0.7707698\n",
      "[Epoch 1/5] [Batch 129/360] [D loss: 0.004546] [G loss: 20.322283] time: 0:01:09.794324\n",
      "(30, 64, 64, 3)\n",
      "0.73786575\n",
      "[Epoch 1/5] [Batch 130/360] [D loss: 0.003544] [G loss: 20.259203] time: 0:01:09.912166\n",
      "(30, 64, 64, 3)\n",
      "0.7498574\n",
      "[Epoch 1/5] [Batch 131/360] [D loss: 0.003922] [G loss: 19.906542] time: 0:01:10.028295\n",
      "(30, 64, 64, 3)\n",
      "0.739419\n",
      "[Epoch 1/5] [Batch 132/360] [D loss: 0.002921] [G loss: 19.926435] time: 0:01:10.143380\n",
      "(30, 64, 64, 3)\n",
      "0.7491116\n",
      "[Epoch 1/5] [Batch 133/360] [D loss: 0.005021] [G loss: 19.755623] time: 0:01:10.258236\n",
      "(30, 64, 64, 3)\n",
      "0.6888116\n",
      "[Epoch 1/5] [Batch 134/360] [D loss: 0.002199] [G loss: 20.404083] time: 0:01:10.381401\n",
      "(30, 64, 64, 3)\n",
      "0.77278143\n",
      "[Epoch 1/5] [Batch 135/360] [D loss: 0.003270] [G loss: 19.858765] time: 0:01:10.499279\n",
      "(30, 64, 64, 3)\n",
      "0.757042\n",
      "[Epoch 1/5] [Batch 136/360] [D loss: 0.003120] [G loss: 19.698122] time: 0:01:10.619574\n",
      "(30, 64, 64, 3)\n",
      "0.7208592\n",
      "[Epoch 1/5] [Batch 137/360] [D loss: 0.005469] [G loss: 19.415550] time: 0:01:10.736689\n",
      "(30, 64, 64, 3)\n",
      "0.79009676\n",
      "[Epoch 1/5] [Batch 138/360] [D loss: 0.003262] [G loss: 20.258961] time: 0:01:10.855896\n",
      "(30, 64, 64, 3)\n",
      "0.7601889\n",
      "[Epoch 1/5] [Batch 139/360] [D loss: 0.003770] [G loss: 19.645018] time: 0:01:10.977933\n",
      "(30, 64, 64, 3)\n",
      "0.7723748\n",
      "[Epoch 1/5] [Batch 140/360] [D loss: 0.002623] [G loss: 19.694513] time: 0:01:11.094577\n",
      "(30, 64, 64, 3)\n",
      "0.79096955\n",
      "[Epoch 1/5] [Batch 141/360] [D loss: 0.003942] [G loss: 19.731972] time: 0:01:11.210021\n",
      "(30, 64, 64, 3)\n",
      "0.7321062\n",
      "[Epoch 1/5] [Batch 142/360] [D loss: 0.002551] [G loss: 19.888794] time: 0:01:11.338168\n",
      "(30, 64, 64, 3)\n",
      "0.7694724\n",
      "[Epoch 1/5] [Batch 143/360] [D loss: 0.003554] [G loss: 19.966364] time: 0:01:11.461259\n",
      "(30, 64, 64, 3)\n",
      "0.7807819\n",
      "[Epoch 1/5] [Batch 144/360] [D loss: 0.003086] [G loss: 19.507284] time: 0:01:11.576474\n",
      "(30, 64, 64, 3)\n",
      "0.6937382\n",
      "[Epoch 1/5] [Batch 145/360] [D loss: 0.005277] [G loss: 19.677393] time: 0:01:11.695831\n",
      "(30, 64, 64, 3)\n",
      "0.7251232\n",
      "[Epoch 1/5] [Batch 146/360] [D loss: 0.003045] [G loss: 19.581692] time: 0:01:11.817450\n",
      "(30, 64, 64, 3)\n",
      "0.76655674\n",
      "[Epoch 1/5] [Batch 147/360] [D loss: 0.003361] [G loss: 19.372820] time: 0:01:11.936414\n",
      "(30, 64, 64, 3)\n",
      "0.80515605\n",
      "[Epoch 1/5] [Batch 148/360] [D loss: 0.001544] [G loss: 19.813292] time: 0:01:12.060986\n",
      "(30, 64, 64, 3)\n",
      "0.7765879\n",
      "[Epoch 1/5] [Batch 149/360] [D loss: 0.002555] [G loss: 19.607744] time: 0:01:12.178651\n",
      "(30, 64, 64, 3)\n",
      "0.748466\n",
      "[Epoch 1/5] [Batch 150/360] [D loss: 0.001351] [G loss: 19.815346] time: 0:01:12.295861\n",
      "(30, 64, 64, 3)\n",
      "0.7477999\n",
      "[Epoch 1/5] [Batch 151/360] [D loss: 0.001100] [G loss: 19.179070] time: 0:01:12.412810\n",
      "(30, 64, 64, 3)\n",
      "0.7805831\n",
      "[Epoch 1/5] [Batch 152/360] [D loss: 0.001514] [G loss: 19.087450] time: 0:01:12.538949\n",
      "(30, 64, 64, 3)\n",
      "0.7444857\n",
      "[Epoch 1/5] [Batch 153/360] [D loss: 0.001356] [G loss: 19.342365] time: 0:01:12.654009\n",
      "(30, 64, 64, 3)\n",
      "0.72842723\n",
      "[Epoch 1/5] [Batch 154/360] [D loss: 0.001684] [G loss: 19.814419] time: 0:01:12.774178\n",
      "(30, 64, 64, 3)\n",
      "0.76509833\n",
      "[Epoch 1/5] [Batch 155/360] [D loss: 0.000900] [G loss: 19.675581] time: 0:01:12.895462\n",
      "(30, 64, 64, 3)\n",
      "0.7457462\n",
      "[Epoch 1/5] [Batch 156/360] [D loss: 0.001151] [G loss: 19.349714] time: 0:01:13.021428\n",
      "(30, 64, 64, 3)\n",
      "0.76935273\n",
      "[Epoch 1/5] [Batch 157/360] [D loss: 0.001057] [G loss: 19.740665] time: 0:01:13.144755\n",
      "(30, 64, 64, 3)\n",
      "0.7644436\n",
      "[Epoch 1/5] [Batch 158/360] [D loss: 0.001151] [G loss: 19.890251] time: 0:01:13.258466\n",
      "(30, 64, 64, 3)\n",
      "0.75495976\n",
      "[Epoch 1/5] [Batch 159/360] [D loss: 0.001241] [G loss: 19.482988] time: 0:01:13.377304\n",
      "(30, 64, 64, 3)\n",
      "0.7735605\n",
      "[Epoch 1/5] [Batch 160/360] [D loss: 0.001037] [G loss: 19.694410] time: 0:01:13.497742\n",
      "(30, 64, 64, 3)\n",
      "0.7331805\n",
      "[Epoch 1/5] [Batch 161/360] [D loss: 0.000927] [G loss: 19.221590] time: 0:01:13.620342\n",
      "(30, 64, 64, 3)\n",
      "0.77995545\n",
      "[Epoch 1/5] [Batch 162/360] [D loss: 0.000834] [G loss: 19.534920] time: 0:01:13.736689\n",
      "(30, 64, 64, 3)\n",
      "0.68603545\n",
      "[Epoch 1/5] [Batch 163/360] [D loss: 0.001437] [G loss: 19.530315] time: 0:01:13.855368\n",
      "(30, 64, 64, 3)\n",
      "0.7362421\n",
      "[Epoch 1/5] [Batch 164/360] [D loss: 0.001690] [G loss: 18.723652] time: 0:01:13.986457\n",
      "(30, 64, 64, 3)\n",
      "0.74517417\n",
      "[Epoch 1/5] [Batch 165/360] [D loss: 0.001744] [G loss: 19.423311] time: 0:01:14.105910\n",
      "(30, 64, 64, 3)\n",
      "0.7620421\n",
      "[Epoch 1/5] [Batch 166/360] [D loss: 0.001227] [G loss: 19.847942] time: 0:01:14.228966\n",
      "(30, 64, 64, 3)\n",
      "0.7723686\n",
      "[Epoch 1/5] [Batch 167/360] [D loss: 0.001453] [G loss: 19.422781] time: 0:01:14.346462\n",
      "(30, 64, 64, 3)\n",
      "0.72649413\n",
      "[Epoch 1/5] [Batch 168/360] [D loss: 0.001524] [G loss: 19.059172] time: 0:01:14.463108\n",
      "(30, 64, 64, 3)\n",
      "0.7211697\n",
      "[Epoch 1/5] [Batch 169/360] [D loss: 0.001033] [G loss: 19.271128] time: 0:01:14.581614\n",
      "(30, 64, 64, 3)\n",
      "0.76194865\n",
      "[Epoch 1/5] [Batch 170/360] [D loss: 0.001380] [G loss: 18.958141] time: 0:01:14.703411\n",
      "(30, 64, 64, 3)\n",
      "0.7406471\n",
      "[Epoch 1/5] [Batch 171/360] [D loss: 0.002658] [G loss: 19.097469] time: 0:01:14.823816\n",
      "(30, 64, 64, 3)\n",
      "0.7449091\n",
      "[Epoch 1/5] [Batch 172/360] [D loss: 0.001132] [G loss: 19.271643] time: 0:01:14.948505\n",
      "(30, 64, 64, 3)\n",
      "0.7665189\n",
      "[Epoch 1/5] [Batch 173/360] [D loss: 0.001550] [G loss: 18.722666] time: 0:01:15.064061\n",
      "(30, 64, 64, 3)\n",
      "0.76154274\n",
      "[Epoch 1/5] [Batch 174/360] [D loss: 0.001145] [G loss: 19.367645] time: 0:01:15.183413\n",
      "(30, 64, 64, 3)\n",
      "0.7636083\n",
      "[Epoch 1/5] [Batch 175/360] [D loss: 0.001754] [G loss: 19.360561] time: 0:01:15.307825\n",
      "(30, 64, 64, 3)\n",
      "0.74661523\n",
      "[Epoch 1/5] [Batch 176/360] [D loss: 0.003132] [G loss: 18.970354] time: 0:01:15.429650\n",
      "(30, 64, 64, 3)\n",
      "0.79287905\n",
      "[Epoch 1/5] [Batch 177/360] [D loss: 0.001624] [G loss: 18.905016] time: 0:01:15.547811\n",
      "(30, 64, 64, 3)\n",
      "0.76450163\n",
      "[Epoch 1/5] [Batch 178/360] [D loss: 0.001670] [G loss: 18.894838] time: 0:01:15.667227\n",
      "(30, 64, 64, 3)\n",
      "0.7582419\n",
      "[Epoch 1/5] [Batch 179/360] [D loss: 0.001089] [G loss: 19.065714] time: 0:01:15.789639\n",
      "(30, 64, 64, 3)\n",
      "0.7748375\n",
      "[Epoch 1/5] [Batch 180/360] [D loss: 0.001260] [G loss: 19.070774] time: 0:01:15.904977\n",
      "(30, 64, 64, 3)\n",
      "0.74779963\n",
      "[Epoch 1/5] [Batch 181/360] [D loss: 0.001523] [G loss: 19.104860] time: 0:01:16.022561\n",
      "(30, 64, 64, 3)\n",
      "0.77560407\n",
      "[Epoch 1/5] [Batch 182/360] [D loss: 0.002168] [G loss: 19.094072] time: 0:01:16.138277\n",
      "(30, 64, 64, 3)\n",
      "0.77378845\n",
      "[Epoch 1/5] [Batch 183/360] [D loss: 0.001614] [G loss: 19.263121] time: 0:01:16.257508\n",
      "(30, 64, 64, 3)\n",
      "0.72045106\n",
      "[Epoch 1/5] [Batch 184/360] [D loss: 0.001144] [G loss: 19.051039] time: 0:01:16.385286\n",
      "(30, 64, 64, 3)\n",
      "0.76190025\n",
      "[Epoch 1/5] [Batch 185/360] [D loss: 0.000951] [G loss: 18.773935] time: 0:01:16.505107\n",
      "(30, 64, 64, 3)\n",
      "0.7536815\n",
      "[Epoch 1/5] [Batch 186/360] [D loss: 0.001974] [G loss: 18.859068] time: 0:01:16.625780\n",
      "(30, 64, 64, 3)\n",
      "0.7406991\n",
      "[Epoch 1/5] [Batch 187/360] [D loss: 0.001807] [G loss: 18.536623] time: 0:01:16.746624\n",
      "(30, 64, 64, 3)\n",
      "0.7732105\n",
      "[Epoch 1/5] [Batch 188/360] [D loss: 0.004647] [G loss: 18.850744] time: 0:01:16.872277\n",
      "(30, 64, 64, 3)\n",
      "0.77436954\n",
      "[Epoch 1/5] [Batch 189/360] [D loss: 0.002995] [G loss: 19.693098] time: 0:01:16.988329\n",
      "(30, 64, 64, 3)\n",
      "0.7311661\n",
      "[Epoch 1/5] [Batch 190/360] [D loss: 0.004739] [G loss: 18.359142] time: 0:01:17.105598\n",
      "(30, 64, 64, 3)\n",
      "0.75871277\n",
      "[Epoch 1/5] [Batch 191/360] [D loss: 0.001382] [G loss: 18.230043] time: 0:01:17.225232\n",
      "(30, 64, 64, 3)\n",
      "0.7495838\n",
      "[Epoch 1/5] [Batch 192/360] [D loss: 0.001706] [G loss: 18.882036] time: 0:01:17.349847\n",
      "(30, 64, 64, 3)\n",
      "0.75951624\n",
      "[Epoch 1/5] [Batch 193/360] [D loss: 0.002627] [G loss: 18.177595] time: 0:01:17.472326\n",
      "(30, 64, 64, 3)\n",
      "0.77186996\n",
      "[Epoch 1/5] [Batch 194/360] [D loss: 0.003925] [G loss: 18.393755] time: 0:01:17.597756\n",
      "(30, 64, 64, 3)\n",
      "0.7343995\n",
      "[Epoch 1/5] [Batch 195/360] [D loss: 0.002797] [G loss: 18.819906] time: 0:01:17.709735\n",
      "(30, 64, 64, 3)\n",
      "0.7293134\n",
      "[Epoch 1/5] [Batch 196/360] [D loss: 0.004207] [G loss: 18.838900] time: 0:01:17.832466\n",
      "(30, 64, 64, 3)\n",
      "0.70348144\n",
      "[Epoch 1/5] [Batch 197/360] [D loss: 0.002886] [G loss: 18.787273] time: 0:01:17.951463\n",
      "(30, 64, 64, 3)\n",
      "0.7400411\n",
      "[Epoch 1/5] [Batch 198/360] [D loss: 0.005167] [G loss: 18.769131] time: 0:01:18.073605\n",
      "(30, 64, 64, 3)\n",
      "0.7421246\n",
      "[Epoch 1/5] [Batch 199/360] [D loss: 0.002811] [G loss: 18.391546] time: 0:01:18.189623\n",
      "(30, 64, 64, 3)\n",
      "0.7404004\n",
      "[Epoch 1/5] [Batch 200/360] [D loss: 0.004499] [G loss: 18.359562] time: 0:01:18.314650\n",
      "(30, 64, 64, 3)\n",
      "0.77920336\n",
      "[Epoch 1/5] [Batch 201/360] [D loss: 0.002717] [G loss: 18.759340] time: 0:01:18.438464\n",
      "(30, 64, 64, 3)\n",
      "0.7530248\n",
      "[Epoch 1/5] [Batch 202/360] [D loss: 0.002420] [G loss: 17.978987] time: 0:01:18.566302\n",
      "(30, 64, 64, 3)\n",
      "0.7645733\n",
      "[Epoch 1/5] [Batch 203/360] [D loss: 0.001727] [G loss: 18.396624] time: 0:01:18.684988\n",
      "(30, 64, 64, 3)\n",
      "0.78080136\n",
      "[Epoch 1/5] [Batch 204/360] [D loss: 0.002203] [G loss: 18.527082] time: 0:01:18.807957\n",
      "(30, 64, 64, 3)\n",
      "0.76878864\n",
      "[Epoch 1/5] [Batch 205/360] [D loss: 0.001571] [G loss: 18.514788] time: 0:01:18.925616\n",
      "(30, 64, 64, 3)\n",
      "0.75916404\n",
      "[Epoch 1/5] [Batch 206/360] [D loss: 0.003363] [G loss: 18.215528] time: 0:01:19.051123\n",
      "(30, 64, 64, 3)\n",
      "0.7423082\n",
      "[Epoch 1/5] [Batch 207/360] [D loss: 0.002583] [G loss: 18.562830] time: 0:01:19.166538\n",
      "(30, 64, 64, 3)\n",
      "0.7628948\n",
      "[Epoch 1/5] [Batch 208/360] [D loss: 0.004709] [G loss: 18.738350] time: 0:01:19.286262\n",
      "(30, 64, 64, 3)\n",
      "0.7086914\n",
      "[Epoch 1/5] [Batch 209/360] [D loss: 0.002985] [G loss: 19.063429] time: 0:01:19.407734\n",
      "(30, 64, 64, 3)\n",
      "0.78584284\n",
      "[Epoch 1/5] [Batch 210/360] [D loss: 0.004374] [G loss: 18.604435] time: 0:01:19.526549\n",
      "(30, 64, 64, 3)\n",
      "0.76152486\n",
      "[Epoch 1/5] [Batch 211/360] [D loss: 0.002994] [G loss: 18.522127] time: 0:01:19.652779\n",
      "(30, 64, 64, 3)\n",
      "0.7581108\n",
      "[Epoch 1/5] [Batch 212/360] [D loss: 0.007391] [G loss: 18.339596] time: 0:01:19.775568\n",
      "(30, 64, 64, 3)\n",
      "0.7451777\n",
      "[Epoch 1/5] [Batch 213/360] [D loss: 0.004397] [G loss: 18.235624] time: 0:01:19.892707\n",
      "(30, 64, 64, 3)\n",
      "0.7719509\n",
      "[Epoch 1/5] [Batch 214/360] [D loss: 0.010763] [G loss: 18.392632] time: 0:01:20.012497\n",
      "(30, 64, 64, 3)\n",
      "0.78676206\n",
      "[Epoch 1/5] [Batch 215/360] [D loss: 0.005927] [G loss: 18.159998] time: 0:01:20.138356\n",
      "(30, 64, 64, 3)\n",
      "0.7991039\n",
      "[Epoch 1/5] [Batch 216/360] [D loss: 0.014002] [G loss: 17.973278] time: 0:01:20.260668\n",
      "(30, 64, 64, 3)\n",
      "0.77249503\n",
      "[Epoch 1/5] [Batch 217/360] [D loss: 0.004937] [G loss: 18.309387] time: 0:01:20.375221\n",
      "(30, 64, 64, 3)\n",
      "0.77447885\n",
      "[Epoch 1/5] [Batch 218/360] [D loss: 0.027077] [G loss: 17.924761] time: 0:01:20.499171\n",
      "(30, 64, 64, 3)\n",
      "0.7599754\n",
      "[Epoch 1/5] [Batch 219/360] [D loss: 0.007840] [G loss: 18.248142] time: 0:01:20.620175\n",
      "(30, 64, 64, 3)\n",
      "0.693427\n",
      "[Epoch 1/5] [Batch 220/360] [D loss: 0.023571] [G loss: 18.473591] time: 0:01:20.753851\n",
      "(30, 64, 64, 3)\n",
      "0.7385947\n",
      "[Epoch 1/5] [Batch 221/360] [D loss: 0.045774] [G loss: 18.145222] time: 0:01:20.869803\n",
      "(30, 64, 64, 3)\n",
      "0.7310116\n",
      "[Epoch 1/5] [Batch 222/360] [D loss: 0.002782] [G loss: 17.915493] time: 0:01:20.992455\n",
      "(30, 64, 64, 3)\n",
      "0.7738007\n",
      "[Epoch 1/5] [Batch 223/360] [D loss: 0.005365] [G loss: 17.437832] time: 0:01:21.111815\n",
      "(30, 64, 64, 3)\n",
      "0.78779763\n",
      "[Epoch 1/5] [Batch 224/360] [D loss: 0.010668] [G loss: 18.294744] time: 0:01:21.245292\n",
      "(30, 64, 64, 3)\n",
      "0.74744296\n",
      "[Epoch 1/5] [Batch 225/360] [D loss: 0.026859] [G loss: 17.930225] time: 0:01:21.361115\n",
      "(30, 64, 64, 3)\n",
      "0.7832832\n",
      "[Epoch 1/5] [Batch 226/360] [D loss: 0.009546] [G loss: 18.337130] time: 0:01:21.481379\n",
      "(30, 64, 64, 3)\n",
      "0.78835624\n",
      "[Epoch 1/5] [Batch 227/360] [D loss: 0.021331] [G loss: 17.903858] time: 0:01:21.602297\n",
      "(30, 64, 64, 3)\n",
      "0.77581024\n",
      "[Epoch 1/5] [Batch 228/360] [D loss: 0.009684] [G loss: 18.633673] time: 0:01:21.722078\n",
      "(30, 64, 64, 3)\n",
      "0.7298983\n",
      "[Epoch 1/5] [Batch 229/360] [D loss: 0.014496] [G loss: 17.598858] time: 0:01:21.840545\n",
      "(30, 64, 64, 3)\n",
      "0.80422825\n",
      "[Epoch 1/5] [Batch 230/360] [D loss: 0.003016] [G loss: 17.889471] time: 0:01:21.961343\n",
      "(30, 64, 64, 3)\n",
      "0.78872204\n",
      "[Epoch 1/5] [Batch 231/360] [D loss: 0.002621] [G loss: 17.770729] time: 0:01:22.078374\n",
      "(30, 64, 64, 3)\n",
      "0.76892596\n",
      "[Epoch 1/5] [Batch 232/360] [D loss: 0.001473] [G loss: 17.526567] time: 0:01:22.202027\n",
      "(30, 64, 64, 3)\n",
      "0.7588861\n",
      "[Epoch 1/5] [Batch 233/360] [D loss: 0.001924] [G loss: 17.686821] time: 0:01:22.326041\n",
      "(30, 64, 64, 3)\n",
      "0.75733924\n",
      "[Epoch 1/5] [Batch 234/360] [D loss: 0.001064] [G loss: 17.707060] time: 0:01:22.441011\n",
      "(30, 64, 64, 3)\n",
      "0.77301145\n",
      "[Epoch 1/5] [Batch 235/360] [D loss: 0.001069] [G loss: 17.642130] time: 0:01:22.561250\n",
      "(30, 64, 64, 3)\n",
      "0.7836621\n",
      "[Epoch 1/5] [Batch 236/360] [D loss: 0.001016] [G loss: 18.346922] time: 0:01:22.678585\n",
      "(30, 64, 64, 3)\n",
      "0.7665437\n",
      "[Epoch 1/5] [Batch 237/360] [D loss: 0.002760] [G loss: 17.936590] time: 0:01:22.792044\n",
      "(30, 64, 64, 3)\n",
      "0.78904694\n",
      "[Epoch 1/5] [Batch 238/360] [D loss: 0.001741] [G loss: 17.838037] time: 0:01:22.917174\n",
      "(30, 64, 64, 3)\n",
      "0.80458456\n",
      "[Epoch 1/5] [Batch 239/360] [D loss: 0.001071] [G loss: 17.704493] time: 0:01:23.038156\n",
      "(30, 64, 64, 3)\n",
      "0.77713853\n",
      "[Epoch 1/5] [Batch 240/360] [D loss: 0.001776] [G loss: 17.692053] time: 0:01:23.158410\n",
      "(30, 64, 64, 3)\n",
      "0.80405474\n",
      "[Epoch 1/5] [Batch 241/360] [D loss: 0.001602] [G loss: 17.298077] time: 0:01:23.278307\n",
      "(30, 64, 64, 3)\n",
      "0.7671449\n",
      "[Epoch 1/5] [Batch 242/360] [D loss: 0.000841] [G loss: 17.959625] time: 0:01:23.397354\n",
      "(30, 64, 64, 3)\n",
      "0.7775548\n",
      "[Epoch 1/5] [Batch 243/360] [D loss: 0.001347] [G loss: 17.683020] time: 0:01:23.518631\n",
      "(30, 64, 64, 3)\n",
      "0.76989913\n",
      "[Epoch 1/5] [Batch 244/360] [D loss: 0.001084] [G loss: 17.815332] time: 0:01:23.642594\n",
      "(30, 64, 64, 3)\n",
      "0.7665464\n",
      "[Epoch 1/5] [Batch 245/360] [D loss: 0.001199] [G loss: 17.859571] time: 0:01:23.759167\n",
      "(30, 64, 64, 3)\n",
      "0.7896471\n",
      "[Epoch 1/5] [Batch 246/360] [D loss: 0.001227] [G loss: 17.482832] time: 0:01:23.881860\n",
      "(30, 64, 64, 3)\n",
      "0.8126011\n",
      "[Epoch 1/5] [Batch 247/360] [D loss: 0.001118] [G loss: 17.719126] time: 0:01:24.004503\n",
      "(30, 64, 64, 3)\n",
      "0.74930596\n",
      "[Epoch 1/5] [Batch 248/360] [D loss: 0.001153] [G loss: 17.114786] time: 0:01:24.126489\n",
      "(30, 64, 64, 3)\n",
      "0.78478605\n",
      "[Epoch 1/5] [Batch 249/360] [D loss: 0.001447] [G loss: 17.464561] time: 0:01:24.246144\n",
      "(30, 64, 64, 3)\n",
      "0.8061667\n",
      "[Epoch 1/5] [Batch 250/360] [D loss: 0.001470] [G loss: 17.537771] time: 0:01:24.364062\n",
      "(30, 64, 64, 3)\n",
      "0.7760572\n",
      "[Epoch 1/5] [Batch 251/360] [D loss: 0.001298] [G loss: 17.442793] time: 0:01:24.488333\n",
      "(30, 64, 64, 3)\n",
      "0.7972105\n",
      "[Epoch 1/5] [Batch 252/360] [D loss: 0.001723] [G loss: 17.301460] time: 0:01:24.604865\n",
      "(30, 64, 64, 3)\n",
      "0.7695362\n",
      "[Epoch 1/5] [Batch 253/360] [D loss: 0.001883] [G loss: 17.400450] time: 0:01:24.723201\n",
      "(30, 64, 64, 3)\n",
      "0.76706773\n",
      "[Epoch 1/5] [Batch 254/360] [D loss: 0.001899] [G loss: 17.704470] time: 0:01:24.841665\n",
      "(30, 64, 64, 3)\n",
      "0.8059165\n",
      "[Epoch 1/5] [Batch 255/360] [D loss: 0.001263] [G loss: 17.243931] time: 0:01:24.972677\n",
      "(30, 64, 64, 3)\n",
      "0.7472183\n",
      "[Epoch 1/5] [Batch 256/360] [D loss: 0.001318] [G loss: 17.074873] time: 0:01:25.096056\n",
      "(30, 64, 64, 3)\n",
      "0.7849842\n",
      "[Epoch 1/5] [Batch 257/360] [D loss: 0.001038] [G loss: 17.126753] time: 0:01:25.210546\n",
      "(30, 64, 64, 3)\n",
      "0.7854751\n",
      "[Epoch 1/5] [Batch 258/360] [D loss: 0.001452] [G loss: 17.147625] time: 0:01:25.329670\n",
      "(30, 64, 64, 3)\n",
      "0.7706313\n",
      "[Epoch 1/5] [Batch 259/360] [D loss: 0.001223] [G loss: 17.135077] time: 0:01:25.447923\n",
      "(30, 64, 64, 3)\n",
      "0.7723477\n",
      "[Epoch 1/5] [Batch 260/360] [D loss: 0.001191] [G loss: 17.435789] time: 0:01:25.578727\n",
      "(30, 64, 64, 3)\n",
      "0.7831743\n",
      "[Epoch 1/5] [Batch 261/360] [D loss: 0.000997] [G loss: 17.438852] time: 0:01:25.698560\n",
      "(30, 64, 64, 3)\n",
      "0.76457566\n",
      "[Epoch 1/5] [Batch 262/360] [D loss: 0.001401] [G loss: 17.207766] time: 0:01:25.817407\n",
      "(30, 64, 64, 3)\n",
      "0.76244545\n",
      "[Epoch 1/5] [Batch 263/360] [D loss: 0.000811] [G loss: 16.643278] time: 0:01:25.932391\n",
      "(30, 64, 64, 3)\n",
      "0.7615521\n",
      "[Epoch 1/5] [Batch 264/360] [D loss: 0.001344] [G loss: 17.218184] time: 0:01:26.060701\n",
      "(30, 64, 64, 3)\n",
      "0.721718\n",
      "[Epoch 1/5] [Batch 265/360] [D loss: 0.000928] [G loss: 16.865892] time: 0:01:26.182539\n",
      "(30, 64, 64, 3)\n",
      "0.7881925\n",
      "[Epoch 1/5] [Batch 266/360] [D loss: 0.001382] [G loss: 17.122601] time: 0:01:26.305562\n",
      "(30, 64, 64, 3)\n",
      "0.7482538\n",
      "[Epoch 1/5] [Batch 267/360] [D loss: 0.000760] [G loss: 17.085371] time: 0:01:26.423932\n",
      "(30, 64, 64, 3)\n",
      "0.8001582\n",
      "[Epoch 1/5] [Batch 268/360] [D loss: 0.001282] [G loss: 16.912720] time: 0:01:26.540567\n",
      "(30, 64, 64, 3)\n",
      "0.7585986\n",
      "[Epoch 1/5] [Batch 269/360] [D loss: 0.001462] [G loss: 17.399454] time: 0:01:26.664136\n",
      "(30, 64, 64, 3)\n",
      "0.78552985\n",
      "[Epoch 1/5] [Batch 270/360] [D loss: 0.000936] [G loss: 17.439184] time: 0:01:26.925265\n",
      "(30, 64, 64, 3)\n",
      "0.8331782\n",
      "[Epoch 1/5] [Batch 271/360] [D loss: 0.001489] [G loss: 17.163149] time: 0:01:27.048359\n",
      "(30, 64, 64, 3)\n",
      "0.7893036\n",
      "[Epoch 1/5] [Batch 272/360] [D loss: 0.000885] [G loss: 17.124983] time: 0:01:27.166057\n",
      "(30, 64, 64, 3)\n",
      "0.71066475\n",
      "[Epoch 1/5] [Batch 273/360] [D loss: 0.001423] [G loss: 17.042549] time: 0:01:27.281788\n",
      "(30, 64, 64, 3)\n",
      "0.7822266\n",
      "[Epoch 1/5] [Batch 274/360] [D loss: 0.000969] [G loss: 16.694786] time: 0:01:27.409261\n",
      "(30, 64, 64, 3)\n",
      "0.7944439\n",
      "[Epoch 1/5] [Batch 275/360] [D loss: 0.001469] [G loss: 17.076744] time: 0:01:27.527368\n",
      "(30, 64, 64, 3)\n",
      "0.7773339\n",
      "[Epoch 1/5] [Batch 276/360] [D loss: 0.001031] [G loss: 16.671158] time: 0:01:27.657169\n",
      "(30, 64, 64, 3)\n",
      "0.759666\n",
      "[Epoch 1/5] [Batch 277/360] [D loss: 0.001030] [G loss: 16.960739] time: 0:01:27.772224\n",
      "(30, 64, 64, 3)\n",
      "0.7593766\n",
      "[Epoch 1/5] [Batch 278/360] [D loss: 0.000890] [G loss: 16.552181] time: 0:01:27.898222\n",
      "(30, 64, 64, 3)\n",
      "0.7363555\n",
      "[Epoch 1/5] [Batch 279/360] [D loss: 0.000967] [G loss: 17.047632] time: 0:01:28.015231\n",
      "(30, 64, 64, 3)\n",
      "0.79547375\n",
      "[Epoch 1/5] [Batch 280/360] [D loss: 0.001134] [G loss: 17.311403] time: 0:01:28.139594\n",
      "(30, 64, 64, 3)\n",
      "0.7892259\n",
      "[Epoch 1/5] [Batch 281/360] [D loss: 0.001107] [G loss: 16.784496] time: 0:01:28.256455\n",
      "(30, 64, 64, 3)\n",
      "0.79557085\n",
      "[Epoch 1/5] [Batch 282/360] [D loss: 0.001422] [G loss: 16.762711] time: 0:01:28.375804\n",
      "(30, 64, 64, 3)\n",
      "0.77979904\n",
      "[Epoch 1/5] [Batch 283/360] [D loss: 0.000819] [G loss: 17.220490] time: 0:01:28.500794\n",
      "(30, 64, 64, 3)\n",
      "0.7508634\n",
      "[Epoch 1/5] [Batch 284/360] [D loss: 0.001321] [G loss: 17.065258] time: 0:01:28.618565\n",
      "(30, 64, 64, 3)\n",
      "0.7929123\n",
      "[Epoch 1/5] [Batch 285/360] [D loss: 0.001225] [G loss: 16.875027] time: 0:01:28.741011\n",
      "(30, 64, 64, 3)\n",
      "0.7692189\n",
      "[Epoch 1/5] [Batch 286/360] [D loss: 0.001031] [G loss: 16.420700] time: 0:01:28.858200\n",
      "(30, 64, 64, 3)\n",
      "0.79940015\n",
      "[Epoch 1/5] [Batch 287/360] [D loss: 0.000990] [G loss: 17.177492] time: 0:01:28.984977\n",
      "(30, 64, 64, 3)\n",
      "0.8287714\n",
      "[Epoch 1/5] [Batch 288/360] [D loss: 0.001034] [G loss: 16.801403] time: 0:01:29.101763\n",
      "(30, 64, 64, 3)\n",
      "0.77519614\n",
      "[Epoch 1/5] [Batch 289/360] [D loss: 0.000975] [G loss: 16.491165] time: 0:01:29.220756\n",
      "(30, 64, 64, 3)\n",
      "0.80283254\n",
      "[Epoch 1/5] [Batch 290/360] [D loss: 0.000971] [G loss: 17.069815] time: 0:01:29.339429\n",
      "(30, 64, 64, 3)\n",
      "0.78251463\n",
      "[Epoch 1/5] [Batch 291/360] [D loss: 0.001269] [G loss: 16.895853] time: 0:01:29.459322\n",
      "(30, 64, 64, 3)\n",
      "0.77344966\n",
      "[Epoch 1/5] [Batch 292/360] [D loss: 0.001127] [G loss: 16.445839] time: 0:01:29.583367\n",
      "(30, 64, 64, 3)\n",
      "0.81051546\n",
      "[Epoch 1/5] [Batch 293/360] [D loss: 0.000859] [G loss: 16.609694] time: 0:01:29.701878\n",
      "(30, 64, 64, 3)\n",
      "0.7801561\n",
      "[Epoch 1/5] [Batch 294/360] [D loss: 0.001249] [G loss: 17.021694] time: 0:01:29.823071\n",
      "(30, 64, 64, 3)\n",
      "0.7660119\n",
      "[Epoch 1/5] [Batch 295/360] [D loss: 0.002118] [G loss: 16.511904] time: 0:01:29.941089\n",
      "(30, 64, 64, 3)\n",
      "0.79058045\n",
      "[Epoch 1/5] [Batch 296/360] [D loss: 0.002683] [G loss: 16.364376] time: 0:01:30.067940\n",
      "(30, 64, 64, 3)\n",
      "0.7405991\n",
      "[Epoch 1/5] [Batch 297/360] [D loss: 0.001096] [G loss: 16.475943] time: 0:01:30.189418\n",
      "(30, 64, 64, 3)\n",
      "0.7704719\n",
      "[Epoch 1/5] [Batch 298/360] [D loss: 0.000816] [G loss: 16.728468] time: 0:01:30.311291\n",
      "(30, 64, 64, 3)\n",
      "0.7721278\n",
      "[Epoch 1/5] [Batch 299/360] [D loss: 0.002380] [G loss: 16.348942] time: 0:01:30.434608\n",
      "(30, 64, 64, 3)\n",
      "0.78439635\n",
      "[Epoch 1/5] [Batch 300/360] [D loss: 0.001356] [G loss: 16.355894] time: 0:01:30.552485\n",
      "(30, 64, 64, 3)\n",
      "0.79391915\n",
      "[Epoch 1/5] [Batch 301/360] [D loss: 0.001420] [G loss: 16.800205] time: 0:01:30.680324\n",
      "(30, 64, 64, 3)\n",
      "0.7767295\n",
      "[Epoch 1/5] [Batch 302/360] [D loss: 0.002037] [G loss: 15.751429] time: 0:01:30.799529\n",
      "(30, 64, 64, 3)\n",
      "0.7973117\n",
      "[Epoch 1/5] [Batch 303/360] [D loss: 0.002793] [G loss: 16.318432] time: 0:01:30.920685\n",
      "(30, 64, 64, 3)\n",
      "0.775352\n",
      "[Epoch 1/5] [Batch 304/360] [D loss: 0.002454] [G loss: 16.466936] time: 0:01:31.044555\n",
      "(30, 64, 64, 3)\n",
      "0.7692812\n",
      "[Epoch 1/5] [Batch 305/360] [D loss: 0.004794] [G loss: 16.280689] time: 0:01:31.165388\n",
      "(30, 64, 64, 3)\n",
      "0.7877259\n",
      "[Epoch 1/5] [Batch 306/360] [D loss: 0.002314] [G loss: 16.645914] time: 0:01:31.286455\n",
      "(30, 64, 64, 3)\n",
      "0.7513199\n",
      "[Epoch 1/5] [Batch 307/360] [D loss: 0.002875] [G loss: 16.693537] time: 0:01:31.402922\n",
      "(30, 64, 64, 3)\n",
      "0.7330664\n",
      "[Epoch 1/5] [Batch 308/360] [D loss: 0.002646] [G loss: 16.719379] time: 0:01:31.526270\n",
      "(30, 64, 64, 3)\n",
      "0.808743\n",
      "[Epoch 1/5] [Batch 309/360] [D loss: 0.004758] [G loss: 16.712215] time: 0:01:31.643287\n",
      "(30, 64, 64, 3)\n",
      "0.799885\n",
      "[Epoch 1/5] [Batch 310/360] [D loss: 0.003639] [G loss: 16.766003] time: 0:01:31.766196\n",
      "(30, 64, 64, 3)\n",
      "0.78380257\n",
      "[Epoch 1/5] [Batch 311/360] [D loss: 0.007302] [G loss: 16.474287] time: 0:01:31.889147\n",
      "(30, 64, 64, 3)\n",
      "0.73499554\n",
      "[Epoch 1/5] [Batch 312/360] [D loss: 0.004486] [G loss: 16.608406] time: 0:01:32.019553\n",
      "(30, 64, 64, 3)\n",
      "0.76178867\n",
      "[Epoch 1/5] [Batch 313/360] [D loss: 0.012741] [G loss: 16.233418] time: 0:01:32.135807\n",
      "(30, 64, 64, 3)\n",
      "0.7820229\n",
      "[Epoch 1/5] [Batch 314/360] [D loss: 0.003518] [G loss: 15.959658] time: 0:01:32.255452\n",
      "(30, 64, 64, 3)\n",
      "0.76082444\n",
      "[Epoch 1/5] [Batch 315/360] [D loss: 0.003475] [G loss: 16.349180] time: 0:01:32.375796\n",
      "(30, 64, 64, 3)\n",
      "0.77583295\n",
      "[Epoch 1/5] [Batch 316/360] [D loss: 0.005996] [G loss: 16.333805] time: 0:01:32.495498\n",
      "(30, 64, 64, 3)\n",
      "0.8216668\n",
      "[Epoch 1/5] [Batch 317/360] [D loss: 0.018184] [G loss: 16.052166] time: 0:01:32.619614\n",
      "(30, 64, 64, 3)\n",
      "0.8118761\n",
      "[Epoch 1/5] [Batch 318/360] [D loss: 0.007837] [G loss: 16.780081] time: 0:01:32.748157\n",
      "(30, 64, 64, 3)\n",
      "0.75060624\n",
      "[Epoch 1/5] [Batch 319/360] [D loss: 0.069743] [G loss: 16.162720] time: 0:01:32.868071\n",
      "(30, 64, 64, 3)\n",
      "0.7818465\n",
      "[Epoch 1/5] [Batch 320/360] [D loss: 0.698647] [G loss: 16.585485] time: 0:01:32.987609\n",
      "(30, 64, 64, 3)\n",
      "0.7875673\n",
      "[Epoch 1/5] [Batch 321/360] [D loss: 0.021905] [G loss: 16.601116] time: 0:01:33.107545\n",
      "(30, 64, 64, 3)\n",
      "0.7815966\n",
      "[Epoch 1/5] [Batch 322/360] [D loss: 0.123601] [G loss: 15.680671] time: 0:01:33.226494\n",
      "(30, 64, 64, 3)\n",
      "0.77248365\n",
      "[Epoch 1/5] [Batch 323/360] [D loss: 0.039591] [G loss: 15.896897] time: 0:01:33.353193\n",
      "(30, 64, 64, 3)\n",
      "0.79655546\n",
      "[Epoch 1/5] [Batch 324/360] [D loss: 0.005025] [G loss: 16.103670] time: 0:01:33.472004\n",
      "(30, 64, 64, 3)\n",
      "0.79185206\n",
      "[Epoch 1/5] [Batch 325/360] [D loss: 0.005515] [G loss: 16.610168] time: 0:01:33.593987\n",
      "(30, 64, 64, 3)\n",
      "0.7753896\n",
      "[Epoch 1/5] [Batch 326/360] [D loss: 0.005384] [G loss: 16.321911] time: 0:01:33.713511\n",
      "(30, 64, 64, 3)\n",
      "0.79694533\n",
      "[Epoch 1/5] [Batch 327/360] [D loss: 0.010065] [G loss: 16.280991] time: 0:01:33.828077\n",
      "(30, 64, 64, 3)\n",
      "0.73158294\n",
      "[Epoch 1/5] [Batch 328/360] [D loss: 0.008739] [G loss: 15.902062] time: 0:01:33.953967\n",
      "(30, 64, 64, 3)\n",
      "0.7356739\n",
      "[Epoch 1/5] [Batch 329/360] [D loss: 0.016175] [G loss: 16.191883] time: 0:01:34.070778\n",
      "(30, 64, 64, 3)\n",
      "0.72725827\n",
      "[Epoch 1/5] [Batch 330/360] [D loss: 0.009170] [G loss: 16.560799] time: 0:01:34.186297\n",
      "(30, 64, 64, 3)\n",
      "0.7877086\n",
      "[Epoch 1/5] [Batch 331/360] [D loss: 0.011034] [G loss: 16.045660] time: 0:01:34.300114\n",
      "(30, 64, 64, 3)\n",
      "0.84238404\n",
      "[Epoch 1/5] [Batch 332/360] [D loss: 0.003285] [G loss: 16.007832] time: 0:01:34.422385\n",
      "(30, 64, 64, 3)\n",
      "0.7919827\n",
      "[Epoch 1/5] [Batch 333/360] [D loss: 0.002563] [G loss: 15.956543] time: 0:01:34.540885\n",
      "(30, 64, 64, 3)\n",
      "0.76526165\n",
      "[Epoch 1/5] [Batch 334/360] [D loss: 0.001622] [G loss: 15.967835] time: 0:01:34.673277\n",
      "(30, 64, 64, 3)\n",
      "0.8143938\n",
      "[Epoch 1/5] [Batch 335/360] [D loss: 0.002644] [G loss: 15.827457] time: 0:01:34.797299\n",
      "(30, 64, 64, 3)\n",
      "0.78052384\n",
      "[Epoch 1/5] [Batch 336/360] [D loss: 0.001474] [G loss: 15.888184] time: 0:01:34.918548\n",
      "(30, 64, 64, 3)\n",
      "0.8080633\n",
      "[Epoch 1/5] [Batch 337/360] [D loss: 0.001405] [G loss: 15.922788] time: 0:01:35.040634\n",
      "(30, 64, 64, 3)\n",
      "0.76782084\n",
      "[Epoch 1/5] [Batch 338/360] [D loss: 0.001755] [G loss: 16.169983] time: 0:01:35.173840\n",
      "(30, 64, 64, 3)\n",
      "0.8032493\n",
      "[Epoch 1/5] [Batch 339/360] [D loss: 0.001973] [G loss: 15.669083] time: 0:01:35.296369\n",
      "(30, 64, 64, 3)\n",
      "0.80110544\n",
      "[Epoch 1/5] [Batch 340/360] [D loss: 0.001632] [G loss: 15.677646] time: 0:01:35.414258\n",
      "(30, 64, 64, 3)\n",
      "0.7729415\n",
      "[Epoch 1/5] [Batch 341/360] [D loss: 0.002312] [G loss: 15.863566] time: 0:01:35.537657\n",
      "(30, 64, 64, 3)\n",
      "0.76815295\n",
      "[Epoch 1/5] [Batch 342/360] [D loss: 0.001585] [G loss: 15.691850] time: 0:01:35.655655\n",
      "(30, 64, 64, 3)\n",
      "0.8114826\n",
      "[Epoch 1/5] [Batch 343/360] [D loss: 0.001414] [G loss: 15.506945] time: 0:01:35.776028\n",
      "(30, 64, 64, 3)\n",
      "0.76661634\n",
      "[Epoch 1/5] [Batch 344/360] [D loss: 0.002664] [G loss: 15.723879] time: 0:01:35.897257\n",
      "(30, 64, 64, 3)\n",
      "0.798673\n",
      "[Epoch 1/5] [Batch 345/360] [D loss: 0.001077] [G loss: 16.099768] time: 0:01:36.019127\n",
      "(30, 64, 64, 3)\n",
      "0.76889443\n",
      "[Epoch 1/5] [Batch 346/360] [D loss: 0.001868] [G loss: 15.459583] time: 0:01:36.141458\n",
      "(30, 64, 64, 3)\n",
      "0.78647995\n",
      "[Epoch 1/5] [Batch 347/360] [D loss: 0.001415] [G loss: 15.723958] time: 0:01:36.257406\n",
      "(30, 64, 64, 3)\n",
      "0.8091054\n",
      "[Epoch 1/5] [Batch 348/360] [D loss: 0.001444] [G loss: 15.578976] time: 0:01:36.375950\n",
      "(30, 64, 64, 3)\n",
      "0.79177904\n",
      "[Epoch 1/5] [Batch 349/360] [D loss: 0.001620] [G loss: 15.715293] time: 0:01:36.494061\n",
      "(30, 64, 64, 3)\n",
      "0.77887124\n",
      "[Epoch 1/5] [Batch 350/360] [D loss: 0.001861] [G loss: 15.187054] time: 0:01:36.622083\n",
      "(30, 64, 64, 3)\n",
      "0.7791405\n",
      "[Epoch 1/5] [Batch 351/360] [D loss: 0.001750] [G loss: 15.622534] time: 0:01:36.747987\n",
      "(30, 64, 64, 3)\n",
      "0.778773\n",
      "[Epoch 1/5] [Batch 352/360] [D loss: 0.001464] [G loss: 15.565844] time: 0:01:36.865323\n",
      "(30, 64, 64, 3)\n",
      "0.82219553\n",
      "[Epoch 1/5] [Batch 353/360] [D loss: 0.001343] [G loss: 15.361913] time: 0:01:36.990408\n",
      "(30, 64, 64, 3)\n",
      "0.80749017\n",
      "[Epoch 1/5] [Batch 354/360] [D loss: 0.001343] [G loss: 15.664861] time: 0:01:37.108685\n",
      "(30, 64, 64, 3)\n",
      "0.79656667\n",
      "[Epoch 1/5] [Batch 355/360] [D loss: 0.001360] [G loss: 15.421711] time: 0:01:37.230745\n",
      "(30, 64, 64, 3)\n",
      "0.7368329\n",
      "[Epoch 1/5] [Batch 356/360] [D loss: 0.001686] [G loss: 15.444577] time: 0:01:37.353736\n",
      "(30, 64, 64, 3)\n",
      "0.7645688\n",
      "[Epoch 1/5] [Batch 357/360] [D loss: 0.001324] [G loss: 16.112631] time: 0:01:37.474074\n",
      "(30, 64, 64, 3)\n",
      "0.81580025\n",
      "[Epoch 1/5] [Batch 358/360] [D loss: 0.001826] [G loss: 15.454701] time: 0:01:37.591178\n",
      "(30, 64, 64, 3)\n",
      "0.7899452\n",
      "[Epoch 1/5] [Batch 359/360] [D loss: 0.001428] [G loss: 15.577633] time: 0:01:37.711204\n",
      "(30, 64, 64, 3)\n",
      "0.799304\n",
      "[Epoch 2/5] [Batch 1/360] [D loss: 0.001865] [G loss: 15.504054] time: 0:01:37.843643\n",
      "(30, 64, 64, 3)\n",
      "0.8150181\n",
      "[Epoch 2/5] [Batch 2/360] [D loss: 0.001601] [G loss: 15.340859] time: 0:01:37.960131\n",
      "(30, 64, 64, 3)\n",
      "0.8134935\n",
      "[Epoch 2/5] [Batch 3/360] [D loss: 0.000990] [G loss: 15.497323] time: 0:01:38.080315\n",
      "(30, 64, 64, 3)\n",
      "0.778794\n",
      "[Epoch 2/5] [Batch 4/360] [D loss: 0.001562] [G loss: 15.863544] time: 0:01:38.198181\n",
      "(30, 64, 64, 3)\n",
      "0.8000961\n",
      "[Epoch 2/5] [Batch 5/360] [D loss: 0.001327] [G loss: 15.344478] time: 0:01:38.314408\n",
      "(30, 64, 64, 3)\n",
      "0.7844293\n",
      "[Epoch 2/5] [Batch 6/360] [D loss: 0.001848] [G loss: 15.578757] time: 0:01:38.438022\n",
      "(30, 64, 64, 3)\n",
      "0.81540537\n",
      "[Epoch 2/5] [Batch 7/360] [D loss: 0.001466] [G loss: 14.977553] time: 0:01:38.558939\n",
      "(30, 64, 64, 3)\n",
      "0.7764811\n",
      "[Epoch 2/5] [Batch 8/360] [D loss: 0.002455] [G loss: 15.252945] time: 0:01:38.676516\n",
      "(30, 64, 64, 3)\n",
      "0.7661896\n",
      "[Epoch 2/5] [Batch 9/360] [D loss: 0.001261] [G loss: 15.258370] time: 0:01:38.797693\n",
      "(30, 64, 64, 3)\n",
      "0.77753526\n",
      "[Epoch 2/5] [Batch 10/360] [D loss: 0.001491] [G loss: 15.971969] time: 0:01:38.915809\n",
      "(30, 64, 64, 3)\n",
      "0.80590373\n",
      "[Epoch 2/5] [Batch 11/360] [D loss: 0.001315] [G loss: 14.817165] time: 0:01:39.034499\n",
      "(30, 64, 64, 3)\n",
      "0.80050945\n",
      "[Epoch 2/5] [Batch 12/360] [D loss: 0.001403] [G loss: 15.144094] time: 0:01:39.150053\n",
      "(30, 64, 64, 3)\n",
      "0.79933405\n",
      "[Epoch 2/5] [Batch 13/360] [D loss: 0.001097] [G loss: 15.326346] time: 0:01:39.287866\n",
      "(30, 64, 64, 3)\n",
      "0.80017406\n",
      "[Epoch 2/5] [Batch 14/360] [D loss: 0.001824] [G loss: 15.207938] time: 0:01:39.413827\n",
      "(30, 64, 64, 3)\n",
      "0.82501215\n",
      "[Epoch 2/5] [Batch 15/360] [D loss: 0.000905] [G loss: 15.242506] time: 0:01:39.533018\n",
      "(30, 64, 64, 3)\n",
      "0.8451689\n",
      "[Epoch 2/5] [Batch 16/360] [D loss: 0.002012] [G loss: 14.894377] time: 0:01:39.656891\n",
      "(30, 64, 64, 3)\n",
      "0.8150365\n",
      "[Epoch 2/5] [Batch 17/360] [D loss: 0.001539] [G loss: 15.488132] time: 0:01:39.776339\n",
      "(30, 64, 64, 3)\n",
      "0.8213437\n",
      "[Epoch 2/5] [Batch 18/360] [D loss: 0.000963] [G loss: 15.772991] time: 0:01:39.895114\n",
      "(30, 64, 64, 3)\n",
      "0.8125794\n",
      "[Epoch 2/5] [Batch 19/360] [D loss: 0.001962] [G loss: 15.443919] time: 0:01:40.019893\n",
      "(30, 64, 64, 3)\n",
      "0.78278285\n",
      "[Epoch 2/5] [Batch 20/360] [D loss: 0.001279] [G loss: 15.341907] time: 0:01:40.139253\n",
      "(30, 64, 64, 3)\n",
      "0.8101468\n",
      "[Epoch 2/5] [Batch 21/360] [D loss: 0.001896] [G loss: 15.053982] time: 0:01:40.261206\n",
      "(30, 64, 64, 3)\n",
      "0.8029173\n",
      "[Epoch 2/5] [Batch 22/360] [D loss: 0.001124] [G loss: 15.151392] time: 0:01:40.380343\n",
      "(30, 64, 64, 3)\n",
      "0.7875738\n",
      "[Epoch 2/5] [Batch 23/360] [D loss: 0.001352] [G loss: 15.012971] time: 0:01:40.500680\n",
      "(30, 64, 64, 3)\n",
      "0.8031109\n",
      "[Epoch 2/5] [Batch 24/360] [D loss: 0.001731] [G loss: 14.679536] time: 0:01:40.627604\n",
      "(30, 64, 64, 3)\n",
      "0.8329676\n",
      "[Epoch 2/5] [Batch 25/360] [D loss: 0.001015] [G loss: 15.210156] time: 0:01:40.752722\n",
      "(30, 64, 64, 3)\n",
      "0.76870036\n",
      "[Epoch 2/5] [Batch 26/360] [D loss: 0.001273] [G loss: 15.066834] time: 0:01:40.867945\n",
      "(30, 64, 64, 3)\n",
      "0.77862555\n",
      "[Epoch 2/5] [Batch 27/360] [D loss: 0.002156] [G loss: 15.133009] time: 0:01:40.991519\n",
      "(30, 64, 64, 3)\n",
      "0.8078244\n",
      "[Epoch 2/5] [Batch 28/360] [D loss: 0.001283] [G loss: 15.261141] time: 0:01:41.109997\n",
      "(30, 64, 64, 3)\n",
      "0.7846846\n",
      "[Epoch 2/5] [Batch 29/360] [D loss: 0.002302] [G loss: 14.884006] time: 0:01:41.227800\n",
      "(30, 64, 64, 3)\n",
      "0.7671296\n",
      "[Epoch 2/5] [Batch 30/360] [D loss: 0.002039] [G loss: 15.715679] time: 0:01:41.351687\n",
      "(30, 64, 64, 3)\n",
      "0.79731756\n",
      "[Epoch 2/5] [Batch 31/360] [D loss: 0.003037] [G loss: 14.631710] time: 0:01:41.469176\n",
      "(30, 64, 64, 3)\n",
      "0.751807\n",
      "[Epoch 2/5] [Batch 32/360] [D loss: 0.002337] [G loss: 15.156815] time: 0:01:41.591513\n",
      "(30, 64, 64, 3)\n",
      "0.7603934\n",
      "[Epoch 2/5] [Batch 33/360] [D loss: 0.002369] [G loss: 14.804118] time: 0:01:41.713185\n",
      "(30, 64, 64, 3)\n",
      "0.7443719\n",
      "[Epoch 2/5] [Batch 34/360] [D loss: 0.003031] [G loss: 14.926254] time: 0:01:41.829742\n",
      "(30, 64, 64, 3)\n",
      "0.75190157\n",
      "[Epoch 2/5] [Batch 35/360] [D loss: 0.002977] [G loss: 14.664973] time: 0:01:41.961992\n",
      "(30, 64, 64, 3)\n",
      "0.85071564\n",
      "[Epoch 2/5] [Batch 36/360] [D loss: 0.001180] [G loss: 14.720579] time: 0:01:42.079749\n",
      "(30, 64, 64, 3)\n",
      "0.77704\n",
      "[Epoch 2/5] [Batch 37/360] [D loss: 0.001149] [G loss: 15.012724] time: 0:01:42.203861\n",
      "(30, 64, 64, 3)\n",
      "0.80160856\n",
      "[Epoch 2/5] [Batch 38/360] [D loss: 0.001980] [G loss: 14.961036] time: 0:01:42.327262\n",
      "(30, 64, 64, 3)\n",
      "0.81655407\n",
      "[Epoch 2/5] [Batch 39/360] [D loss: 0.001610] [G loss: 14.665757] time: 0:01:42.449911\n",
      "(30, 64, 64, 3)\n",
      "0.77287906\n",
      "[Epoch 2/5] [Batch 40/360] [D loss: 0.001248] [G loss: 15.148565] time: 0:01:42.568310\n",
      "(30, 64, 64, 3)\n",
      "0.78989315\n",
      "[Epoch 2/5] [Batch 41/360] [D loss: 0.003055] [G loss: 14.457153] time: 0:01:42.689684\n",
      "(30, 64, 64, 3)\n",
      "0.78523177\n",
      "[Epoch 2/5] [Batch 42/360] [D loss: 0.001736] [G loss: 14.965236] time: 0:01:42.805144\n",
      "(30, 64, 64, 3)\n",
      "0.749198\n",
      "[Epoch 2/5] [Batch 43/360] [D loss: 0.003190] [G loss: 14.630758] time: 0:01:42.927714\n",
      "(30, 64, 64, 3)\n",
      "0.76596594\n",
      "[Epoch 2/5] [Batch 44/360] [D loss: 0.002719] [G loss: 14.480120] time: 0:01:43.044539\n",
      "(30, 64, 64, 3)\n",
      "0.8030321\n",
      "[Epoch 2/5] [Batch 45/360] [D loss: 0.002837] [G loss: 14.590240] time: 0:01:43.164269\n",
      "(30, 64, 64, 3)\n",
      "0.7925768\n",
      "[Epoch 2/5] [Batch 46/360] [D loss: 0.002997] [G loss: 14.922165] time: 0:01:43.275968\n",
      "(30, 64, 64, 3)\n",
      "0.79164606\n",
      "[Epoch 2/5] [Batch 47/360] [D loss: 0.004673] [G loss: 14.895555] time: 0:01:43.396659\n",
      "(30, 64, 64, 3)\n",
      "0.8040208\n",
      "[Epoch 2/5] [Batch 48/360] [D loss: 0.003651] [G loss: 14.858329] time: 0:01:43.518986\n",
      "(30, 64, 64, 3)\n",
      "0.7284086\n",
      "[Epoch 2/5] [Batch 49/360] [D loss: 0.005796] [G loss: 14.894564] time: 0:01:43.646443\n",
      "(30, 64, 64, 3)\n",
      "0.75855565\n",
      "[Epoch 2/5] [Batch 50/360] [D loss: 0.004028] [G loss: 14.499943] time: 0:01:43.768936\n",
      "(30, 64, 64, 3)\n",
      "0.8083539\n",
      "[Epoch 2/5] [Batch 51/360] [D loss: 0.004817] [G loss: 14.207929] time: 0:01:43.889282\n",
      "(30, 64, 64, 3)\n",
      "0.82444364\n",
      "[Epoch 2/5] [Batch 52/360] [D loss: 0.003430] [G loss: 14.889347] time: 0:01:44.009902\n",
      "(30, 64, 64, 3)\n",
      "0.7726538\n",
      "[Epoch 2/5] [Batch 53/360] [D loss: 0.004044] [G loss: 14.448857] time: 0:01:44.126028\n",
      "(30, 64, 64, 3)\n",
      "0.7932704\n",
      "[Epoch 2/5] [Batch 54/360] [D loss: 0.002220] [G loss: 14.600880] time: 0:01:44.247808\n",
      "(30, 64, 64, 3)\n",
      "0.8359456\n",
      "[Epoch 2/5] [Batch 55/360] [D loss: 0.002905] [G loss: 14.719766] time: 0:01:44.365373\n",
      "(30, 64, 64, 3)\n",
      "0.7987216\n",
      "[Epoch 2/5] [Batch 56/360] [D loss: 0.002364] [G loss: 14.370199] time: 0:01:44.485510\n",
      "(30, 64, 64, 3)\n",
      "0.7686486\n",
      "[Epoch 2/5] [Batch 57/360] [D loss: 0.003920] [G loss: 14.032789] time: 0:01:44.605157\n",
      "(30, 64, 64, 3)\n",
      "0.7740337\n",
      "[Epoch 2/5] [Batch 58/360] [D loss: 0.002706] [G loss: 14.506069] time: 0:01:44.722419\n",
      "(30, 64, 64, 3)\n",
      "0.80977565\n",
      "[Epoch 2/5] [Batch 59/360] [D loss: 0.003261] [G loss: 14.490721] time: 0:01:44.848175\n",
      "(30, 64, 64, 3)\n",
      "0.78532296\n",
      "[Epoch 2/5] [Batch 60/360] [D loss: 0.002849] [G loss: 14.815809] time: 0:01:44.966132\n",
      "(30, 64, 64, 3)\n",
      "0.7556097\n",
      "[Epoch 2/5] [Batch 61/360] [D loss: 0.003138] [G loss: 14.420338] time: 0:01:45.090494\n",
      "(30, 64, 64, 3)\n",
      "0.7883771\n",
      "[Epoch 2/5] [Batch 62/360] [D loss: 0.002109] [G loss: 14.700200] time: 0:01:45.205520\n",
      "(30, 64, 64, 3)\n",
      "0.8217158\n",
      "[Epoch 2/5] [Batch 63/360] [D loss: 0.003306] [G loss: 14.251508] time: 0:01:45.330060\n",
      "(30, 64, 64, 3)\n",
      "0.8056912\n",
      "[Epoch 2/5] [Batch 64/360] [D loss: 0.002033] [G loss: 14.236553] time: 0:01:45.456697\n",
      "(30, 64, 64, 3)\n",
      "0.81044227\n",
      "[Epoch 2/5] [Batch 65/360] [D loss: 0.003635] [G loss: 14.806904] time: 0:01:45.578932\n",
      "(30, 64, 64, 3)\n",
      "0.7900922\n",
      "[Epoch 2/5] [Batch 66/360] [D loss: 0.002446] [G loss: 14.233760] time: 0:01:45.694789\n",
      "(30, 64, 64, 3)\n",
      "0.80965453\n",
      "[Epoch 2/5] [Batch 67/360] [D loss: 0.003841] [G loss: 14.259334] time: 0:01:45.820904\n",
      "(30, 64, 64, 3)\n",
      "0.8324165\n",
      "[Epoch 2/5] [Batch 68/360] [D loss: 0.003021] [G loss: 14.512804] time: 0:01:45.946787\n",
      "(30, 64, 64, 3)\n",
      "0.8113045\n",
      "[Epoch 2/5] [Batch 69/360] [D loss: 0.004241] [G loss: 14.513972] time: 0:01:46.064485\n",
      "(30, 64, 64, 3)\n",
      "0.8351021\n",
      "[Epoch 2/5] [Batch 70/360] [D loss: 0.002082] [G loss: 14.597665] time: 0:01:46.182179\n",
      "(30, 64, 64, 3)\n",
      "0.7649395\n",
      "[Epoch 2/5] [Batch 71/360] [D loss: 0.004033] [G loss: 14.248145] time: 0:01:46.307470\n",
      "(30, 64, 64, 3)\n",
      "0.80544895\n",
      "[Epoch 2/5] [Batch 72/360] [D loss: 0.001691] [G loss: 14.560001] time: 0:01:46.431785\n",
      "(30, 64, 64, 3)\n",
      "0.822942\n",
      "[Epoch 2/5] [Batch 73/360] [D loss: 0.003059] [G loss: 14.343644] time: 0:01:46.548007\n",
      "(30, 64, 64, 3)\n",
      "0.79544663\n",
      "[Epoch 2/5] [Batch 74/360] [D loss: 0.001329] [G loss: 13.897043] time: 0:01:46.666680\n",
      "(30, 64, 64, 3)\n",
      "0.8058726\n",
      "[Epoch 2/5] [Batch 75/360] [D loss: 0.002249] [G loss: 14.190707] time: 0:01:46.784849\n",
      "(30, 64, 64, 3)\n",
      "0.8243105\n",
      "[Epoch 2/5] [Batch 76/360] [D loss: 0.002566] [G loss: 14.027042] time: 0:01:46.903959\n",
      "(30, 64, 64, 3)\n",
      "0.7912495\n",
      "[Epoch 2/5] [Batch 77/360] [D loss: 0.004358] [G loss: 13.792895] time: 0:01:47.032394\n",
      "(30, 64, 64, 3)\n",
      "0.75767785\n",
      "[Epoch 2/5] [Batch 78/360] [D loss: 0.003348] [G loss: 14.367148] time: 0:01:47.143767\n",
      "(30, 64, 64, 3)\n",
      "0.79794437\n",
      "[Epoch 2/5] [Batch 79/360] [D loss: 0.004119] [G loss: 14.213723] time: 0:01:47.265905\n",
      "(30, 64, 64, 3)\n",
      "0.7801387\n",
      "[Epoch 2/5] [Batch 80/360] [D loss: 0.002608] [G loss: 14.126794] time: 0:01:47.389883\n",
      "(30, 64, 64, 3)\n",
      "0.80388427\n",
      "[Epoch 2/5] [Batch 81/360] [D loss: 0.004172] [G loss: 13.933043] time: 0:01:47.523821\n",
      "(30, 64, 64, 3)\n",
      "0.80955076\n",
      "[Epoch 2/5] [Batch 82/360] [D loss: 0.002106] [G loss: 14.555076] time: 0:01:47.642930\n",
      "(30, 64, 64, 3)\n",
      "0.8654172\n",
      "[Epoch 2/5] [Batch 83/360] [D loss: 0.003121] [G loss: 13.940825] time: 0:01:47.763850\n",
      "(30, 64, 64, 3)\n",
      "0.8174248\n",
      "[Epoch 2/5] [Batch 84/360] [D loss: 0.002176] [G loss: 14.297724] time: 0:01:47.880135\n",
      "(30, 64, 64, 3)\n",
      "0.81717116\n",
      "[Epoch 2/5] [Batch 85/360] [D loss: 0.003330] [G loss: 13.391348] time: 0:01:48.001635\n",
      "(30, 64, 64, 3)\n",
      "0.8139942\n",
      "[Epoch 2/5] [Batch 86/360] [D loss: 0.002357] [G loss: 14.251601] time: 0:01:48.122049\n",
      "(30, 64, 64, 3)\n",
      "0.7745022\n",
      "[Epoch 2/5] [Batch 87/360] [D loss: 0.005192] [G loss: 13.989328] time: 0:01:48.242821\n",
      "(30, 64, 64, 3)\n",
      "0.82675886\n",
      "[Epoch 2/5] [Batch 88/360] [D loss: 0.002772] [G loss: 13.816001] time: 0:01:48.360944\n",
      "(30, 64, 64, 3)\n",
      "0.79334384\n",
      "[Epoch 2/5] [Batch 89/360] [D loss: 0.004131] [G loss: 13.762951] time: 0:01:48.478746\n",
      "(30, 64, 64, 3)\n",
      "0.8298473\n",
      "[Epoch 2/5] [Batch 90/360] [D loss: 0.002288] [G loss: 13.614291] time: 0:01:48.599418\n",
      "(30, 64, 64, 3)\n",
      "0.83315706\n",
      "[Epoch 2/5] [Batch 91/360] [D loss: 0.003212] [G loss: 13.961251] time: 0:01:48.721824\n",
      "(30, 64, 64, 3)\n",
      "0.81873125\n",
      "[Epoch 2/5] [Batch 92/360] [D loss: 0.001999] [G loss: 13.951338] time: 0:01:48.841336\n",
      "(30, 64, 64, 3)\n",
      "0.8217463\n",
      "[Epoch 2/5] [Batch 93/360] [D loss: 0.002818] [G loss: 13.647893] time: 0:01:48.960382\n",
      "(30, 64, 64, 3)\n",
      "0.8203513\n",
      "[Epoch 2/5] [Batch 94/360] [D loss: 0.002629] [G loss: 13.857103] time: 0:01:49.075390\n",
      "(30, 64, 64, 3)\n",
      "0.82827497\n",
      "[Epoch 2/5] [Batch 95/360] [D loss: 0.001953] [G loss: 13.783704] time: 0:01:49.197997\n",
      "(30, 64, 64, 3)\n",
      "0.7945103\n",
      "[Epoch 2/5] [Batch 96/360] [D loss: 0.002025] [G loss: 13.968927] time: 0:01:49.317540\n",
      "(30, 64, 64, 3)\n",
      "0.77169985\n",
      "[Epoch 2/5] [Batch 97/360] [D loss: 0.001588] [G loss: 13.772673] time: 0:01:49.445202\n",
      "(30, 64, 64, 3)\n",
      "0.81405324\n",
      "[Epoch 2/5] [Batch 98/360] [D loss: 0.001257] [G loss: 14.012964] time: 0:01:49.568008\n",
      "(30, 64, 64, 3)\n",
      "0.7784989\n",
      "[Epoch 2/5] [Batch 99/360] [D loss: 0.001728] [G loss: 14.213404] time: 0:01:49.698939\n",
      "(30, 64, 64, 3)\n",
      "0.77028066\n",
      "[Epoch 2/5] [Batch 100/360] [D loss: 0.001566] [G loss: 13.581115] time: 0:01:49.817208\n",
      "(30, 64, 64, 3)\n",
      "0.79369086\n",
      "[Epoch 2/5] [Batch 101/360] [D loss: 0.001440] [G loss: 14.181555] time: 0:01:49.934718\n",
      "(30, 64, 64, 3)\n",
      "0.808008\n",
      "[Epoch 2/5] [Batch 102/360] [D loss: 0.001209] [G loss: 13.745683] time: 0:01:50.052977\n",
      "(30, 64, 64, 3)\n",
      "0.8212063\n",
      "[Epoch 2/5] [Batch 103/360] [D loss: 0.001760] [G loss: 13.811289] time: 0:01:50.174632\n",
      "(30, 64, 64, 3)\n",
      "0.81325203\n",
      "[Epoch 2/5] [Batch 104/360] [D loss: 0.001102] [G loss: 14.555681] time: 0:01:50.291056\n",
      "(30, 64, 64, 3)\n",
      "0.8385737\n",
      "[Epoch 2/5] [Batch 105/360] [D loss: 0.001692] [G loss: 13.429092] time: 0:01:50.405996\n",
      "(30, 64, 64, 3)\n",
      "0.7919962\n",
      "[Epoch 2/5] [Batch 106/360] [D loss: 0.001228] [G loss: 13.693772] time: 0:01:50.527429\n",
      "(30, 64, 64, 3)\n",
      "0.8208046\n",
      "[Epoch 2/5] [Batch 107/360] [D loss: 0.001623] [G loss: 13.965142] time: 0:01:50.651404\n",
      "(30, 64, 64, 3)\n",
      "0.79587907\n",
      "[Epoch 2/5] [Batch 108/360] [D loss: 0.001240] [G loss: 13.762531] time: 0:01:50.777090\n",
      "(30, 64, 64, 3)\n",
      "0.75471425\n",
      "[Epoch 2/5] [Batch 109/360] [D loss: 0.001781] [G loss: 13.356310] time: 0:01:50.896180\n",
      "(30, 64, 64, 3)\n",
      "0.816225\n",
      "[Epoch 2/5] [Batch 110/360] [D loss: 0.001392] [G loss: 13.783149] time: 0:01:51.017140\n",
      "(30, 64, 64, 3)\n",
      "0.81384104\n",
      "[Epoch 2/5] [Batch 111/360] [D loss: 0.001031] [G loss: 13.955252] time: 0:01:51.135561\n",
      "(30, 64, 64, 3)\n",
      "0.81900674\n",
      "[Epoch 2/5] [Batch 112/360] [D loss: 0.001541] [G loss: 13.723985] time: 0:01:51.250011\n",
      "(30, 64, 64, 3)\n",
      "0.80834645\n",
      "[Epoch 2/5] [Batch 113/360] [D loss: 0.001816] [G loss: 13.281702] time: 0:01:51.372352\n",
      "(30, 64, 64, 3)\n",
      "0.7973232\n",
      "[Epoch 2/5] [Batch 114/360] [D loss: 0.001647] [G loss: 13.814425] time: 0:01:51.491562\n",
      "(30, 64, 64, 3)\n",
      "0.8198984\n",
      "[Epoch 2/5] [Batch 115/360] [D loss: 0.001175] [G loss: 13.675918] time: 0:01:51.608361\n",
      "(30, 64, 64, 3)\n",
      "0.8101818\n",
      "[Epoch 2/5] [Batch 116/360] [D loss: 0.001244] [G loss: 13.304482] time: 0:01:51.728969\n",
      "(30, 64, 64, 3)\n",
      "0.81269306\n",
      "[Epoch 2/5] [Batch 117/360] [D loss: 0.001884] [G loss: 13.345645] time: 0:01:51.853233\n",
      "(30, 64, 64, 3)\n",
      "0.8310876\n",
      "[Epoch 2/5] [Batch 118/360] [D loss: 0.001255] [G loss: 13.736806] time: 0:01:51.971649\n",
      "(30, 64, 64, 3)\n",
      "0.83482975\n",
      "[Epoch 2/5] [Batch 119/360] [D loss: 0.002112] [G loss: 12.987454] time: 0:01:52.093841\n",
      "(30, 64, 64, 3)\n",
      "0.7843199\n",
      "[Epoch 2/5] [Batch 120/360] [D loss: 0.001067] [G loss: 13.279637] time: 0:01:52.212918\n",
      "(30, 64, 64, 3)\n",
      "0.79125476\n",
      "[Epoch 2/5] [Batch 121/360] [D loss: 0.001500] [G loss: 13.357156] time: 0:01:52.332593\n",
      "(30, 64, 64, 3)\n",
      "0.8131521\n",
      "[Epoch 2/5] [Batch 122/360] [D loss: 0.001116] [G loss: 13.678793] time: 0:01:52.458018\n",
      "(30, 64, 64, 3)\n",
      "0.7910083\n",
      "[Epoch 2/5] [Batch 123/360] [D loss: 0.001287] [G loss: 13.442102] time: 0:01:52.579462\n",
      "(30, 64, 64, 3)\n",
      "0.8003836\n",
      "[Epoch 2/5] [Batch 124/360] [D loss: 0.000919] [G loss: 13.426449] time: 0:01:52.694430\n",
      "(30, 64, 64, 3)\n",
      "0.8261833\n",
      "[Epoch 2/5] [Batch 125/360] [D loss: 0.001218] [G loss: 13.293813] time: 0:01:52.811824\n",
      "(30, 64, 64, 3)\n",
      "0.81277436\n",
      "[Epoch 2/5] [Batch 126/360] [D loss: 0.001148] [G loss: 13.662149] time: 0:01:52.935293\n",
      "(30, 64, 64, 3)\n",
      "0.8038451\n",
      "[Epoch 2/5] [Batch 127/360] [D loss: 0.002656] [G loss: 12.931033] time: 0:01:53.054653\n",
      "(30, 64, 64, 3)\n",
      "0.81760985\n",
      "[Epoch 2/5] [Batch 128/360] [D loss: 0.001190] [G loss: 13.433762] time: 0:01:53.182802\n",
      "(30, 64, 64, 3)\n",
      "0.8167067\n",
      "[Epoch 2/5] [Batch 129/360] [D loss: 0.001557] [G loss: 13.333703] time: 0:01:53.305225\n",
      "(30, 64, 64, 3)\n",
      "0.78366375\n",
      "[Epoch 2/5] [Batch 130/360] [D loss: 0.000938] [G loss: 13.443808] time: 0:01:53.422906\n",
      "(30, 64, 64, 3)\n",
      "0.8310738\n",
      "[Epoch 2/5] [Batch 131/360] [D loss: 0.002200] [G loss: 13.390313] time: 0:01:53.551870\n",
      "(30, 64, 64, 3)\n",
      "0.8284598\n",
      "[Epoch 2/5] [Batch 132/360] [D loss: 0.000907] [G loss: 13.563547] time: 0:01:53.669968\n",
      "(30, 64, 64, 3)\n",
      "0.7603004\n",
      "[Epoch 2/5] [Batch 133/360] [D loss: 0.001408] [G loss: 13.355580] time: 0:01:53.788414\n",
      "(30, 64, 64, 3)\n",
      "0.8048704\n",
      "[Epoch 2/5] [Batch 134/360] [D loss: 0.001881] [G loss: 13.366755] time: 0:01:53.903872\n",
      "(30, 64, 64, 3)\n",
      "0.87142617\n",
      "[Epoch 2/5] [Batch 135/360] [D loss: 0.001894] [G loss: 13.403311] time: 0:01:54.030319\n",
      "(30, 64, 64, 3)\n",
      "0.8093665\n",
      "[Epoch 2/5] [Batch 136/360] [D loss: 0.001053] [G loss: 12.835032] time: 0:01:54.147707\n",
      "(30, 64, 64, 3)\n",
      "0.81408554\n",
      "[Epoch 2/5] [Batch 137/360] [D loss: 0.002231] [G loss: 13.282683] time: 0:01:54.264560\n",
      "(30, 64, 64, 3)\n",
      "0.8029277\n",
      "[Epoch 2/5] [Batch 138/360] [D loss: 0.001863] [G loss: 13.344195] time: 0:01:54.381055\n",
      "(30, 64, 64, 3)\n",
      "0.7757736\n",
      "[Epoch 2/5] [Batch 139/360] [D loss: 0.002012] [G loss: 13.390804] time: 0:01:54.506175\n",
      "(30, 64, 64, 3)\n",
      "0.8050137\n",
      "[Epoch 2/5] [Batch 140/360] [D loss: 0.001257] [G loss: 13.822905] time: 0:01:54.628741\n",
      "(30, 64, 64, 3)\n",
      "0.7398898\n",
      "[Epoch 2/5] [Batch 141/360] [D loss: 0.002523] [G loss: 12.836387] time: 0:01:54.750942\n",
      "(30, 64, 64, 3)\n",
      "0.8151013\n",
      "[Epoch 2/5] [Batch 142/360] [D loss: 0.001769] [G loss: 13.350795] time: 0:01:54.869376\n",
      "(30, 64, 64, 3)\n",
      "0.8673959\n",
      "[Epoch 2/5] [Batch 143/360] [D loss: 0.002294] [G loss: 13.557534] time: 0:01:54.990441\n",
      "(30, 64, 64, 3)\n",
      "0.79153377\n",
      "[Epoch 2/5] [Batch 144/360] [D loss: 0.001590] [G loss: 13.263250] time: 0:01:55.107746\n",
      "(30, 64, 64, 3)\n",
      "0.8122743\n",
      "[Epoch 2/5] [Batch 145/360] [D loss: 0.003225] [G loss: 12.543417] time: 0:01:55.239430\n",
      "(30, 64, 64, 3)\n",
      "0.80763125\n",
      "[Epoch 2/5] [Batch 146/360] [D loss: 0.001660] [G loss: 12.890576] time: 0:01:55.356147\n",
      "(30, 64, 64, 3)\n",
      "0.79470634\n",
      "[Epoch 2/5] [Batch 147/360] [D loss: 0.001983] [G loss: 13.055636] time: 0:01:55.476419\n",
      "(30, 64, 64, 3)\n",
      "0.8501714\n",
      "[Epoch 2/5] [Batch 148/360] [D loss: 0.002134] [G loss: 13.122053] time: 0:01:55.597652\n",
      "(30, 64, 64, 3)\n",
      "0.8297267\n",
      "[Epoch 2/5] [Batch 149/360] [D loss: 0.002730] [G loss: 13.040813] time: 0:01:55.723008\n",
      "(30, 64, 64, 3)\n",
      "0.8047881\n",
      "[Epoch 2/5] [Batch 150/360] [D loss: 0.001677] [G loss: 12.850617] time: 0:01:55.840880\n",
      "(30, 64, 64, 3)\n",
      "0.8372755\n",
      "[Epoch 2/5] [Batch 151/360] [D loss: 0.004018] [G loss: 13.246948] time: 0:01:55.965558\n",
      "(30, 64, 64, 3)\n",
      "0.8475676\n",
      "[Epoch 2/5] [Batch 152/360] [D loss: 0.002825] [G loss: 13.269065] time: 0:01:56.082558\n",
      "(30, 64, 64, 3)\n",
      "0.8182507\n",
      "[Epoch 2/5] [Batch 153/360] [D loss: 0.005654] [G loss: 12.558314] time: 0:01:56.203766\n",
      "(30, 64, 64, 3)\n",
      "0.8319743\n",
      "[Epoch 2/5] [Batch 154/360] [D loss: 0.002687] [G loss: 13.112386] time: 0:01:56.323125\n",
      "(30, 64, 64, 3)\n",
      "0.80003005\n",
      "[Epoch 2/5] [Batch 155/360] [D loss: 0.006520] [G loss: 12.664839] time: 0:01:56.440809\n",
      "(30, 64, 64, 3)\n",
      "0.8104319\n",
      "[Epoch 2/5] [Batch 156/360] [D loss: 0.004583] [G loss: 12.627782] time: 0:01:56.556284\n",
      "(30, 64, 64, 3)\n",
      "0.79523116\n",
      "[Epoch 2/5] [Batch 157/360] [D loss: 0.010775] [G loss: 12.323942] time: 0:01:56.671594\n",
      "(30, 64, 64, 3)\n",
      "0.82516795\n",
      "[Epoch 2/5] [Batch 158/360] [D loss: 0.005141] [G loss: 12.486670] time: 0:01:56.792919\n",
      "(30, 64, 64, 3)\n",
      "0.74850374\n",
      "[Epoch 2/5] [Batch 159/360] [D loss: 0.015985] [G loss: 12.877934] time: 0:01:56.921006\n",
      "(30, 64, 64, 3)\n",
      "0.8364268\n",
      "[Epoch 2/5] [Batch 160/360] [D loss: 0.004595] [G loss: 12.622439] time: 0:01:57.040244\n",
      "(30, 64, 64, 3)\n",
      "0.8234696\n",
      "[Epoch 2/5] [Batch 161/360] [D loss: 0.011774] [G loss: 12.841226] time: 0:01:57.153149\n",
      "(30, 64, 64, 3)\n",
      "0.8100292\n",
      "[Epoch 2/5] [Batch 162/360] [D loss: 0.010731] [G loss: 12.902904] time: 0:01:57.273321\n",
      "(30, 64, 64, 3)\n",
      "0.83683616\n",
      "[Epoch 2/5] [Batch 163/360] [D loss: 0.022688] [G loss: 12.435968] time: 0:01:57.389671\n",
      "(30, 64, 64, 3)\n",
      "0.81813574\n",
      "[Epoch 2/5] [Batch 164/360] [D loss: 0.005088] [G loss: 12.814034] time: 0:01:57.510172\n",
      "(30, 64, 64, 3)\n",
      "0.84841365\n",
      "[Epoch 2/5] [Batch 165/360] [D loss: 0.033160] [G loss: 12.972609] time: 0:01:57.630802\n",
      "(30, 64, 64, 3)\n",
      "0.8126003\n",
      "[Epoch 2/5] [Batch 166/360] [D loss: 0.005452] [G loss: 12.742643] time: 0:01:57.749835\n",
      "(30, 64, 64, 3)\n",
      "0.8195534\n",
      "[Epoch 2/5] [Batch 167/360] [D loss: 0.002556] [G loss: 12.645414] time: 0:01:57.872739\n",
      "(30, 64, 64, 3)\n",
      "0.8198331\n",
      "[Epoch 2/5] [Batch 168/360] [D loss: 0.007202] [G loss: 12.807766] time: 0:01:57.988000\n",
      "(30, 64, 64, 3)\n",
      "0.8445392\n",
      "[Epoch 2/5] [Batch 169/360] [D loss: 0.053283] [G loss: 12.835234] time: 0:01:58.106679\n",
      "(30, 64, 64, 3)\n",
      "0.81823474\n",
      "[Epoch 2/5] [Batch 170/360] [D loss: 0.043904] [G loss: 12.785317] time: 0:01:58.226337\n",
      "(30, 64, 64, 3)\n",
      "0.81971836\n",
      "[Epoch 2/5] [Batch 171/360] [D loss: 0.032426] [G loss: 12.927114] time: 0:01:58.358174\n",
      "(30, 64, 64, 3)\n",
      "0.82864374\n",
      "[Epoch 2/5] [Batch 172/360] [D loss: 0.044220] [G loss: 12.038145] time: 0:01:58.483996\n",
      "(30, 64, 64, 3)\n",
      "0.8033523\n",
      "[Epoch 2/5] [Batch 173/360] [D loss: 0.013972] [G loss: 12.796570] time: 0:01:58.597338\n",
      "(30, 64, 64, 3)\n",
      "0.8168893\n",
      "[Epoch 2/5] [Batch 174/360] [D loss: 0.012034] [G loss: 12.388315] time: 0:01:58.715862\n",
      "(30, 64, 64, 3)\n",
      "0.8456827\n",
      "[Epoch 2/5] [Batch 175/360] [D loss: 0.002640] [G loss: 12.443643] time: 0:01:58.833798\n",
      "(30, 64, 64, 3)\n",
      "0.83616734\n",
      "[Epoch 2/5] [Batch 176/360] [D loss: 0.001308] [G loss: 12.505554] time: 0:01:58.956003\n",
      "(30, 64, 64, 3)\n",
      "0.8170004\n",
      "[Epoch 2/5] [Batch 177/360] [D loss: 0.002479] [G loss: 12.477273] time: 0:01:59.070927\n",
      "(30, 64, 64, 3)\n",
      "0.78685457\n",
      "[Epoch 2/5] [Batch 178/360] [D loss: 0.001374] [G loss: 12.201305] time: 0:01:59.188189\n",
      "(30, 64, 64, 3)\n",
      "0.79220366\n",
      "[Epoch 2/5] [Batch 179/360] [D loss: 0.001735] [G loss: 12.457372] time: 0:01:59.309883\n",
      "(30, 64, 64, 3)\n",
      "0.8139952\n",
      "[Epoch 2/5] [Batch 180/360] [D loss: 0.001630] [G loss: 12.172366] time: 0:01:59.437654\n",
      "(30, 64, 64, 3)\n",
      "0.7828166\n",
      "[Epoch 2/5] [Batch 181/360] [D loss: 0.001251] [G loss: 12.572821] time: 0:01:59.559159\n",
      "(30, 64, 64, 3)\n",
      "0.8180211\n",
      "[Epoch 2/5] [Batch 182/360] [D loss: 0.001448] [G loss: 12.310754] time: 0:01:59.681424\n",
      "(30, 64, 64, 3)\n",
      "0.87693435\n",
      "[Epoch 2/5] [Batch 183/360] [D loss: 0.001648] [G loss: 12.599699] time: 0:01:59.808361\n",
      "(30, 64, 64, 3)\n",
      "0.8402135\n",
      "[Epoch 2/5] [Batch 184/360] [D loss: 0.002200] [G loss: 11.734802] time: 0:01:59.933460\n",
      "(30, 64, 64, 3)\n",
      "0.77331287\n",
      "[Epoch 2/5] [Batch 185/360] [D loss: 0.001756] [G loss: 12.310613] time: 0:02:00.065799\n",
      "(30, 64, 64, 3)\n",
      "0.81477994\n",
      "[Epoch 2/5] [Batch 186/360] [D loss: 0.001260] [G loss: 12.183614] time: 0:02:00.194629\n",
      "(30, 64, 64, 3)\n",
      "0.7377305\n",
      "[Epoch 2/5] [Batch 187/360] [D loss: 0.001254] [G loss: 12.032464] time: 0:02:00.306326\n",
      "(30, 64, 64, 3)\n",
      "0.83231425\n",
      "[Epoch 2/5] [Batch 188/360] [D loss: 0.001516] [G loss: 12.079260] time: 0:02:00.421209\n",
      "(30, 64, 64, 3)\n",
      "0.8379731\n",
      "[Epoch 2/5] [Batch 189/360] [D loss: 0.001060] [G loss: 12.136658] time: 0:02:00.541637\n",
      "(30, 64, 64, 3)\n",
      "0.8503069\n",
      "[Epoch 2/5] [Batch 190/360] [D loss: 0.001414] [G loss: 11.758294] time: 0:02:00.654585\n",
      "(30, 64, 64, 3)\n",
      "0.80462915\n",
      "[Epoch 2/5] [Batch 191/360] [D loss: 0.001655] [G loss: 12.445901] time: 0:02:00.771785\n",
      "(30, 64, 64, 3)\n",
      "0.8206586\n",
      "[Epoch 2/5] [Batch 192/360] [D loss: 0.001233] [G loss: 11.753195] time: 0:02:00.884290\n",
      "(30, 64, 64, 3)\n",
      "0.80774397\n",
      "[Epoch 2/5] [Batch 193/360] [D loss: 0.001184] [G loss: 12.433202] time: 0:02:01.010043\n",
      "(30, 64, 64, 3)\n",
      "0.81845313\n",
      "[Epoch 2/5] [Batch 194/360] [D loss: 0.001097] [G loss: 12.495203] time: 0:02:01.142866\n",
      "(30, 64, 64, 3)\n",
      "0.8257243\n",
      "[Epoch 2/5] [Batch 195/360] [D loss: 0.001259] [G loss: 12.194688] time: 0:02:01.260906\n",
      "(30, 64, 64, 3)\n",
      "0.8560503\n",
      "[Epoch 2/5] [Batch 196/360] [D loss: 0.001168] [G loss: 12.187960] time: 0:02:01.370537\n",
      "(30, 64, 64, 3)\n",
      "0.8422342\n",
      "[Epoch 2/5] [Batch 197/360] [D loss: 0.001057] [G loss: 12.624170] time: 0:02:01.489763\n",
      "(30, 64, 64, 3)\n",
      "0.7945464\n",
      "[Epoch 2/5] [Batch 198/360] [D loss: 0.001803] [G loss: 12.694646] time: 0:02:01.604040\n",
      "(30, 64, 64, 3)\n",
      "0.8495349\n",
      "[Epoch 2/5] [Batch 199/360] [D loss: 0.002077] [G loss: 12.545837] time: 0:02:01.719532\n",
      "(30, 64, 64, 3)\n",
      "0.8361599\n",
      "[Epoch 2/5] [Batch 200/360] [D loss: 0.001784] [G loss: 11.963952] time: 0:02:01.831579\n",
      "(30, 64, 64, 3)\n",
      "0.84703475\n",
      "[Epoch 2/5] [Batch 201/360] [D loss: 0.001696] [G loss: 11.888048] time: 0:02:01.944718\n",
      "(30, 64, 64, 3)\n",
      "0.81531745\n",
      "[Epoch 2/5] [Batch 202/360] [D loss: 0.001404] [G loss: 11.534925] time: 0:02:02.057018\n",
      "(30, 64, 64, 3)\n",
      "0.8171997\n",
      "[Epoch 2/5] [Batch 203/360] [D loss: 0.001937] [G loss: 11.813662] time: 0:02:02.174277\n",
      "(30, 64, 64, 3)\n",
      "0.85107946\n",
      "[Epoch 2/5] [Batch 204/360] [D loss: 0.001083] [G loss: 12.152392] time: 0:02:02.288890\n",
      "(30, 64, 64, 3)\n",
      "0.8334286\n",
      "[Epoch 2/5] [Batch 205/360] [D loss: 0.001107] [G loss: 12.420873] time: 0:02:02.405259\n",
      "(30, 64, 64, 3)\n",
      "0.8449237\n",
      "[Epoch 2/5] [Batch 206/360] [D loss: 0.001124] [G loss: 12.323874] time: 0:02:02.519260\n",
      "(30, 64, 64, 3)\n",
      "0.83427876\n",
      "[Epoch 2/5] [Batch 207/360] [D loss: 0.002218] [G loss: 12.459403] time: 0:02:02.635198\n",
      "(30, 64, 64, 3)\n",
      "0.8261177\n",
      "[Epoch 2/5] [Batch 208/360] [D loss: 0.001147] [G loss: 12.132230] time: 0:02:02.747746\n",
      "(30, 64, 64, 3)\n",
      "0.8366389\n",
      "[Epoch 2/5] [Batch 209/360] [D loss: 0.001861] [G loss: 12.104783] time: 0:02:02.862547\n",
      "(30, 64, 64, 3)\n",
      "0.8398902\n",
      "[Epoch 2/5] [Batch 210/360] [D loss: 0.001362] [G loss: 12.071416] time: 0:02:02.976951\n",
      "(30, 64, 64, 3)\n",
      "0.8151004\n",
      "[Epoch 2/5] [Batch 211/360] [D loss: 0.001855] [G loss: 11.729049] time: 0:02:03.091322\n",
      "(30, 64, 64, 3)\n",
      "0.7965452\n",
      "[Epoch 2/5] [Batch 212/360] [D loss: 0.001858] [G loss: 12.332907] time: 0:02:03.205030\n",
      "(30, 64, 64, 3)\n",
      "0.85493714\n",
      "[Epoch 2/5] [Batch 213/360] [D loss: 0.001432] [G loss: 11.861503] time: 0:02:03.317571\n",
      "(30, 64, 64, 3)\n",
      "0.8411598\n",
      "[Epoch 2/5] [Batch 214/360] [D loss: 0.001622] [G loss: 11.799678] time: 0:02:03.432482\n",
      "(30, 64, 64, 3)\n",
      "0.80116844\n",
      "[Epoch 2/5] [Batch 215/360] [D loss: 0.001782] [G loss: 12.152375] time: 0:02:03.558027\n",
      "(30, 64, 64, 3)\n",
      "0.8393914\n",
      "[Epoch 2/5] [Batch 216/360] [D loss: 0.001164] [G loss: 11.610133] time: 0:02:03.675150\n",
      "(30, 64, 64, 3)\n",
      "0.81912756\n",
      "[Epoch 2/5] [Batch 217/360] [D loss: 0.001485] [G loss: 11.652178] time: 0:02:03.788846\n",
      "(30, 64, 64, 3)\n",
      "0.84934133\n",
      "[Epoch 2/5] [Batch 218/360] [D loss: 0.002337] [G loss: 11.679440] time: 0:02:03.900076\n",
      "(30, 64, 64, 3)\n",
      "0.8091338\n",
      "[Epoch 2/5] [Batch 219/360] [D loss: 0.001654] [G loss: 11.729647] time: 0:02:04.015594\n",
      "(30, 64, 64, 3)\n",
      "0.79575545\n",
      "[Epoch 2/5] [Batch 220/360] [D loss: 0.001559] [G loss: 11.879891] time: 0:02:04.129452\n",
      "(30, 64, 64, 3)\n",
      "0.8366607\n",
      "[Epoch 2/5] [Batch 221/360] [D loss: 0.001083] [G loss: 12.116935] time: 0:02:04.245670\n",
      "(30, 64, 64, 3)\n",
      "0.8111736\n",
      "[Epoch 2/5] [Batch 222/360] [D loss: 0.001259] [G loss: 11.880869] time: 0:02:04.359812\n",
      "(30, 64, 64, 3)\n",
      "0.85020775\n",
      "[Epoch 2/5] [Batch 223/360] [D loss: 0.001342] [G loss: 11.626603] time: 0:02:04.486046\n",
      "(30, 64, 64, 3)\n",
      "0.8149285\n",
      "[Epoch 2/5] [Batch 224/360] [D loss: 0.002133] [G loss: 11.790737] time: 0:02:04.596997\n",
      "(30, 64, 64, 3)\n",
      "0.84958196\n",
      "[Epoch 2/5] [Batch 225/360] [D loss: 0.001692] [G loss: 11.870417] time: 0:02:04.713818\n",
      "(30, 64, 64, 3)\n",
      "0.83407515\n",
      "[Epoch 2/5] [Batch 226/360] [D loss: 0.001887] [G loss: 11.203745] time: 0:02:04.823003\n",
      "(30, 64, 64, 3)\n",
      "0.8774011\n",
      "[Epoch 2/5] [Batch 227/360] [D loss: 0.001451] [G loss: 11.679551] time: 0:02:04.940895\n",
      "(30, 64, 64, 3)\n",
      "0.8155103\n",
      "[Epoch 2/5] [Batch 228/360] [D loss: 0.003312] [G loss: 11.970490] time: 0:02:05.052691\n",
      "(30, 64, 64, 3)\n",
      "0.7849038\n",
      "[Epoch 2/5] [Batch 229/360] [D loss: 0.001890] [G loss: 12.124085] time: 0:02:05.171207\n",
      "(30, 64, 64, 3)\n",
      "0.83380157\n",
      "[Epoch 2/5] [Batch 230/360] [D loss: 0.002024] [G loss: 11.681960] time: 0:02:05.289633\n",
      "(30, 64, 64, 3)\n",
      "0.8131092\n",
      "[Epoch 2/5] [Batch 231/360] [D loss: 0.001592] [G loss: 11.730455] time: 0:02:05.402749\n",
      "(30, 64, 64, 3)\n",
      "0.8432283\n",
      "[Epoch 2/5] [Batch 232/360] [D loss: 0.003034] [G loss: 11.379775] time: 0:02:05.516401\n",
      "(30, 64, 64, 3)\n",
      "0.84061414\n",
      "[Epoch 2/5] [Batch 233/360] [D loss: 0.002407] [G loss: 11.808043] time: 0:02:05.631900\n",
      "(30, 64, 64, 3)\n",
      "0.84705615\n",
      "[Epoch 2/5] [Batch 234/360] [D loss: 0.004172] [G loss: 11.693029] time: 0:02:05.747689\n",
      "(30, 64, 64, 3)\n",
      "0.8288191\n",
      "[Epoch 2/5] [Batch 235/360] [D loss: 0.003010] [G loss: 11.668107] time: 0:02:05.859725\n",
      "(30, 64, 64, 3)\n",
      "0.7941826\n",
      "[Epoch 2/5] [Batch 236/360] [D loss: 0.003271] [G loss: 11.987851] time: 0:02:05.973657\n",
      "(30, 64, 64, 3)\n",
      "0.7954443\n",
      "[Epoch 2/5] [Batch 237/360] [D loss: 0.002914] [G loss: 11.324718] time: 0:02:06.088941\n",
      "(30, 64, 64, 3)\n",
      "0.8136894\n",
      "[Epoch 2/5] [Batch 238/360] [D loss: 0.002763] [G loss: 11.332167] time: 0:02:06.200255\n",
      "(30, 64, 64, 3)\n",
      "0.8445852\n",
      "[Epoch 2/5] [Batch 239/360] [D loss: 0.001679] [G loss: 11.637704] time: 0:02:06.322139\n",
      "(30, 64, 64, 3)\n",
      "0.8155069\n",
      "[Epoch 2/5] [Batch 240/360] [D loss: 0.002618] [G loss: 11.769593] time: 0:02:06.434072\n",
      "(30, 64, 64, 3)\n",
      "0.82561964\n",
      "[Epoch 2/5] [Batch 241/360] [D loss: 0.001340] [G loss: 11.863331] time: 0:02:06.553515\n",
      "(30, 64, 64, 3)\n",
      "0.86402315\n",
      "[Epoch 2/5] [Batch 242/360] [D loss: 0.002637] [G loss: 11.236301] time: 0:02:06.664858\n",
      "(30, 64, 64, 3)\n",
      "0.8490588\n",
      "[Epoch 2/5] [Batch 243/360] [D loss: 0.001382] [G loss: 11.453766] time: 0:02:06.801118\n",
      "(30, 64, 64, 3)\n",
      "0.82120466\n",
      "[Epoch 2/5] [Batch 244/360] [D loss: 0.001619] [G loss: 11.418061] time: 0:02:06.915498\n",
      "(30, 64, 64, 3)\n",
      "0.8279133\n",
      "[Epoch 2/5] [Batch 245/360] [D loss: 0.001197] [G loss: 11.741486] time: 0:02:07.032348\n",
      "(30, 64, 64, 3)\n",
      "0.80272675\n",
      "[Epoch 2/5] [Batch 246/360] [D loss: 0.001342] [G loss: 11.402732] time: 0:02:07.143813\n",
      "(30, 64, 64, 3)\n",
      "0.84686995\n",
      "[Epoch 2/5] [Batch 247/360] [D loss: 0.001595] [G loss: 11.311319] time: 0:02:07.271472\n",
      "(30, 64, 64, 3)\n",
      "0.855406\n",
      "[Epoch 2/5] [Batch 248/360] [D loss: 0.001878] [G loss: 11.098317] time: 0:02:07.385493\n",
      "(30, 64, 64, 3)\n",
      "0.85034436\n",
      "[Epoch 2/5] [Batch 249/360] [D loss: 0.001720] [G loss: 11.423615] time: 0:02:07.504151\n",
      "(30, 64, 64, 3)\n",
      "0.83685213\n",
      "[Epoch 2/5] [Batch 250/360] [D loss: 0.002129] [G loss: 11.178675] time: 0:02:07.618312\n",
      "(30, 64, 64, 3)\n",
      "0.82985467\n",
      "[Epoch 2/5] [Batch 251/360] [D loss: 0.001127] [G loss: 11.288984] time: 0:02:07.734376\n",
      "(30, 64, 64, 3)\n",
      "0.85391736\n",
      "[Epoch 2/5] [Batch 252/360] [D loss: 0.001441] [G loss: 11.269962] time: 0:02:07.852017\n",
      "(30, 64, 64, 3)\n",
      "0.8515029\n",
      "[Epoch 2/5] [Batch 253/360] [D loss: 0.001499] [G loss: 10.938915] time: 0:02:07.974373\n",
      "(30, 64, 64, 3)\n",
      "0.82904387\n",
      "[Epoch 2/5] [Batch 254/360] [D loss: 0.001237] [G loss: 11.147384] time: 0:02:08.086317\n",
      "(30, 64, 64, 3)\n",
      "0.81227875\n",
      "[Epoch 2/5] [Batch 255/360] [D loss: 0.002028] [G loss: 11.177870] time: 0:02:08.212141\n",
      "(30, 64, 64, 3)\n",
      "0.7946637\n",
      "[Epoch 2/5] [Batch 256/360] [D loss: 0.002766] [G loss: 11.893881] time: 0:02:08.324759\n",
      "(30, 64, 64, 3)\n",
      "0.8147924\n",
      "[Epoch 2/5] [Batch 257/360] [D loss: 0.002147] [G loss: 11.564489] time: 0:02:08.444813\n",
      "(30, 64, 64, 3)\n",
      "0.8303774\n",
      "[Epoch 2/5] [Batch 258/360] [D loss: 0.002189] [G loss: 11.300530] time: 0:02:08.563364\n",
      "(30, 64, 64, 3)\n",
      "0.83080775\n",
      "[Epoch 2/5] [Batch 259/360] [D loss: 0.002698] [G loss: 11.345817] time: 0:02:08.679914\n",
      "(30, 64, 64, 3)\n",
      "0.8574114\n",
      "[Epoch 2/5] [Batch 260/360] [D loss: 0.003196] [G loss: 11.098001] time: 0:02:08.795865\n",
      "(30, 64, 64, 3)\n",
      "0.8222073\n",
      "[Epoch 2/5] [Batch 261/360] [D loss: 0.002500] [G loss: 11.281912] time: 0:02:08.914636\n",
      "(30, 64, 64, 3)\n",
      "0.8808901\n",
      "[Epoch 2/5] [Batch 262/360] [D loss: 0.002438] [G loss: 10.950852] time: 0:02:09.030781\n",
      "(30, 64, 64, 3)\n",
      "0.8163007\n",
      "[Epoch 2/5] [Batch 263/360] [D loss: 0.001999] [G loss: 11.092238] time: 0:02:09.144996\n",
      "(30, 64, 64, 3)\n",
      "0.8557214\n",
      "[Epoch 2/5] [Batch 264/360] [D loss: 0.001827] [G loss: 11.063994] time: 0:02:09.255520\n",
      "(30, 64, 64, 3)\n",
      "0.8367224\n",
      "[Epoch 2/5] [Batch 265/360] [D loss: 0.001434] [G loss: 11.475991] time: 0:02:09.374916\n",
      "(30, 64, 64, 3)\n",
      "0.8581886\n",
      "[Epoch 2/5] [Batch 266/360] [D loss: 0.002007] [G loss: 10.965759] time: 0:02:09.491090\n",
      "(30, 64, 64, 3)\n",
      "0.8595653\n",
      "[Epoch 2/5] [Batch 267/360] [D loss: 0.001597] [G loss: 10.842285] time: 0:02:09.606128\n",
      "(30, 64, 64, 3)\n",
      "0.8136992\n",
      "[Epoch 2/5] [Batch 268/360] [D loss: 0.002589] [G loss: 10.928126] time: 0:02:09.721544\n",
      "(30, 64, 64, 3)\n",
      "0.7956826\n",
      "[Epoch 2/5] [Batch 269/360] [D loss: 0.001398] [G loss: 10.685077] time: 0:02:09.838877\n",
      "(30, 64, 64, 3)\n",
      "0.8115179\n",
      "[Epoch 2/5] [Batch 270/360] [D loss: 0.001914] [G loss: 11.038281] time: 0:02:09.954511\n",
      "(30, 64, 64, 3)\n",
      "0.79872686\n",
      "[Epoch 2/5] [Batch 271/360] [D loss: 0.001538] [G loss: 11.077439] time: 0:02:10.087964\n",
      "(30, 64, 64, 3)\n",
      "0.8452497\n",
      "[Epoch 2/5] [Batch 272/360] [D loss: 0.003330] [G loss: 10.948124] time: 0:02:10.199723\n",
      "(30, 64, 64, 3)\n",
      "0.814727\n",
      "[Epoch 2/5] [Batch 273/360] [D loss: 0.002015] [G loss: 11.184002] time: 0:02:10.315641\n",
      "(30, 64, 64, 3)\n",
      "0.8359472\n",
      "[Epoch 2/5] [Batch 274/360] [D loss: 0.002374] [G loss: 10.731877] time: 0:02:10.428991\n",
      "(30, 64, 64, 3)\n",
      "0.8574886\n",
      "[Epoch 2/5] [Batch 275/360] [D loss: 0.002014] [G loss: 11.156249] time: 0:02:10.549248\n",
      "(30, 64, 64, 3)\n",
      "0.8045058\n",
      "[Epoch 2/5] [Batch 276/360] [D loss: 0.004896] [G loss: 10.979691] time: 0:02:10.666203\n",
      "(30, 64, 64, 3)\n",
      "0.8521297\n",
      "[Epoch 2/5] [Batch 277/360] [D loss: 0.002537] [G loss: 11.301049] time: 0:02:10.783773\n",
      "(30, 64, 64, 3)\n",
      "0.8797164\n",
      "[Epoch 2/5] [Batch 278/360] [D loss: 0.006452] [G loss: 11.383467] time: 0:02:10.895482\n",
      "(30, 64, 64, 3)\n",
      "0.8448415\n",
      "[Epoch 2/5] [Batch 279/360] [D loss: 0.003779] [G loss: 11.415400] time: 0:02:11.017430\n",
      "(30, 64, 64, 3)\n",
      "0.8151533\n",
      "[Epoch 2/5] [Batch 280/360] [D loss: 0.009948] [G loss: 11.094884] time: 0:02:11.133519\n",
      "(30, 64, 64, 3)\n",
      "0.8351154\n",
      "[Epoch 2/5] [Batch 281/360] [D loss: 0.002588] [G loss: 10.492372] time: 0:02:11.249815\n",
      "(30, 64, 64, 3)\n",
      "0.83624226\n",
      "[Epoch 2/5] [Batch 282/360] [D loss: 0.003531] [G loss: 11.075194] time: 0:02:11.369164\n",
      "(30, 64, 64, 3)\n",
      "0.8598113\n",
      "[Epoch 2/5] [Batch 283/360] [D loss: 0.003512] [G loss: 11.044867] time: 0:02:11.489762\n",
      "(30, 64, 64, 3)\n",
      "0.8413277\n",
      "[Epoch 2/5] [Batch 284/360] [D loss: 0.010362] [G loss: 11.434493] time: 0:02:11.612862\n",
      "(30, 64, 64, 3)\n",
      "0.8082185\n",
      "[Epoch 2/5] [Batch 285/360] [D loss: 0.003736] [G loss: 10.973075] time: 0:02:11.731283\n",
      "(30, 64, 64, 3)\n",
      "0.8554182\n",
      "[Epoch 2/5] [Batch 286/360] [D loss: 0.008370] [G loss: 11.118493] time: 0:02:11.850137\n",
      "(30, 64, 64, 3)\n",
      "0.85013646\n",
      "[Epoch 2/5] [Batch 287/360] [D loss: 0.006629] [G loss: 10.935396] time: 0:02:11.971152\n",
      "(30, 64, 64, 3)\n",
      "0.82624626\n",
      "[Epoch 2/5] [Batch 288/360] [D loss: 0.123990] [G loss: 11.539941] time: 0:02:12.093465\n",
      "(30, 64, 64, 3)\n",
      "0.882566\n",
      "[Epoch 2/5] [Batch 289/360] [D loss: 0.605892] [G loss: 10.235416] time: 0:02:12.213679\n",
      "(30, 64, 64, 3)\n",
      "0.8502422\n",
      "[Epoch 2/5] [Batch 290/360] [D loss: 0.358507] [G loss: 10.355299] time: 0:02:12.332102\n",
      "(30, 64, 64, 3)\n",
      "0.8073478\n",
      "[Epoch 2/5] [Batch 291/360] [D loss: 0.116451] [G loss: 10.204720] time: 0:02:12.457813\n",
      "(30, 64, 64, 3)\n",
      "0.82408786\n",
      "[Epoch 2/5] [Batch 292/360] [D loss: 0.106182] [G loss: 10.144342] time: 0:02:12.578995\n",
      "(30, 64, 64, 3)\n",
      "0.842786\n",
      "[Epoch 2/5] [Batch 293/360] [D loss: 0.084466] [G loss: 10.495559] time: 0:02:12.701268\n",
      "(30, 64, 64, 3)\n",
      "0.88800925\n",
      "[Epoch 2/5] [Batch 294/360] [D loss: 0.030589] [G loss: 10.903447] time: 0:02:12.818853\n",
      "(30, 64, 64, 3)\n",
      "0.8860591\n",
      "[Epoch 2/5] [Batch 295/360] [D loss: 0.024510] [G loss: 10.764921] time: 0:02:12.936527\n",
      "(30, 64, 64, 3)\n",
      "0.85498\n",
      "[Epoch 2/5] [Batch 296/360] [D loss: 0.009634] [G loss: 10.425719] time: 0:02:13.052746\n",
      "(30, 64, 64, 3)\n",
      "0.82547903\n",
      "[Epoch 2/5] [Batch 297/360] [D loss: 0.306241] [G loss: 10.964815] time: 0:02:13.181351\n",
      "(30, 64, 64, 3)\n",
      "0.81625986\n",
      "[Epoch 2/5] [Batch 298/360] [D loss: 0.339520] [G loss: 10.661495] time: 0:02:13.296995\n",
      "(30, 64, 64, 3)\n",
      "0.85109216\n",
      "[Epoch 2/5] [Batch 299/360] [D loss: 0.188400] [G loss: 10.160696] time: 0:02:13.414067\n",
      "(30, 64, 64, 3)\n",
      "0.88536245\n",
      "[Epoch 2/5] [Batch 300/360] [D loss: 0.020395] [G loss: 10.525766] time: 0:02:13.531497\n",
      "(30, 64, 64, 3)\n",
      "0.8373182\n",
      "[Epoch 2/5] [Batch 301/360] [D loss: 0.062950] [G loss: 10.221532] time: 0:02:13.649256\n",
      "(30, 64, 64, 3)\n",
      "0.8350279\n",
      "[Epoch 2/5] [Batch 302/360] [D loss: 0.015057] [G loss: 10.563248] time: 0:02:13.769994\n",
      "(30, 64, 64, 3)\n",
      "0.8135584\n",
      "[Epoch 2/5] [Batch 303/360] [D loss: 0.018124] [G loss: 10.799929] time: 0:02:13.900001\n",
      "(30, 64, 64, 3)\n",
      "0.851902\n",
      "[Epoch 2/5] [Batch 304/360] [D loss: 0.008248] [G loss: 10.357463] time: 0:02:14.018057\n",
      "(30, 64, 64, 3)\n",
      "0.8377676\n",
      "[Epoch 2/5] [Batch 305/360] [D loss: 0.008234] [G loss: 10.854849] time: 0:02:14.136667\n",
      "(30, 64, 64, 3)\n",
      "0.8438212\n",
      "[Epoch 2/5] [Batch 306/360] [D loss: 0.005350] [G loss: 10.726554] time: 0:02:14.252557\n",
      "(30, 64, 64, 3)\n",
      "0.8667745\n",
      "[Epoch 2/5] [Batch 307/360] [D loss: 0.006300] [G loss: 10.385961] time: 0:02:14.367447\n",
      "(30, 64, 64, 3)\n",
      "0.79627943\n",
      "[Epoch 2/5] [Batch 308/360] [D loss: 0.004116] [G loss: 10.724878] time: 0:02:14.479489\n",
      "(30, 64, 64, 3)\n",
      "0.8354845\n",
      "[Epoch 2/5] [Batch 309/360] [D loss: 0.009615] [G loss: 10.662698] time: 0:02:14.595281\n",
      "(30, 64, 64, 3)\n",
      "0.8559373\n",
      "[Epoch 2/5] [Batch 310/360] [D loss: 0.007064] [G loss: 10.450709] time: 0:02:14.712300\n",
      "(30, 64, 64, 3)\n",
      "0.81001645\n",
      "[Epoch 2/5] [Batch 311/360] [D loss: 0.039971] [G loss: 9.903595] time: 0:02:14.832081\n",
      "(30, 64, 64, 3)\n",
      "0.7876706\n",
      "[Epoch 2/5] [Batch 312/360] [D loss: 0.016143] [G loss: 11.042831] time: 0:02:14.944663\n",
      "(30, 64, 64, 3)\n",
      "0.8428955\n",
      "[Epoch 2/5] [Batch 313/360] [D loss: 0.100471] [G loss: 10.487893] time: 0:02:15.061250\n",
      "(30, 64, 64, 3)\n",
      "0.8641016\n",
      "[Epoch 2/5] [Batch 314/360] [D loss: 0.002752] [G loss: 10.634994] time: 0:02:15.178180\n",
      "(30, 64, 64, 3)\n",
      "0.78541136\n",
      "[Epoch 2/5] [Batch 315/360] [D loss: 0.017362] [G loss: 10.308233] time: 0:02:15.295547\n",
      "(30, 64, 64, 3)\n",
      "0.90032274\n",
      "[Epoch 2/5] [Batch 316/360] [D loss: 0.014142] [G loss: 10.667692] time: 0:02:15.406568\n",
      "(30, 64, 64, 3)\n",
      "0.8497708\n",
      "[Epoch 2/5] [Batch 317/360] [D loss: 0.009711] [G loss: 10.432642] time: 0:02:15.522598\n",
      "(30, 64, 64, 3)\n",
      "0.8131633\n",
      "[Epoch 2/5] [Batch 318/360] [D loss: 0.003681] [G loss: 10.793157] time: 0:02:15.635423\n",
      "(30, 64, 64, 3)\n",
      "0.8273153\n",
      "[Epoch 2/5] [Batch 319/360] [D loss: 0.002478] [G loss: 10.487402] time: 0:02:15.748672\n",
      "(30, 64, 64, 3)\n",
      "0.8430918\n",
      "[Epoch 2/5] [Batch 320/360] [D loss: 0.002884] [G loss: 10.198953] time: 0:02:15.863641\n",
      "(30, 64, 64, 3)\n",
      "0.8303545\n",
      "[Epoch 2/5] [Batch 321/360] [D loss: 0.002376] [G loss: 10.185186] time: 0:02:15.980319\n",
      "(30, 64, 64, 3)\n",
      "0.8224495\n",
      "[Epoch 2/5] [Batch 322/360] [D loss: 0.002616] [G loss: 10.682290] time: 0:02:16.095668\n",
      "(30, 64, 64, 3)\n",
      "0.8558801\n",
      "[Epoch 2/5] [Batch 323/360] [D loss: 0.003109] [G loss: 10.474634] time: 0:02:16.211324\n",
      "(30, 64, 64, 3)\n",
      "0.8395116\n",
      "[Epoch 2/5] [Batch 324/360] [D loss: 0.002929] [G loss: 10.497635] time: 0:02:16.327362\n",
      "(30, 64, 64, 3)\n",
      "0.8139758\n",
      "[Epoch 2/5] [Batch 325/360] [D loss: 0.002746] [G loss: 10.321259] time: 0:02:16.447857\n",
      "(30, 64, 64, 3)\n",
      "0.87108517\n",
      "[Epoch 2/5] [Batch 326/360] [D loss: 0.002873] [G loss: 9.883098] time: 0:02:16.562365\n",
      "(30, 64, 64, 3)\n",
      "0.8275473\n",
      "[Epoch 2/5] [Batch 327/360] [D loss: 0.002399] [G loss: 10.272642] time: 0:02:16.678309\n",
      "(30, 64, 64, 3)\n",
      "0.7569807\n",
      "[Epoch 2/5] [Batch 328/360] [D loss: 0.002353] [G loss: 10.562722] time: 0:02:16.792502\n",
      "(30, 64, 64, 3)\n",
      "0.8814027\n",
      "[Epoch 2/5] [Batch 329/360] [D loss: 0.002661] [G loss: 10.244437] time: 0:02:16.914063\n",
      "(30, 64, 64, 3)\n",
      "0.8444932\n",
      "[Epoch 2/5] [Batch 330/360] [D loss: 0.002011] [G loss: 10.049240] time: 0:02:17.028929\n",
      "(30, 64, 64, 3)\n",
      "0.8195916\n",
      "[Epoch 2/5] [Batch 331/360] [D loss: 0.002247] [G loss: 9.862649] time: 0:02:17.145579\n",
      "(30, 64, 64, 3)\n",
      "0.8455774\n",
      "[Epoch 2/5] [Batch 332/360] [D loss: 0.002618] [G loss: 9.714381] time: 0:02:17.259084\n",
      "(30, 64, 64, 3)\n",
      "0.8377635\n",
      "[Epoch 2/5] [Batch 333/360] [D loss: 0.002118] [G loss: 10.261379] time: 0:02:17.377132\n",
      "(30, 64, 64, 3)\n",
      "0.8475316\n",
      "[Epoch 2/5] [Batch 334/360] [D loss: 0.002242] [G loss: 10.205773] time: 0:02:17.491999\n",
      "(30, 64, 64, 3)\n",
      "0.85726625\n",
      "[Epoch 2/5] [Batch 335/360] [D loss: 0.002902] [G loss: 10.173405] time: 0:02:17.610810\n",
      "(30, 64, 64, 3)\n",
      "0.86310774\n",
      "[Epoch 2/5] [Batch 336/360] [D loss: 0.002052] [G loss: 10.219726] time: 0:02:17.727391\n",
      "(30, 64, 64, 3)\n",
      "0.8416855\n",
      "[Epoch 2/5] [Batch 337/360] [D loss: 0.002585] [G loss: 9.456897] time: 0:02:17.839142\n",
      "(30, 64, 64, 3)\n",
      "0.8752284\n",
      "[Epoch 2/5] [Batch 338/360] [D loss: 0.001590] [G loss: 10.399918] time: 0:02:17.960842\n",
      "(30, 64, 64, 3)\n",
      "0.81894875\n",
      "[Epoch 2/5] [Batch 339/360] [D loss: 0.002655] [G loss: 10.162286] time: 0:02:18.088737\n",
      "(30, 64, 64, 3)\n",
      "0.8455922\n",
      "[Epoch 2/5] [Batch 340/360] [D loss: 0.002714] [G loss: 10.019846] time: 0:02:18.202142\n",
      "(30, 64, 64, 3)\n",
      "0.805182\n",
      "[Epoch 2/5] [Batch 341/360] [D loss: 0.002139] [G loss: 10.362100] time: 0:02:18.325724\n",
      "(30, 64, 64, 3)\n",
      "0.84412974\n",
      "[Epoch 2/5] [Batch 342/360] [D loss: 0.002233] [G loss: 9.869324] time: 0:02:18.440098\n",
      "(30, 64, 64, 3)\n",
      "0.8393435\n",
      "[Epoch 2/5] [Batch 343/360] [D loss: 0.002298] [G loss: 10.115280] time: 0:02:18.557003\n",
      "(30, 64, 64, 3)\n",
      "0.88617104\n",
      "[Epoch 2/5] [Batch 344/360] [D loss: 0.002554] [G loss: 10.338230] time: 0:02:18.669135\n",
      "(30, 64, 64, 3)\n",
      "0.86958575\n",
      "[Epoch 2/5] [Batch 345/360] [D loss: 0.002100] [G loss: 9.957190] time: 0:02:18.780751\n",
      "(30, 64, 64, 3)\n",
      "0.7718327\n",
      "[Epoch 2/5] [Batch 346/360] [D loss: 0.002084] [G loss: 10.461267] time: 0:02:18.892690\n",
      "(30, 64, 64, 3)\n",
      "0.8162486\n",
      "[Epoch 2/5] [Batch 347/360] [D loss: 0.001796] [G loss: 10.130079] time: 0:02:19.015429\n",
      "(30, 64, 64, 3)\n",
      "0.86447114\n",
      "[Epoch 2/5] [Batch 348/360] [D loss: 0.003867] [G loss: 10.129871] time: 0:02:19.128245\n",
      "(30, 64, 64, 3)\n",
      "0.82470125\n",
      "[Epoch 2/5] [Batch 349/360] [D loss: 0.002044] [G loss: 10.356775] time: 0:02:19.256033\n",
      "(30, 64, 64, 3)\n",
      "0.86367196\n",
      "[Epoch 2/5] [Batch 350/360] [D loss: 0.002092] [G loss: 9.737739] time: 0:02:19.370106\n",
      "(30, 64, 64, 3)\n",
      "0.82608557\n",
      "[Epoch 2/5] [Batch 351/360] [D loss: 0.001745] [G loss: 10.045995] time: 0:02:19.493231\n",
      "(30, 64, 64, 3)\n",
      "0.8446262\n",
      "[Epoch 2/5] [Batch 352/360] [D loss: 0.002100] [G loss: 10.047675] time: 0:02:19.606262\n",
      "(30, 64, 64, 3)\n",
      "0.87005454\n",
      "[Epoch 2/5] [Batch 353/360] [D loss: 0.002648] [G loss: 9.911969] time: 0:02:19.719718\n",
      "(30, 64, 64, 3)\n",
      "0.8217287\n",
      "[Epoch 2/5] [Batch 354/360] [D loss: 0.001726] [G loss: 9.990673] time: 0:02:19.831710\n",
      "(30, 64, 64, 3)\n",
      "0.8178434\n",
      "[Epoch 2/5] [Batch 355/360] [D loss: 0.002081] [G loss: 9.957809] time: 0:02:19.951366\n",
      "(30, 64, 64, 3)\n",
      "0.888807\n",
      "[Epoch 2/5] [Batch 356/360] [D loss: 0.001410] [G loss: 9.618997] time: 0:02:20.067457\n",
      "(30, 64, 64, 3)\n",
      "0.82428783\n",
      "[Epoch 2/5] [Batch 357/360] [D loss: 0.001940] [G loss: 9.863944] time: 0:02:20.181252\n",
      "(30, 64, 64, 3)\n",
      "0.81483406\n",
      "[Epoch 2/5] [Batch 358/360] [D loss: 0.001916] [G loss: 9.753997] time: 0:02:20.293718\n",
      "(30, 64, 64, 3)\n",
      "0.8552027\n",
      "[Epoch 2/5] [Batch 359/360] [D loss: 0.001777] [G loss: 9.972507] time: 0:02:20.407404\n",
      "(30, 64, 64, 3)\n",
      "0.84558296\n",
      "[Epoch 3/5] [Batch 0/360] [D loss: 0.001963] [G loss: 9.410977] time: 0:02:20.524337\n",
      "(30, 64, 64, 3)\n",
      "0.8614724\n",
      "[Epoch 3/5] [Batch 2/360] [D loss: 0.001880] [G loss: 9.971770] time: 0:02:20.661767\n",
      "(30, 64, 64, 3)\n",
      "0.86219007\n",
      "[Epoch 3/5] [Batch 3/360] [D loss: 0.002278] [G loss: 10.029014] time: 0:02:20.778880\n",
      "(30, 64, 64, 3)\n",
      "0.8379507\n",
      "[Epoch 3/5] [Batch 4/360] [D loss: 0.002191] [G loss: 11.326733] time: 0:02:20.891733\n",
      "(30, 64, 64, 3)\n",
      "0.7817467\n",
      "[Epoch 3/5] [Batch 5/360] [D loss: 0.002102] [G loss: 9.884821] time: 0:02:21.006853\n",
      "(30, 64, 64, 3)\n",
      "0.845073\n",
      "[Epoch 3/5] [Batch 6/360] [D loss: 0.002802] [G loss: 9.364900] time: 0:02:21.122287\n",
      "(30, 64, 64, 3)\n",
      "0.84406114\n",
      "[Epoch 3/5] [Batch 7/360] [D loss: 0.001848] [G loss: 9.728912] time: 0:02:21.237056\n",
      "(30, 64, 64, 3)\n",
      "0.84324265\n",
      "[Epoch 3/5] [Batch 8/360] [D loss: 0.002466] [G loss: 9.754891] time: 0:02:21.350511\n",
      "(30, 64, 64, 3)\n",
      "0.85452944\n",
      "[Epoch 3/5] [Batch 9/360] [D loss: 0.001394] [G loss: 9.777670] time: 0:02:21.463690\n",
      "(30, 64, 64, 3)\n",
      "0.8763037\n",
      "[Epoch 3/5] [Batch 10/360] [D loss: 0.002995] [G loss: 9.448690] time: 0:02:21.582472\n",
      "(30, 64, 64, 3)\n",
      "0.88437\n",
      "[Epoch 3/5] [Batch 11/360] [D loss: 0.002150] [G loss: 9.692674] time: 0:02:21.694182\n",
      "(30, 64, 64, 3)\n",
      "0.86754984\n",
      "[Epoch 3/5] [Batch 12/360] [D loss: 0.003664] [G loss: 9.769579] time: 0:02:21.806011\n",
      "(30, 64, 64, 3)\n",
      "0.85241014\n",
      "[Epoch 3/5] [Batch 13/360] [D loss: 0.002265] [G loss: 9.403501] time: 0:02:21.921333\n",
      "(30, 64, 64, 3)\n",
      "0.85868376\n",
      "[Epoch 3/5] [Batch 14/360] [D loss: 0.002404] [G loss: 9.592263] time: 0:02:22.040929\n",
      "(30, 64, 64, 3)\n",
      "0.8480789\n",
      "[Epoch 3/5] [Batch 15/360] [D loss: 0.001909] [G loss: 10.013638] time: 0:02:22.157458\n",
      "(30, 64, 64, 3)\n",
      "0.83987737\n",
      "[Epoch 3/5] [Batch 16/360] [D loss: 0.002240] [G loss: 9.270451] time: 0:02:22.272214\n",
      "(30, 64, 64, 3)\n",
      "0.8591493\n",
      "[Epoch 3/5] [Batch 17/360] [D loss: 0.001647] [G loss: 9.750846] time: 0:02:22.383852\n",
      "(30, 64, 64, 3)\n",
      "0.790215\n",
      "[Epoch 3/5] [Batch 18/360] [D loss: 0.002493] [G loss: 10.032991] time: 0:02:22.497931\n",
      "(30, 64, 64, 3)\n",
      "0.85486174\n",
      "[Epoch 3/5] [Batch 19/360] [D loss: 0.001812] [G loss: 9.755960] time: 0:02:22.615748\n",
      "(30, 64, 64, 3)\n",
      "0.85707504\n",
      "[Epoch 3/5] [Batch 20/360] [D loss: 0.001773] [G loss: 9.826967] time: 0:02:22.732098\n",
      "(30, 64, 64, 3)\n",
      "0.84486586\n",
      "[Epoch 3/5] [Batch 21/360] [D loss: 0.001951] [G loss: 9.841202] time: 0:02:22.845952\n",
      "(30, 64, 64, 3)\n",
      "0.866858\n",
      "[Epoch 3/5] [Batch 22/360] [D loss: 0.002017] [G loss: 9.423040] time: 0:02:22.967000\n",
      "(30, 64, 64, 3)\n",
      "0.8343632\n",
      "[Epoch 3/5] [Batch 23/360] [D loss: 0.001666] [G loss: 9.728006] time: 0:02:23.081251\n",
      "(30, 64, 64, 3)\n",
      "0.86649704\n",
      "[Epoch 3/5] [Batch 24/360] [D loss: 0.003147] [G loss: 9.734717] time: 0:02:23.196543\n",
      "(30, 64, 64, 3)\n",
      "0.808245\n",
      "[Epoch 3/5] [Batch 25/360] [D loss: 0.002017] [G loss: 9.962825] time: 0:02:23.309204\n",
      "(30, 64, 64, 3)\n",
      "0.8667515\n",
      "[Epoch 3/5] [Batch 26/360] [D loss: 0.002176] [G loss: 9.933420] time: 0:02:23.429633\n",
      "(30, 64, 64, 3)\n",
      "0.8884673\n",
      "[Epoch 3/5] [Batch 27/360] [D loss: 0.003240] [G loss: 9.725723] time: 0:02:23.541863\n",
      "(30, 64, 64, 3)\n",
      "0.87231135\n",
      "[Epoch 3/5] [Batch 28/360] [D loss: 0.002531] [G loss: 9.430475] time: 0:02:23.655653\n",
      "(30, 64, 64, 3)\n",
      "0.8283117\n",
      "[Epoch 3/5] [Batch 29/360] [D loss: 0.002075] [G loss: 9.537074] time: 0:02:23.766370\n",
      "(30, 64, 64, 3)\n",
      "0.8179841\n",
      "[Epoch 3/5] [Batch 30/360] [D loss: 0.003080] [G loss: 10.174437] time: 0:02:23.878938\n",
      "(30, 64, 64, 3)\n",
      "0.8264515\n",
      "[Epoch 3/5] [Batch 31/360] [D loss: 0.002776] [G loss: 9.579630] time: 0:02:23.993679\n",
      "(30, 64, 64, 3)\n",
      "0.85589916\n",
      "[Epoch 3/5] [Batch 32/360] [D loss: 0.006521] [G loss: 9.004028] time: 0:02:24.108915\n",
      "(30, 64, 64, 3)\n",
      "0.8033636\n",
      "[Epoch 3/5] [Batch 33/360] [D loss: 0.003972] [G loss: 9.308249] time: 0:02:24.232617\n",
      "(30, 64, 64, 3)\n",
      "0.80878705\n",
      "[Epoch 3/5] [Batch 34/360] [D loss: 0.006659] [G loss: 9.560420] time: 0:02:24.349144\n",
      "(30, 64, 64, 3)\n",
      "0.8696119\n",
      "[Epoch 3/5] [Batch 35/360] [D loss: 0.003778] [G loss: 9.303199] time: 0:02:24.461657\n",
      "(30, 64, 64, 3)\n",
      "0.8987239\n",
      "[Epoch 3/5] [Batch 36/360] [D loss: 0.006387] [G loss: 9.604029] time: 0:02:24.576482\n",
      "(30, 64, 64, 3)\n",
      "0.8764489\n",
      "[Epoch 3/5] [Batch 37/360] [D loss: 0.004539] [G loss: 9.298489] time: 0:02:24.692172\n",
      "(30, 64, 64, 3)\n",
      "0.85520744\n",
      "[Epoch 3/5] [Batch 38/360] [D loss: 0.008316] [G loss: 9.379424] time: 0:02:24.810068\n",
      "(30, 64, 64, 3)\n",
      "0.80471057\n",
      "[Epoch 3/5] [Batch 39/360] [D loss: 0.004665] [G loss: 9.560516] time: 0:02:24.922605\n",
      "(30, 64, 64, 3)\n",
      "0.82013005\n",
      "[Epoch 3/5] [Batch 40/360] [D loss: 0.012473] [G loss: 9.630503] time: 0:02:25.041297\n",
      "(30, 64, 64, 3)\n",
      "0.844434\n",
      "[Epoch 3/5] [Batch 41/360] [D loss: 0.004887] [G loss: 9.482474] time: 0:02:25.152215\n",
      "(30, 64, 64, 3)\n",
      "0.88405603\n",
      "[Epoch 3/5] [Batch 42/360] [D loss: 0.017811] [G loss: 8.983130] time: 0:02:25.270855\n",
      "(30, 64, 64, 3)\n",
      "0.8940633\n",
      "[Epoch 3/5] [Batch 43/360] [D loss: 0.006816] [G loss: 9.461520] time: 0:02:25.384248\n",
      "(30, 64, 64, 3)\n",
      "0.86301404\n",
      "[Epoch 3/5] [Batch 44/360] [D loss: 0.012329] [G loss: 9.338877] time: 0:02:25.498676\n",
      "(30, 64, 64, 3)\n",
      "0.8376179\n",
      "[Epoch 3/5] [Batch 45/360] [D loss: 0.005723] [G loss: 9.517518] time: 0:02:25.612279\n",
      "(30, 64, 64, 3)\n",
      "0.8456523\n",
      "[Epoch 3/5] [Batch 46/360] [D loss: 0.007714] [G loss: 9.512231] time: 0:02:25.726693\n",
      "(30, 64, 64, 3)\n",
      "0.8614144\n",
      "[Epoch 3/5] [Batch 47/360] [D loss: 0.002086] [G loss: 9.014482] time: 0:02:25.843494\n",
      "(30, 64, 64, 3)\n",
      "0.8366294\n",
      "[Epoch 3/5] [Batch 48/360] [D loss: 0.003307] [G loss: 9.449696] time: 0:02:25.962206\n",
      "(30, 64, 64, 3)\n",
      "0.85918266\n",
      "[Epoch 3/5] [Batch 49/360] [D loss: 0.002388] [G loss: 9.453706] time: 0:02:26.076730\n",
      "(30, 64, 64, 3)\n",
      "0.8474825\n",
      "[Epoch 3/5] [Batch 50/360] [D loss: 0.002791] [G loss: 9.755759] time: 0:02:26.193297\n",
      "(30, 64, 64, 3)\n",
      "0.84381896\n",
      "[Epoch 3/5] [Batch 51/360] [D loss: 0.001658] [G loss: 8.989955] time: 0:02:26.308598\n",
      "(30, 64, 64, 3)\n",
      "0.8391573\n",
      "[Epoch 3/5] [Batch 52/360] [D loss: 0.004292] [G loss: 9.440296] time: 0:02:26.424226\n",
      "(30, 64, 64, 3)\n",
      "0.8460419\n",
      "[Epoch 3/5] [Batch 53/360] [D loss: 0.001798] [G loss: 9.695004] time: 0:02:26.540073\n",
      "(30, 64, 64, 3)\n",
      "0.859874\n",
      "[Epoch 3/5] [Batch 54/360] [D loss: 0.001792] [G loss: 9.690814] time: 0:02:26.658619\n",
      "(30, 64, 64, 3)\n",
      "0.87935996\n",
      "[Epoch 3/5] [Batch 55/360] [D loss: 0.001928] [G loss: 9.210919] time: 0:02:26.773464\n",
      "(30, 64, 64, 3)\n",
      "0.84566337\n",
      "[Epoch 3/5] [Batch 56/360] [D loss: 0.002124] [G loss: 9.870480] time: 0:02:26.887147\n",
      "(30, 64, 64, 3)\n",
      "0.87804574\n",
      "[Epoch 3/5] [Batch 57/360] [D loss: 0.002657] [G loss: 8.886973] time: 0:02:27.001420\n",
      "(30, 64, 64, 3)\n",
      "0.84943444\n",
      "[Epoch 3/5] [Batch 58/360] [D loss: 0.002051] [G loss: 9.114467] time: 0:02:27.120403\n",
      "(30, 64, 64, 3)\n",
      "0.8487158\n",
      "[Epoch 3/5] [Batch 59/360] [D loss: 0.002085] [G loss: 8.890168] time: 0:02:27.235587\n",
      "(30, 64, 64, 3)\n",
      "0.83110756\n",
      "[Epoch 3/5] [Batch 60/360] [D loss: 0.001653] [G loss: 9.824347] time: 0:02:27.364280\n",
      "(30, 64, 64, 3)\n",
      "0.8206689\n",
      "[Epoch 3/5] [Batch 61/360] [D loss: 0.003407] [G loss: 9.043693] time: 0:02:27.477066\n",
      "(30, 64, 64, 3)\n",
      "0.8016309\n",
      "[Epoch 3/5] [Batch 62/360] [D loss: 0.001933] [G loss: 9.422869] time: 0:02:27.594694\n",
      "(30, 64, 64, 3)\n",
      "0.87498426\n",
      "[Epoch 3/5] [Batch 63/360] [D loss: 0.002287] [G loss: 9.209704] time: 0:02:27.706263\n",
      "(30, 64, 64, 3)\n",
      "0.91014385\n",
      "[Epoch 3/5] [Batch 64/360] [D loss: 0.001896] [G loss: 8.817125] time: 0:02:27.824628\n",
      "(30, 64, 64, 3)\n",
      "0.85936135\n",
      "[Epoch 3/5] [Batch 65/360] [D loss: 0.002513] [G loss: 8.666714] time: 0:02:27.937627\n",
      "(30, 64, 64, 3)\n",
      "0.87296504\n",
      "[Epoch 3/5] [Batch 66/360] [D loss: 0.002003] [G loss: 8.879902] time: 0:02:28.054199\n",
      "(30, 64, 64, 3)\n",
      "0.8527489\n",
      "[Epoch 3/5] [Batch 67/360] [D loss: 0.002400] [G loss: 8.865849] time: 0:02:28.167830\n",
      "(30, 64, 64, 3)\n",
      "0.80377775\n",
      "[Epoch 3/5] [Batch 68/360] [D loss: 0.002000] [G loss: 9.130593] time: 0:02:28.281328\n",
      "(30, 64, 64, 3)\n",
      "0.76895934\n",
      "[Epoch 3/5] [Batch 69/360] [D loss: 0.002946] [G loss: 9.719142] time: 0:02:28.397473\n",
      "(30, 64, 64, 3)\n",
      "0.86945605\n",
      "[Epoch 3/5] [Batch 70/360] [D loss: 0.002440] [G loss: 8.863896] time: 0:02:28.515900\n",
      "(30, 64, 64, 3)\n",
      "0.84829634\n",
      "[Epoch 3/5] [Batch 71/360] [D loss: 0.003321] [G loss: 9.276434] time: 0:02:28.630049\n",
      "(30, 64, 64, 3)\n",
      "0.85214144\n",
      "[Epoch 3/5] [Batch 72/360] [D loss: 0.003164] [G loss: 8.843845] time: 0:02:28.747851\n",
      "(30, 64, 64, 3)\n",
      "0.8278857\n",
      "[Epoch 3/5] [Batch 73/360] [D loss: 0.003088] [G loss: 9.077366] time: 0:02:28.864293\n",
      "(30, 64, 64, 3)\n",
      "0.82656765\n",
      "[Epoch 3/5] [Batch 74/360] [D loss: 0.001552] [G loss: 9.620190] time: 0:02:28.981040\n",
      "(30, 64, 64, 3)\n",
      "0.84128976\n",
      "[Epoch 3/5] [Batch 75/360] [D loss: 0.003687] [G loss: 9.035836] time: 0:02:29.101524\n",
      "(30, 64, 64, 3)\n",
      "0.86469954\n",
      "[Epoch 3/5] [Batch 76/360] [D loss: 0.003034] [G loss: 8.643601] time: 0:02:29.236240\n",
      "(30, 64, 64, 3)\n",
      "0.8357989\n",
      "[Epoch 3/5] [Batch 77/360] [D loss: 0.002755] [G loss: 8.986193] time: 0:02:29.350542\n",
      "(30, 64, 64, 3)\n",
      "0.850197\n",
      "[Epoch 3/5] [Batch 78/360] [D loss: 0.001877] [G loss: 9.202651] time: 0:02:29.473379\n",
      "(30, 64, 64, 3)\n",
      "0.8554155\n",
      "[Epoch 3/5] [Batch 79/360] [D loss: 0.002264] [G loss: 8.313989] time: 0:02:29.586379\n",
      "(30, 64, 64, 3)\n",
      "0.81711453\n",
      "[Epoch 3/5] [Batch 80/360] [D loss: 0.001955] [G loss: 8.966487] time: 0:02:29.704311\n",
      "(30, 64, 64, 3)\n",
      "0.89506745\n",
      "[Epoch 3/5] [Batch 81/360] [D loss: 0.001759] [G loss: 9.002005] time: 0:02:29.819373\n",
      "(30, 64, 64, 3)\n",
      "0.8116803\n",
      "[Epoch 3/5] [Batch 82/360] [D loss: 0.002264] [G loss: 9.276550] time: 0:02:29.938206\n",
      "(30, 64, 64, 3)\n",
      "0.8602056\n",
      "[Epoch 3/5] [Batch 83/360] [D loss: 0.004466] [G loss: 9.319942] time: 0:02:30.054771\n",
      "(30, 64, 64, 3)\n",
      "0.86049294\n",
      "[Epoch 3/5] [Batch 84/360] [D loss: 0.002020] [G loss: 8.753443] time: 0:02:30.169702\n",
      "(30, 64, 64, 3)\n",
      "0.83157206\n",
      "[Epoch 3/5] [Batch 85/360] [D loss: 0.007405] [G loss: 9.136863] time: 0:02:30.282729\n",
      "(30, 64, 64, 3)\n",
      "0.8793721\n",
      "[Epoch 3/5] [Batch 86/360] [D loss: 0.002325] [G loss: 8.995714] time: 0:02:30.398276\n",
      "(30, 64, 64, 3)\n",
      "0.8506799\n",
      "[Epoch 3/5] [Batch 87/360] [D loss: 0.003644] [G loss: 8.988168] time: 0:02:30.516221\n",
      "(30, 64, 64, 3)\n",
      "0.8395264\n",
      "[Epoch 3/5] [Batch 88/360] [D loss: 0.005384] [G loss: 9.178043] time: 0:02:30.633708\n",
      "(30, 64, 64, 3)\n",
      "0.90337753\n",
      "[Epoch 3/5] [Batch 89/360] [D loss: 0.024371] [G loss: 8.716798] time: 0:02:30.747944\n",
      "(30, 64, 64, 3)\n",
      "0.83222073\n",
      "[Epoch 3/5] [Batch 90/360] [D loss: 0.003678] [G loss: 8.819526] time: 0:02:30.865461\n",
      "(30, 64, 64, 3)\n",
      "0.8861608\n",
      "[Epoch 3/5] [Batch 91/360] [D loss: 0.009940] [G loss: 8.653614] time: 0:02:30.982920\n",
      "(30, 64, 64, 3)\n",
      "0.8708057\n",
      "[Epoch 3/5] [Batch 92/360] [D loss: 0.012254] [G loss: 9.107785] time: 0:02:31.098931\n",
      "(30, 64, 64, 3)\n",
      "0.8237665\n",
      "[Epoch 3/5] [Batch 93/360] [D loss: 0.036180] [G loss: 9.393577] time: 0:02:31.212091\n",
      "(30, 64, 64, 3)\n",
      "0.85183436\n",
      "[Epoch 3/5] [Batch 94/360] [D loss: 0.004126] [G loss: 8.782240] time: 0:02:31.325438\n",
      "(30, 64, 64, 3)\n",
      "0.8136694\n",
      "[Epoch 3/5] [Batch 95/360] [D loss: 0.181587] [G loss: 9.445140] time: 0:02:31.440807\n",
      "(30, 64, 64, 3)\n",
      "0.84597176\n",
      "[Epoch 3/5] [Batch 96/360] [D loss: 0.558080] [G loss: 8.870576] time: 0:02:31.558107\n",
      "(30, 64, 64, 3)\n",
      "0.85058117\n",
      "[Epoch 3/5] [Batch 97/360] [D loss: 0.347041] [G loss: 7.780938] time: 0:02:31.670508\n",
      "(30, 64, 64, 3)\n",
      "0.85483485\n",
      "[Epoch 3/5] [Batch 98/360] [D loss: 0.137377] [G loss: 8.851110] time: 0:02:31.783574\n",
      "(30, 64, 64, 3)\n",
      "0.85411996\n",
      "[Epoch 3/5] [Batch 99/360] [D loss: 0.075332] [G loss: 8.296818] time: 0:02:31.896538\n",
      "(30, 64, 64, 3)\n",
      "0.83416146\n",
      "[Epoch 3/5] [Batch 100/360] [D loss: 0.026734] [G loss: 8.782244] time: 0:02:32.013376\n",
      "(30, 64, 64, 3)\n",
      "0.8817761\n",
      "[Epoch 3/5] [Batch 101/360] [D loss: 0.031905] [G loss: 8.257742] time: 0:02:32.126244\n",
      "(30, 64, 64, 3)\n",
      "0.88118505\n",
      "[Epoch 3/5] [Batch 102/360] [D loss: 0.009139] [G loss: 9.244946] time: 0:02:32.239272\n",
      "(30, 64, 64, 3)\n",
      "0.8169484\n",
      "[Epoch 3/5] [Batch 103/360] [D loss: 0.025065] [G loss: 8.641173] time: 0:02:32.352860\n",
      "(30, 64, 64, 3)\n",
      "0.9041202\n",
      "[Epoch 3/5] [Batch 104/360] [D loss: 0.007573] [G loss: 9.032695] time: 0:02:32.465804\n",
      "(30, 64, 64, 3)\n",
      "0.8429753\n",
      "[Epoch 3/5] [Batch 105/360] [D loss: 0.043546] [G loss: 8.807038] time: 0:02:32.581753\n",
      "(30, 64, 64, 3)\n",
      "0.8821055\n",
      "[Epoch 3/5] [Batch 106/360] [D loss: 0.005625] [G loss: 9.069892] time: 0:02:32.698443\n",
      "(30, 64, 64, 3)\n",
      "0.8174951\n",
      "[Epoch 3/5] [Batch 107/360] [D loss: 0.027713] [G loss: 8.411716] time: 0:02:32.812532\n",
      "(30, 64, 64, 3)\n",
      "0.81999713\n",
      "[Epoch 3/5] [Batch 108/360] [D loss: 0.003929] [G loss: 8.663436] time: 0:02:32.930714\n",
      "(30, 64, 64, 3)\n",
      "0.8341007\n",
      "[Epoch 3/5] [Batch 109/360] [D loss: 0.077378] [G loss: 8.522211] time: 0:02:33.051245\n",
      "(30, 64, 64, 3)\n",
      "0.79797363\n",
      "[Epoch 3/5] [Batch 110/360] [D loss: 0.107279] [G loss: 8.907894] time: 0:02:33.166414\n",
      "(30, 64, 64, 3)\n",
      "0.8786123\n",
      "[Epoch 3/5] [Batch 111/360] [D loss: 0.027013] [G loss: 8.213216] time: 0:02:33.281459\n",
      "(30, 64, 64, 3)\n",
      "0.8532923\n",
      "[Epoch 3/5] [Batch 112/360] [D loss: 0.028899] [G loss: 8.805760] time: 0:02:33.396848\n",
      "(30, 64, 64, 3)\n",
      "0.8476181\n",
      "[Epoch 3/5] [Batch 113/360] [D loss: 0.232141] [G loss: 8.618880] time: 0:02:33.510402\n",
      "(30, 64, 64, 3)\n",
      "0.81773967\n",
      "[Epoch 3/5] [Batch 114/360] [D loss: 0.310083] [G loss: 8.404543] time: 0:02:33.643685\n",
      "(30, 64, 64, 3)\n",
      "0.89380294\n",
      "[Epoch 3/5] [Batch 115/360] [D loss: 0.020029] [G loss: 8.719715] time: 0:02:33.758134\n",
      "(30, 64, 64, 3)\n",
      "0.87316704\n",
      "[Epoch 3/5] [Batch 116/360] [D loss: 0.034941] [G loss: 8.134004] time: 0:02:33.873881\n",
      "(30, 64, 64, 3)\n",
      "0.8344988\n",
      "[Epoch 3/5] [Batch 117/360] [D loss: 0.019578] [G loss: 8.656510] time: 0:02:33.985303\n",
      "(30, 64, 64, 3)\n",
      "0.8532049\n",
      "[Epoch 3/5] [Batch 118/360] [D loss: 0.008611] [G loss: 8.835078] time: 0:02:34.110070\n",
      "(30, 64, 64, 3)\n",
      "0.8910367\n",
      "[Epoch 3/5] [Batch 119/360] [D loss: 0.006864] [G loss: 8.532395] time: 0:02:34.222605\n",
      "(30, 64, 64, 3)\n",
      "0.87838477\n",
      "[Epoch 3/5] [Batch 120/360] [D loss: 0.005252] [G loss: 8.227019] time: 0:02:34.339151\n",
      "(30, 64, 64, 3)\n",
      "0.83125204\n",
      "[Epoch 3/5] [Batch 121/360] [D loss: 0.003851] [G loss: 8.425342] time: 0:02:34.455927\n",
      "(30, 64, 64, 3)\n",
      "0.8479077\n",
      "[Epoch 3/5] [Batch 122/360] [D loss: 0.003272] [G loss: 7.958748] time: 0:02:34.584686\n",
      "(30, 64, 64, 3)\n",
      "0.87613183\n",
      "[Epoch 3/5] [Batch 123/360] [D loss: 0.003248] [G loss: 8.560013] time: 0:02:34.703574\n",
      "(30, 64, 64, 3)\n",
      "0.922074\n",
      "[Epoch 3/5] [Batch 124/360] [D loss: 0.004391] [G loss: 8.767846] time: 0:02:34.818900\n",
      "(30, 64, 64, 3)\n",
      "0.85353804\n",
      "[Epoch 3/5] [Batch 125/360] [D loss: 0.002281] [G loss: 8.531373] time: 0:02:34.933532\n",
      "(30, 64, 64, 3)\n",
      "0.89482117\n",
      "[Epoch 3/5] [Batch 126/360] [D loss: 0.003144] [G loss: 8.108580] time: 0:02:35.052764\n",
      "(30, 64, 64, 3)\n",
      "0.8614976\n",
      "[Epoch 3/5] [Batch 127/360] [D loss: 0.002519] [G loss: 8.543246] time: 0:02:35.165696\n",
      "(30, 64, 64, 3)\n",
      "0.88799137\n",
      "[Epoch 3/5] [Batch 128/360] [D loss: 0.002499] [G loss: 8.713467] time: 0:02:35.284276\n",
      "(30, 64, 64, 3)\n",
      "0.8477991\n",
      "[Epoch 3/5] [Batch 129/360] [D loss: 0.002881] [G loss: 9.146462] time: 0:02:35.400656\n",
      "(30, 64, 64, 3)\n",
      "0.86649054\n",
      "[Epoch 3/5] [Batch 130/360] [D loss: 0.002983] [G loss: 8.276900] time: 0:02:35.517710\n",
      "(30, 64, 64, 3)\n",
      "0.8498139\n",
      "[Epoch 3/5] [Batch 131/360] [D loss: 0.002649] [G loss: 8.409660] time: 0:02:35.630620\n",
      "(30, 64, 64, 3)\n",
      "0.8709946\n",
      "[Epoch 3/5] [Batch 132/360] [D loss: 0.003313] [G loss: 8.816785] time: 0:02:35.751943\n",
      "(30, 64, 64, 3)\n",
      "0.8367124\n",
      "[Epoch 3/5] [Batch 133/360] [D loss: 0.003019] [G loss: 9.208386] time: 0:02:35.867816\n",
      "(30, 64, 64, 3)\n",
      "0.8478654\n",
      "[Epoch 3/5] [Batch 134/360] [D loss: 0.002597] [G loss: 8.062478] time: 0:02:35.990222\n",
      "(30, 64, 64, 3)\n",
      "0.90086824\n",
      "[Epoch 3/5] [Batch 135/360] [D loss: 0.003409] [G loss: 8.464594] time: 0:02:36.111846\n",
      "(30, 64, 64, 3)\n",
      "0.8491831\n",
      "[Epoch 3/5] [Batch 136/360] [D loss: 0.002963] [G loss: 8.322080] time: 0:02:36.227972\n",
      "(30, 64, 64, 3)\n",
      "0.849453\n",
      "[Epoch 3/5] [Batch 137/360] [D loss: 0.003023] [G loss: 8.120786] time: 0:02:36.339139\n",
      "(30, 64, 64, 3)\n",
      "0.85525966\n",
      "[Epoch 3/5] [Batch 138/360] [D loss: 0.002785] [G loss: 8.494962] time: 0:02:36.454684\n",
      "(30, 64, 64, 3)\n",
      "0.9056695\n",
      "[Epoch 3/5] [Batch 139/360] [D loss: 0.002173] [G loss: 8.193686] time: 0:02:36.568924\n",
      "(30, 64, 64, 3)\n",
      "0.8537927\n",
      "[Epoch 3/5] [Batch 140/360] [D loss: 0.001908] [G loss: 8.439573] time: 0:02:36.680716\n",
      "(30, 64, 64, 3)\n",
      "0.87264794\n",
      "[Epoch 3/5] [Batch 141/360] [D loss: 0.002558] [G loss: 8.853749] time: 0:02:36.792767\n",
      "(30, 64, 64, 3)\n",
      "0.8568209\n",
      "[Epoch 3/5] [Batch 142/360] [D loss: 0.002256] [G loss: 8.320137] time: 0:02:36.907966\n",
      "(30, 64, 64, 3)\n",
      "0.8811588\n",
      "[Epoch 3/5] [Batch 143/360] [D loss: 0.003381] [G loss: 8.252646] time: 0:02:37.020936\n",
      "(30, 64, 64, 3)\n",
      "0.8370215\n",
      "[Epoch 3/5] [Batch 144/360] [D loss: 0.002634] [G loss: 8.680453] time: 0:02:37.135477\n",
      "(30, 64, 64, 3)\n",
      "0.8438335\n",
      "[Epoch 3/5] [Batch 145/360] [D loss: 0.002293] [G loss: 8.389696] time: 0:02:37.251919\n",
      "(30, 64, 64, 3)\n",
      "0.8281181\n",
      "[Epoch 3/5] [Batch 146/360] [D loss: 0.003333] [G loss: 8.207697] time: 0:02:37.371163\n",
      "(30, 64, 64, 3)\n",
      "0.855241\n",
      "[Epoch 3/5] [Batch 147/360] [D loss: 0.002034] [G loss: 8.312780] time: 0:02:37.484805\n",
      "(30, 64, 64, 3)\n",
      "0.89010054\n",
      "[Epoch 3/5] [Batch 148/360] [D loss: 0.001895] [G loss: 8.460854] time: 0:02:37.622294\n",
      "(30, 64, 64, 3)\n",
      "0.87212753\n",
      "[Epoch 3/5] [Batch 149/360] [D loss: 0.002623] [G loss: 8.266912] time: 0:02:37.732126\n",
      "(30, 64, 64, 3)\n",
      "0.8547721\n",
      "[Epoch 3/5] [Batch 150/360] [D loss: 0.003824] [G loss: 8.104144] time: 0:02:37.850098\n",
      "(30, 64, 64, 3)\n",
      "0.86750513\n",
      "[Epoch 3/5] [Batch 151/360] [D loss: 0.002684] [G loss: 8.038722] time: 0:02:37.964800\n",
      "(30, 64, 64, 3)\n",
      "0.87080437\n",
      "[Epoch 3/5] [Batch 152/360] [D loss: 0.002025] [G loss: 8.103327] time: 0:02:38.079243\n",
      "(30, 64, 64, 3)\n",
      "0.8764041\n",
      "[Epoch 3/5] [Batch 153/360] [D loss: 0.002637] [G loss: 8.481089] time: 0:02:38.195154\n",
      "(30, 64, 64, 3)\n",
      "0.8314783\n",
      "[Epoch 3/5] [Batch 154/360] [D loss: 0.002423] [G loss: 8.172490] time: 0:02:38.315575\n",
      "(30, 64, 64, 3)\n",
      "0.863918\n",
      "[Epoch 3/5] [Batch 155/360] [D loss: 0.002360] [G loss: 7.836712] time: 0:02:38.429134\n",
      "(30, 64, 64, 3)\n",
      "0.8459777\n",
      "[Epoch 3/5] [Batch 156/360] [D loss: 0.002790] [G loss: 8.273293] time: 0:02:38.547767\n",
      "(30, 64, 64, 3)\n",
      "0.9037106\n",
      "[Epoch 3/5] [Batch 157/360] [D loss: 0.002397] [G loss: 8.186865] time: 0:02:38.662165\n",
      "(30, 64, 64, 3)\n",
      "0.82837933\n",
      "[Epoch 3/5] [Batch 158/360] [D loss: 0.003852] [G loss: 7.717009] time: 0:02:38.775435\n",
      "(30, 64, 64, 3)\n",
      "0.90946126\n",
      "[Epoch 3/5] [Batch 159/360] [D loss: 0.002069] [G loss: 8.337521] time: 0:02:38.894432\n",
      "(30, 64, 64, 3)\n",
      "0.8327524\n",
      "[Epoch 3/5] [Batch 160/360] [D loss: 0.001930] [G loss: 8.155632] time: 0:02:39.009003\n",
      "(30, 64, 64, 3)\n",
      "0.8550856\n",
      "[Epoch 3/5] [Batch 161/360] [D loss: 0.002793] [G loss: 8.208607] time: 0:02:39.125312\n",
      "(30, 64, 64, 3)\n",
      "0.85038894\n",
      "[Epoch 3/5] [Batch 162/360] [D loss: 0.002356] [G loss: 8.624203] time: 0:02:39.240697\n",
      "(30, 64, 64, 3)\n",
      "0.8599389\n",
      "[Epoch 3/5] [Batch 163/360] [D loss: 0.002868] [G loss: 8.735776] time: 0:02:39.356286\n",
      "(30, 64, 64, 3)\n",
      "0.86826617\n",
      "[Epoch 3/5] [Batch 164/360] [D loss: 0.002449] [G loss: 8.083203] time: 0:02:39.475872\n",
      "(30, 64, 64, 3)\n",
      "0.85389227\n",
      "[Epoch 3/5] [Batch 165/360] [D loss: 0.002005] [G loss: 8.842428] time: 0:02:39.590084\n",
      "(30, 64, 64, 3)\n",
      "0.89200383\n",
      "[Epoch 3/5] [Batch 166/360] [D loss: 0.002284] [G loss: 8.535547] time: 0:02:39.704840\n",
      "(30, 64, 64, 3)\n",
      "0.8100714\n",
      "[Epoch 3/5] [Batch 167/360] [D loss: 0.002641] [G loss: 8.037158] time: 0:02:39.819254\n",
      "(30, 64, 64, 3)\n",
      "0.9111684\n",
      "[Epoch 3/5] [Batch 168/360] [D loss: 0.001871] [G loss: 8.225337] time: 0:02:39.943921\n",
      "(30, 64, 64, 3)\n",
      "0.8647936\n",
      "[Epoch 3/5] [Batch 169/360] [D loss: 0.004062] [G loss: 7.521882] time: 0:02:40.057184\n",
      "(30, 64, 64, 3)\n",
      "0.8934968\n",
      "[Epoch 3/5] [Batch 170/360] [D loss: 0.002081] [G loss: 8.083688] time: 0:02:40.178961\n",
      "(30, 64, 64, 3)\n",
      "0.8752086\n",
      "[Epoch 3/5] [Batch 171/360] [D loss: 0.002891] [G loss: 8.181764] time: 0:02:40.289387\n",
      "(30, 64, 64, 3)\n",
      "0.82872534\n",
      "[Epoch 3/5] [Batch 172/360] [D loss: 0.002253] [G loss: 8.080878] time: 0:02:40.416517\n",
      "(30, 64, 64, 3)\n",
      "0.88543314\n",
      "[Epoch 3/5] [Batch 173/360] [D loss: 0.002172] [G loss: 7.847740] time: 0:02:40.529138\n",
      "(30, 64, 64, 3)\n",
      "0.8566235\n",
      "[Epoch 3/5] [Batch 174/360] [D loss: 0.005887] [G loss: 7.908439] time: 0:02:40.641889\n",
      "(30, 64, 64, 3)\n",
      "0.84051794\n",
      "[Epoch 3/5] [Batch 175/360] [D loss: 0.002035] [G loss: 8.617092] time: 0:02:40.754567\n",
      "(30, 64, 64, 3)\n",
      "0.8844412\n",
      "[Epoch 3/5] [Batch 176/360] [D loss: 0.003269] [G loss: 8.441050] time: 0:02:40.872073\n",
      "(30, 64, 64, 3)\n",
      "0.85187435\n",
      "[Epoch 3/5] [Batch 177/360] [D loss: 0.002448] [G loss: 8.271762] time: 0:02:40.986203\n",
      "(30, 64, 64, 3)\n",
      "0.822113\n",
      "[Epoch 3/5] [Batch 178/360] [D loss: 0.002478] [G loss: 7.939461] time: 0:02:41.100654\n",
      "(30, 64, 64, 3)\n",
      "0.85512155\n",
      "[Epoch 3/5] [Batch 179/360] [D loss: 0.002547] [G loss: 8.051368] time: 0:02:41.212513\n",
      "(30, 64, 64, 3)\n",
      "0.85308\n",
      "[Epoch 3/5] [Batch 180/360] [D loss: 0.003117] [G loss: 8.092301] time: 0:02:41.336640\n",
      "(30, 64, 64, 3)\n",
      "0.88315296\n",
      "[Epoch 3/5] [Batch 181/360] [D loss: 0.003407] [G loss: 7.802632] time: 0:02:41.452489\n",
      "(30, 64, 64, 3)\n",
      "0.8989151\n",
      "[Epoch 3/5] [Batch 182/360] [D loss: 0.002473] [G loss: 8.461137] time: 0:02:41.572736\n",
      "(30, 64, 64, 3)\n",
      "0.8621977\n",
      "[Epoch 3/5] [Batch 183/360] [D loss: 0.003249] [G loss: 8.480678] time: 0:02:41.682533\n",
      "(30, 64, 64, 3)\n",
      "0.879209\n",
      "[Epoch 3/5] [Batch 184/360] [D loss: 0.002455] [G loss: 7.718492] time: 0:02:41.798620\n",
      "(30, 64, 64, 3)\n",
      "0.8910211\n",
      "[Epoch 3/5] [Batch 185/360] [D loss: 0.003199] [G loss: 8.439812] time: 0:02:41.916178\n",
      "(30, 64, 64, 3)\n",
      "0.8162999\n",
      "[Epoch 3/5] [Batch 186/360] [D loss: 0.002395] [G loss: 7.892189] time: 0:02:42.033551\n",
      "(30, 64, 64, 3)\n",
      "0.8514032\n",
      "[Epoch 3/5] [Batch 187/360] [D loss: 0.003064] [G loss: 7.868211] time: 0:02:42.148435\n",
      "(30, 64, 64, 3)\n",
      "0.89762324\n",
      "[Epoch 3/5] [Batch 188/360] [D loss: 0.003096] [G loss: 7.983663] time: 0:02:42.267322\n",
      "(30, 64, 64, 3)\n",
      "0.8904424\n",
      "[Epoch 3/5] [Batch 189/360] [D loss: 0.002794] [G loss: 7.886003] time: 0:02:42.381362\n",
      "(30, 64, 64, 3)\n",
      "0.8123234\n",
      "[Epoch 3/5] [Batch 190/360] [D loss: 0.002605] [G loss: 7.849268] time: 0:02:42.516215\n",
      "(30, 64, 64, 3)\n",
      "0.8552893\n",
      "[Epoch 3/5] [Batch 191/360] [D loss: 0.002612] [G loss: 7.597035] time: 0:02:42.631338\n",
      "(30, 64, 64, 3)\n",
      "0.86947864\n",
      "[Epoch 3/5] [Batch 192/360] [D loss: 0.002792] [G loss: 8.196136] time: 0:02:42.747719\n",
      "(30, 64, 64, 3)\n",
      "0.8658784\n",
      "[Epoch 3/5] [Batch 193/360] [D loss: 0.003849] [G loss: 7.701122] time: 0:02:42.859940\n",
      "(30, 64, 64, 3)\n",
      "0.8672635\n",
      "[Epoch 3/5] [Batch 194/360] [D loss: 0.002304] [G loss: 7.838802] time: 0:02:42.976079\n",
      "(30, 64, 64, 3)\n",
      "0.85448664\n",
      "[Epoch 3/5] [Batch 195/360] [D loss: 0.002707] [G loss: 7.489380] time: 0:02:43.091356\n",
      "(30, 64, 64, 3)\n",
      "0.8334179\n",
      "[Epoch 3/5] [Batch 196/360] [D loss: 0.002250] [G loss: 7.838442] time: 0:02:43.215389\n",
      "(30, 64, 64, 3)\n",
      "0.89147264\n",
      "[Epoch 3/5] [Batch 197/360] [D loss: 0.001922] [G loss: 8.013010] time: 0:02:43.330392\n",
      "(30, 64, 64, 3)\n",
      "0.86023927\n",
      "[Epoch 3/5] [Batch 198/360] [D loss: 0.002634] [G loss: 8.389808] time: 0:02:43.456117\n",
      "(30, 64, 64, 3)\n",
      "0.8892181\n",
      "[Epoch 3/5] [Batch 199/360] [D loss: 0.002584] [G loss: 7.477674] time: 0:02:43.573932\n",
      "(30, 64, 64, 3)\n",
      "0.8861858\n",
      "[Epoch 3/5] [Batch 200/360] [D loss: 0.003181] [G loss: 8.131341] time: 0:02:43.690868\n",
      "(30, 64, 64, 3)\n",
      "0.873082\n",
      "[Epoch 3/5] [Batch 201/360] [D loss: 0.001889] [G loss: 7.608652] time: 0:02:43.800868\n",
      "(30, 64, 64, 3)\n",
      "0.89210004\n",
      "[Epoch 3/5] [Batch 202/360] [D loss: 0.002760] [G loss: 8.179278] time: 0:02:43.918448\n",
      "(30, 64, 64, 3)\n",
      "0.8473243\n",
      "[Epoch 3/5] [Batch 203/360] [D loss: 0.002501] [G loss: 8.174123] time: 0:02:44.031696\n",
      "(30, 64, 64, 3)\n",
      "0.84620667\n",
      "[Epoch 3/5] [Batch 204/360] [D loss: 0.002786] [G loss: 7.784943] time: 0:02:44.147523\n",
      "(30, 64, 64, 3)\n",
      "0.862679\n",
      "[Epoch 3/5] [Batch 205/360] [D loss: 0.003009] [G loss: 7.923720] time: 0:02:44.258996\n",
      "(30, 64, 64, 3)\n",
      "0.8683149\n",
      "[Epoch 3/5] [Batch 206/360] [D loss: 0.002919] [G loss: 7.595128] time: 0:02:44.374265\n",
      "(30, 64, 64, 3)\n",
      "0.8354783\n",
      "[Epoch 3/5] [Batch 207/360] [D loss: 0.002887] [G loss: 7.945902] time: 0:02:44.488144\n",
      "(30, 64, 64, 3)\n",
      "0.8908141\n",
      "[Epoch 3/5] [Batch 208/360] [D loss: 0.003082] [G loss: 7.849305] time: 0:02:44.606020\n",
      "(30, 64, 64, 3)\n",
      "0.8648112\n",
      "[Epoch 3/5] [Batch 209/360] [D loss: 0.001924] [G loss: 7.630374] time: 0:02:44.719225\n",
      "(30, 64, 64, 3)\n",
      "0.800276\n",
      "[Epoch 3/5] [Batch 210/360] [D loss: 0.003026] [G loss: 8.291463] time: 0:02:44.834959\n",
      "(30, 64, 64, 3)\n",
      "0.8831008\n",
      "[Epoch 3/5] [Batch 211/360] [D loss: 0.003025] [G loss: 7.883601] time: 0:02:44.950928\n",
      "(30, 64, 64, 3)\n",
      "0.88398474\n",
      "[Epoch 3/5] [Batch 212/360] [D loss: 0.003140] [G loss: 7.661338] time: 0:02:45.067238\n",
      "(30, 64, 64, 3)\n",
      "0.88097763\n",
      "[Epoch 3/5] [Batch 213/360] [D loss: 0.003158] [G loss: 7.435933] time: 0:02:45.184712\n",
      "(30, 64, 64, 3)\n",
      "0.891963\n",
      "[Epoch 3/5] [Batch 214/360] [D loss: 0.002952] [G loss: 7.566432] time: 0:02:45.299849\n",
      "(30, 64, 64, 3)\n",
      "0.8608384\n",
      "[Epoch 3/5] [Batch 215/360] [D loss: 0.002836] [G loss: 7.260299] time: 0:02:45.415325\n",
      "(30, 64, 64, 3)\n",
      "0.89059883\n",
      "[Epoch 3/5] [Batch 216/360] [D loss: 0.003404] [G loss: 8.080939] time: 0:02:45.530752\n",
      "(30, 64, 64, 3)\n",
      "0.8969374\n",
      "[Epoch 3/5] [Batch 217/360] [D loss: 0.002373] [G loss: 7.262891] time: 0:02:45.650196\n",
      "(30, 64, 64, 3)\n",
      "0.86623216\n",
      "[Epoch 3/5] [Batch 218/360] [D loss: 0.006137] [G loss: 7.595888] time: 0:02:45.763253\n",
      "(30, 64, 64, 3)\n",
      "0.8621211\n",
      "[Epoch 3/5] [Batch 219/360] [D loss: 0.002574] [G loss: 7.661037] time: 0:02:45.875859\n",
      "(30, 64, 64, 3)\n",
      "0.8414802\n",
      "[Epoch 3/5] [Batch 220/360] [D loss: 0.006792] [G loss: 7.349625] time: 0:02:46.006935\n",
      "(30, 64, 64, 3)\n",
      "0.8886922\n",
      "[Epoch 3/5] [Batch 221/360] [D loss: 0.004754] [G loss: 7.666425] time: 0:02:46.118961\n",
      "(30, 64, 64, 3)\n",
      "0.8498519\n",
      "[Epoch 3/5] [Batch 222/360] [D loss: 0.024975] [G loss: 8.311715] time: 0:02:46.236729\n",
      "(30, 64, 64, 3)\n",
      "0.86794394\n",
      "[Epoch 3/5] [Batch 223/360] [D loss: 0.003547] [G loss: 7.831859] time: 0:02:46.351320\n",
      "(30, 64, 64, 3)\n",
      "0.8666186\n",
      "[Epoch 3/5] [Batch 224/360] [D loss: 0.020332] [G loss: 7.886775] time: 0:02:46.465244\n",
      "(30, 64, 64, 3)\n",
      "0.88047695\n",
      "[Epoch 3/5] [Batch 225/360] [D loss: 0.006306] [G loss: 7.471627] time: 0:02:46.579148\n",
      "(30, 64, 64, 3)\n",
      "0.8681757\n",
      "[Epoch 3/5] [Batch 226/360] [D loss: 0.089024] [G loss: 8.211399] time: 0:02:46.694511\n",
      "(30, 64, 64, 3)\n",
      "0.8397711\n",
      "[Epoch 3/5] [Batch 227/360] [D loss: 0.966946] [G loss: 6.937598] time: 0:02:46.809800\n",
      "(30, 64, 64, 3)\n",
      "0.8857916\n",
      "[Epoch 3/5] [Batch 228/360] [D loss: 0.086172] [G loss: 7.741454] time: 0:02:46.927438\n",
      "(30, 64, 64, 3)\n",
      "0.87222844\n",
      "[Epoch 3/5] [Batch 229/360] [D loss: 0.198750] [G loss: 7.215263] time: 0:02:47.039005\n",
      "(30, 64, 64, 3)\n",
      "0.85506886\n",
      "[Epoch 3/5] [Batch 230/360] [D loss: 0.140981] [G loss: 7.307857] time: 0:02:47.156793\n",
      "(30, 64, 64, 3)\n",
      "0.8826074\n",
      "[Epoch 3/5] [Batch 231/360] [D loss: 0.062776] [G loss: 7.412201] time: 0:02:47.271613\n",
      "(30, 64, 64, 3)\n",
      "0.79617333\n",
      "[Epoch 3/5] [Batch 232/360] [D loss: 0.029394] [G loss: 7.828678] time: 0:02:47.387458\n",
      "(30, 64, 64, 3)\n",
      "0.8217602\n",
      "[Epoch 3/5] [Batch 233/360] [D loss: 0.010810] [G loss: 7.176683] time: 0:02:47.501449\n",
      "(30, 64, 64, 3)\n",
      "0.8769779\n",
      "[Epoch 3/5] [Batch 234/360] [D loss: 0.008160] [G loss: 7.827195] time: 0:02:47.622507\n",
      "(30, 64, 64, 3)\n",
      "0.8964002\n",
      "[Epoch 3/5] [Batch 235/360] [D loss: 0.005142] [G loss: 7.877662] time: 0:02:47.740921\n",
      "(30, 64, 64, 3)\n",
      "0.87022996\n",
      "[Epoch 3/5] [Batch 236/360] [D loss: 0.006757] [G loss: 6.891593] time: 0:02:47.964124\n",
      "(30, 64, 64, 3)\n",
      "0.8713467\n",
      "[Epoch 3/5] [Batch 237/360] [D loss: 0.005281] [G loss: 7.539391] time: 0:02:48.078716\n",
      "(30, 64, 64, 3)\n",
      "0.8489819\n",
      "[Epoch 3/5] [Batch 238/360] [D loss: 0.004395] [G loss: 8.018459] time: 0:02:48.194593\n",
      "(30, 64, 64, 3)\n",
      "0.8554964\n",
      "[Epoch 3/5] [Batch 239/360] [D loss: 0.004066] [G loss: 7.349884] time: 0:02:48.308671\n",
      "(30, 64, 64, 3)\n",
      "0.9037793\n",
      "[Epoch 3/5] [Batch 240/360] [D loss: 0.003951] [G loss: 7.064247] time: 0:02:48.427885\n",
      "(30, 64, 64, 3)\n",
      "0.86720306\n",
      "[Epoch 3/5] [Batch 241/360] [D loss: 0.005364] [G loss: 7.242690] time: 0:02:48.544960\n",
      "(30, 64, 64, 3)\n",
      "0.8791887\n",
      "[Epoch 3/5] [Batch 242/360] [D loss: 0.003668] [G loss: 7.248281] time: 0:02:48.662254\n",
      "(30, 64, 64, 3)\n",
      "0.8868237\n",
      "[Epoch 3/5] [Batch 243/360] [D loss: 0.004009] [G loss: 7.664412] time: 0:02:48.777898\n",
      "(30, 64, 64, 3)\n",
      "0.91954917\n",
      "[Epoch 3/5] [Batch 244/360] [D loss: 0.004541] [G loss: 7.430067] time: 0:02:48.895375\n",
      "(30, 64, 64, 3)\n",
      "0.87638944\n",
      "[Epoch 3/5] [Batch 245/360] [D loss: 0.003192] [G loss: 7.297874] time: 0:02:49.014571\n",
      "(30, 64, 64, 3)\n",
      "0.82480365\n",
      "[Epoch 3/5] [Batch 246/360] [D loss: 0.005674] [G loss: 7.948792] time: 0:02:49.129827\n",
      "(30, 64, 64, 3)\n",
      "0.8867049\n",
      "[Epoch 3/5] [Batch 247/360] [D loss: 0.003512] [G loss: 7.416120] time: 0:02:49.243535\n",
      "(30, 64, 64, 3)\n",
      "0.88510436\n",
      "[Epoch 3/5] [Batch 248/360] [D loss: 0.004614] [G loss: 7.417011] time: 0:02:49.355837\n",
      "(30, 64, 64, 3)\n",
      "0.8712675\n",
      "[Epoch 3/5] [Batch 249/360] [D loss: 0.004396] [G loss: 7.642924] time: 0:02:49.472886\n",
      "(30, 64, 64, 3)\n",
      "0.8601368\n",
      "[Epoch 3/5] [Batch 250/360] [D loss: 0.002645] [G loss: 7.772183] time: 0:02:49.589469\n",
      "(30, 64, 64, 3)\n",
      "0.8354648\n",
      "[Epoch 3/5] [Batch 251/360] [D loss: 0.005181] [G loss: 7.196385] time: 0:02:49.704137\n",
      "(30, 64, 64, 3)\n",
      "0.88820285\n",
      "[Epoch 3/5] [Batch 252/360] [D loss: 0.003632] [G loss: 7.374903] time: 0:02:49.822616\n",
      "(30, 64, 64, 3)\n",
      "0.8912873\n",
      "[Epoch 3/5] [Batch 253/360] [D loss: 0.005663] [G loss: 7.141043] time: 0:02:49.938188\n",
      "(30, 64, 64, 3)\n",
      "0.8708644\n",
      "[Epoch 3/5] [Batch 254/360] [D loss: 0.003785] [G loss: 7.292217] time: 0:02:50.057208\n",
      "(30, 64, 64, 3)\n",
      "0.8891528\n",
      "[Epoch 3/5] [Batch 255/360] [D loss: 0.003972] [G loss: 7.366860] time: 0:02:50.174122\n",
      "(30, 64, 64, 3)\n",
      "0.8827408\n",
      "[Epoch 3/5] [Batch 256/360] [D loss: 0.002878] [G loss: 7.363336] time: 0:02:50.289687\n",
      "(30, 64, 64, 3)\n",
      "0.87848425\n",
      "[Epoch 3/5] [Batch 257/360] [D loss: 0.003171] [G loss: 7.542339] time: 0:02:50.406586\n",
      "(30, 64, 64, 3)\n",
      "0.87603265\n",
      "[Epoch 3/5] [Batch 258/360] [D loss: 0.003353] [G loss: 7.388772] time: 0:02:50.525000\n",
      "(30, 64, 64, 3)\n",
      "0.8486044\n",
      "[Epoch 3/5] [Batch 259/360] [D loss: 0.003101] [G loss: 7.120951] time: 0:02:50.638658\n",
      "(30, 64, 64, 3)\n",
      "0.85807306\n",
      "[Epoch 3/5] [Batch 260/360] [D loss: 0.002897] [G loss: 7.618762] time: 0:02:50.756466\n",
      "(30, 64, 64, 3)\n",
      "0.87526673\n",
      "[Epoch 3/5] [Batch 261/360] [D loss: 0.003181] [G loss: 7.926464] time: 0:02:50.871315\n",
      "(30, 64, 64, 3)\n",
      "0.8520884\n",
      "[Epoch 3/5] [Batch 262/360] [D loss: 0.002458] [G loss: 7.157948] time: 0:02:50.990182\n",
      "(30, 64, 64, 3)\n",
      "0.91003865\n",
      "[Epoch 3/5] [Batch 263/360] [D loss: 0.002560] [G loss: 7.331551] time: 0:02:51.105838\n",
      "(30, 64, 64, 3)\n",
      "0.8863723\n",
      "[Epoch 3/5] [Batch 264/360] [D loss: 0.005970] [G loss: 7.088057] time: 0:02:51.231427\n",
      "(30, 64, 64, 3)\n",
      "0.86580753\n",
      "[Epoch 3/5] [Batch 265/360] [D loss: 0.002126] [G loss: 7.213309] time: 0:02:51.344822\n",
      "(30, 64, 64, 3)\n",
      "0.88273865\n",
      "[Epoch 3/5] [Batch 266/360] [D loss: 0.003373] [G loss: 7.182622] time: 0:02:51.459497\n",
      "(30, 64, 64, 3)\n",
      "0.87244576\n",
      "[Epoch 3/5] [Batch 267/360] [D loss: 0.002319] [G loss: 7.256009] time: 0:02:51.575505\n",
      "(30, 64, 64, 3)\n",
      "0.8328989\n",
      "[Epoch 3/5] [Batch 268/360] [D loss: 0.003845] [G loss: 7.279599] time: 0:02:51.692690\n",
      "(30, 64, 64, 3)\n",
      "0.841506\n",
      "[Epoch 3/5] [Batch 269/360] [D loss: 0.003646] [G loss: 7.451867] time: 0:02:51.805946\n",
      "(30, 64, 64, 3)\n",
      "0.82781696\n",
      "[Epoch 3/5] [Batch 270/360] [D loss: 0.004967] [G loss: 7.662880] time: 0:02:51.922584\n",
      "(30, 64, 64, 3)\n",
      "0.88302594\n",
      "[Epoch 3/5] [Batch 271/360] [D loss: 0.003811] [G loss: 7.395918] time: 0:02:52.039281\n",
      "(30, 64, 64, 3)\n",
      "0.8882267\n",
      "[Epoch 3/5] [Batch 272/360] [D loss: 0.005024] [G loss: 7.450099] time: 0:02:52.158662\n",
      "(30, 64, 64, 3)\n",
      "0.8796174\n",
      "[Epoch 3/5] [Batch 273/360] [D loss: 0.003720] [G loss: 7.237147] time: 0:02:52.273053\n",
      "(30, 64, 64, 3)\n",
      "0.91122\n",
      "[Epoch 3/5] [Batch 274/360] [D loss: 0.004078] [G loss: 7.198245] time: 0:02:52.395460\n",
      "(30, 64, 64, 3)\n",
      "0.8833988\n",
      "[Epoch 3/5] [Batch 275/360] [D loss: 0.002434] [G loss: 6.982782] time: 0:02:52.514418\n",
      "(30, 64, 64, 3)\n",
      "0.8987778\n",
      "[Epoch 3/5] [Batch 276/360] [D loss: 0.004322] [G loss: 7.081975] time: 0:02:52.633406\n",
      "(30, 64, 64, 3)\n",
      "0.9034121\n",
      "[Epoch 3/5] [Batch 277/360] [D loss: 0.003739] [G loss: 7.864011] time: 0:02:52.746007\n",
      "(30, 64, 64, 3)\n",
      "0.9020953\n",
      "[Epoch 3/5] [Batch 278/360] [D loss: 0.005109] [G loss: 7.370529] time: 0:02:52.862202\n",
      "(30, 64, 64, 3)\n",
      "0.8554047\n",
      "[Epoch 3/5] [Batch 279/360] [D loss: 0.003093] [G loss: 7.032344] time: 0:02:52.980825\n",
      "(30, 64, 64, 3)\n",
      "0.87523705\n",
      "[Epoch 3/5] [Batch 280/360] [D loss: 0.006260] [G loss: 7.130299] time: 0:02:53.098318\n",
      "(30, 64, 64, 3)\n",
      "0.8270321\n",
      "[Epoch 3/5] [Batch 281/360] [D loss: 0.004872] [G loss: 7.142674] time: 0:02:53.213797\n",
      "(30, 64, 64, 3)\n",
      "0.8496153\n",
      "[Epoch 3/5] [Batch 282/360] [D loss: 0.003585] [G loss: 7.578104] time: 0:02:53.327305\n",
      "(30, 64, 64, 3)\n",
      "0.8635302\n",
      "[Epoch 3/5] [Batch 283/360] [D loss: 0.005067] [G loss: 6.737785] time: 0:02:53.442981\n",
      "(30, 64, 64, 3)\n",
      "0.9193547\n",
      "[Epoch 3/5] [Batch 284/360] [D loss: 0.002535] [G loss: 7.066349] time: 0:02:53.560786\n",
      "(30, 64, 64, 3)\n",
      "0.8760371\n",
      "[Epoch 3/5] [Batch 285/360] [D loss: 0.002810] [G loss: 7.675579] time: 0:02:53.677109\n",
      "(30, 64, 64, 3)\n",
      "0.9126868\n",
      "[Epoch 3/5] [Batch 286/360] [D loss: 0.003364] [G loss: 7.129538] time: 0:02:53.791714\n",
      "(30, 64, 64, 3)\n",
      "0.8842635\n",
      "[Epoch 3/5] [Batch 287/360] [D loss: 0.003729] [G loss: 6.666924] time: 0:02:53.906770\n",
      "(30, 64, 64, 3)\n",
      "0.8655165\n",
      "[Epoch 3/5] [Batch 288/360] [D loss: 0.003275] [G loss: 7.551912] time: 0:02:54.024794\n",
      "(30, 64, 64, 3)\n",
      "0.80572027\n",
      "[Epoch 3/5] [Batch 289/360] [D loss: 0.003147] [G loss: 7.777574] time: 0:02:54.139268\n",
      "(30, 64, 64, 3)\n",
      "0.83621407\n",
      "[Epoch 3/5] [Batch 290/360] [D loss: 0.002957] [G loss: 6.962732] time: 0:02:54.257058\n",
      "(30, 64, 64, 3)\n",
      "0.851392\n",
      "[Epoch 3/5] [Batch 291/360] [D loss: 0.002294] [G loss: 6.957286] time: 0:02:54.373559\n",
      "(30, 64, 64, 3)\n",
      "0.90939\n",
      "[Epoch 3/5] [Batch 292/360] [D loss: 0.002511] [G loss: 7.175112] time: 0:02:54.490922\n",
      "(30, 64, 64, 3)\n",
      "0.8625638\n",
      "[Epoch 3/5] [Batch 293/360] [D loss: 0.005197] [G loss: 7.030388] time: 0:02:54.602054\n",
      "(30, 64, 64, 3)\n",
      "0.9218941\n",
      "[Epoch 3/5] [Batch 294/360] [D loss: 0.002786] [G loss: 7.172831] time: 0:02:54.720999\n",
      "(30, 64, 64, 3)\n",
      "0.8253505\n",
      "[Epoch 3/5] [Batch 295/360] [D loss: 0.005145] [G loss: 6.985191] time: 0:02:54.832130\n",
      "(30, 64, 64, 3)\n",
      "0.89017224\n",
      "[Epoch 3/5] [Batch 296/360] [D loss: 0.002703] [G loss: 6.514422] time: 0:02:54.955996\n",
      "(30, 64, 64, 3)\n",
      "0.90463084\n",
      "[Epoch 3/5] [Batch 297/360] [D loss: 0.006760] [G loss: 6.678197] time: 0:02:55.068238\n",
      "(30, 64, 64, 3)\n",
      "0.8332739\n",
      "[Epoch 3/5] [Batch 298/360] [D loss: 0.003609] [G loss: 7.546834] time: 0:02:55.189623\n",
      "(30, 64, 64, 3)\n",
      "0.8925331\n",
      "[Epoch 3/5] [Batch 299/360] [D loss: 0.004015] [G loss: 7.442204] time: 0:02:55.308693\n",
      "(30, 64, 64, 3)\n",
      "0.86098784\n",
      "[Epoch 3/5] [Batch 300/360] [D loss: 0.002749] [G loss: 7.054540] time: 0:02:55.426000\n",
      "(30, 64, 64, 3)\n",
      "0.87128395\n",
      "[Epoch 3/5] [Batch 301/360] [D loss: 0.006602] [G loss: 6.719950] time: 0:02:55.542628\n",
      "(30, 64, 64, 3)\n",
      "0.8863044\n",
      "[Epoch 3/5] [Batch 302/360] [D loss: 0.003005] [G loss: 7.393870] time: 0:02:55.668454\n",
      "(30, 64, 64, 3)\n",
      "0.87107164\n",
      "[Epoch 3/5] [Batch 303/360] [D loss: 0.004304] [G loss: 6.992070] time: 0:02:55.784856\n",
      "(30, 64, 64, 3)\n",
      "0.9295271\n",
      "[Epoch 3/5] [Batch 304/360] [D loss: 0.003249] [G loss: 6.940505] time: 0:02:55.914357\n",
      "(30, 64, 64, 3)\n",
      "0.8513732\n",
      "[Epoch 3/5] [Batch 305/360] [D loss: 0.005430] [G loss: 7.198134] time: 0:02:56.030900\n",
      "(30, 64, 64, 3)\n",
      "0.8713491\n",
      "[Epoch 3/5] [Batch 306/360] [D loss: 0.004293] [G loss: 6.978273] time: 0:02:56.144799\n",
      "(30, 64, 64, 3)\n",
      "0.9001084\n",
      "[Epoch 3/5] [Batch 307/360] [D loss: 0.003212] [G loss: 7.230421] time: 0:02:56.257562\n",
      "(30, 64, 64, 3)\n",
      "0.8556511\n",
      "[Epoch 3/5] [Batch 308/360] [D loss: 0.003328] [G loss: 7.144554] time: 0:02:56.376589\n",
      "(30, 64, 64, 3)\n",
      "0.84855765\n",
      "[Epoch 3/5] [Batch 309/360] [D loss: 0.003093] [G loss: 6.794358] time: 0:02:56.492718\n",
      "(30, 64, 64, 3)\n",
      "0.85668427\n",
      "[Epoch 3/5] [Batch 310/360] [D loss: 0.004333] [G loss: 6.933608] time: 0:02:56.608843\n",
      "(30, 64, 64, 3)\n",
      "0.8886301\n",
      "[Epoch 3/5] [Batch 311/360] [D loss: 0.002968] [G loss: 7.261141] time: 0:02:56.723211\n",
      "(30, 64, 64, 3)\n",
      "0.8872092\n",
      "[Epoch 3/5] [Batch 312/360] [D loss: 0.002819] [G loss: 7.083316] time: 0:02:56.841574\n",
      "(30, 64, 64, 3)\n",
      "0.87526464\n",
      "[Epoch 3/5] [Batch 313/360] [D loss: 0.003227] [G loss: 7.160425] time: 0:02:56.959278\n",
      "(30, 64, 64, 3)\n",
      "0.8852134\n",
      "[Epoch 3/5] [Batch 314/360] [D loss: 0.006214] [G loss: 7.134549] time: 0:02:57.081784\n",
      "(30, 64, 64, 3)\n",
      "0.8834476\n",
      "[Epoch 3/5] [Batch 315/360] [D loss: 0.002679] [G loss: 7.117002] time: 0:02:57.196937\n",
      "(30, 64, 64, 3)\n",
      "0.85893184\n",
      "[Epoch 3/5] [Batch 316/360] [D loss: 0.005908] [G loss: 7.028704] time: 0:02:57.307793\n",
      "(30, 64, 64, 3)\n",
      "0.8598204\n",
      "[Epoch 3/5] [Batch 317/360] [D loss: 0.002955] [G loss: 7.119651] time: 0:02:57.426269\n",
      "(30, 64, 64, 3)\n",
      "0.88300705\n",
      "[Epoch 3/5] [Batch 318/360] [D loss: 0.010078] [G loss: 7.486684] time: 0:02:57.557048\n",
      "(30, 64, 64, 3)\n",
      "0.8660522\n",
      "[Epoch 3/5] [Batch 319/360] [D loss: 0.003193] [G loss: 6.620072] time: 0:02:57.670736\n",
      "(30, 64, 64, 3)\n",
      "0.86545277\n",
      "[Epoch 3/5] [Batch 320/360] [D loss: 0.003256] [G loss: 6.669576] time: 0:02:57.783985\n",
      "(30, 64, 64, 3)\n",
      "0.87603325\n",
      "[Epoch 3/5] [Batch 321/360] [D loss: 0.002946] [G loss: 6.547978] time: 0:02:57.901565\n",
      "(30, 64, 64, 3)\n",
      "0.8968294\n",
      "[Epoch 3/5] [Batch 322/360] [D loss: 0.005407] [G loss: 7.181606] time: 0:02:58.016325\n",
      "(30, 64, 64, 3)\n",
      "0.8804359\n",
      "[Epoch 3/5] [Batch 323/360] [D loss: 0.003603] [G loss: 6.494828] time: 0:02:58.128979\n",
      "(30, 64, 64, 3)\n",
      "0.90674967\n",
      "[Epoch 3/5] [Batch 324/360] [D loss: 0.007802] [G loss: 6.937190] time: 0:02:58.245405\n",
      "(30, 64, 64, 3)\n",
      "0.90233135\n",
      "[Epoch 3/5] [Batch 325/360] [D loss: 0.003614] [G loss: 7.005255] time: 0:02:58.358293\n",
      "(30, 64, 64, 3)\n",
      "0.89714926\n",
      "[Epoch 3/5] [Batch 326/360] [D loss: 0.011640] [G loss: 6.714967] time: 0:02:58.479119\n",
      "(30, 64, 64, 3)\n",
      "0.8466866\n",
      "[Epoch 3/5] [Batch 327/360] [D loss: 0.003567] [G loss: 7.069600] time: 0:02:58.592448\n",
      "(30, 64, 64, 3)\n",
      "0.87999755\n",
      "[Epoch 3/5] [Batch 328/360] [D loss: 0.014805] [G loss: 7.197040] time: 0:02:58.717288\n",
      "(30, 64, 64, 3)\n",
      "0.865716\n",
      "[Epoch 3/5] [Batch 329/360] [D loss: 0.004595] [G loss: 6.694538] time: 0:02:58.834725\n",
      "(30, 64, 64, 3)\n",
      "0.8611265\n",
      "[Epoch 3/5] [Batch 330/360] [D loss: 0.038230] [G loss: 6.868953] time: 0:02:58.955788\n",
      "(30, 64, 64, 3)\n",
      "0.83350974\n",
      "[Epoch 3/5] [Batch 331/360] [D loss: 0.320571] [G loss: 7.409474] time: 0:02:59.070439\n",
      "(30, 64, 64, 3)\n",
      "0.846128\n",
      "[Epoch 3/5] [Batch 332/360] [D loss: 0.666453] [G loss: 6.933452] time: 0:02:59.185040\n",
      "(30, 64, 64, 3)\n",
      "0.88721824\n",
      "[Epoch 3/5] [Batch 333/360] [D loss: 0.470035] [G loss: 6.099909] time: 0:02:59.301859\n",
      "(30, 64, 64, 3)\n",
      "0.8909154\n",
      "[Epoch 3/5] [Batch 334/360] [D loss: 0.305816] [G loss: 6.331779] time: 0:02:59.416623\n",
      "(30, 64, 64, 3)\n",
      "0.8309738\n",
      "[Epoch 3/5] [Batch 335/360] [D loss: 0.230901] [G loss: 6.230987] time: 0:02:59.535495\n",
      "(30, 64, 64, 3)\n",
      "0.8348729\n",
      "[Epoch 3/5] [Batch 336/360] [D loss: 0.137939] [G loss: 6.166253] time: 0:02:59.652941\n",
      "(30, 64, 64, 3)\n",
      "0.88844603\n",
      "[Epoch 3/5] [Batch 337/360] [D loss: 0.037822] [G loss: 6.302364] time: 0:02:59.769913\n",
      "(30, 64, 64, 3)\n",
      "0.85070485\n",
      "[Epoch 3/5] [Batch 338/360] [D loss: 0.048850] [G loss: 6.433626] time: 0:02:59.886372\n",
      "(30, 64, 64, 3)\n",
      "0.87927943\n",
      "[Epoch 3/5] [Batch 339/360] [D loss: 0.027880] [G loss: 6.042718] time: 0:03:00.001545\n",
      "(30, 64, 64, 3)\n",
      "0.8740104\n",
      "[Epoch 3/5] [Batch 340/360] [D loss: 0.021748] [G loss: 6.558167] time: 0:03:00.116518\n",
      "(30, 64, 64, 3)\n",
      "0.83829063\n",
      "[Epoch 3/5] [Batch 341/360] [D loss: 0.020948] [G loss: 6.351874] time: 0:03:00.234503\n",
      "(30, 64, 64, 3)\n",
      "0.9049329\n",
      "[Epoch 3/5] [Batch 342/360] [D loss: 0.012414] [G loss: 7.188271] time: 0:03:00.350791\n",
      "(30, 64, 64, 3)\n",
      "0.8479832\n",
      "[Epoch 3/5] [Batch 343/360] [D loss: 0.017779] [G loss: 6.649075] time: 0:03:00.463088\n",
      "(30, 64, 64, 3)\n",
      "0.88232994\n",
      "[Epoch 3/5] [Batch 344/360] [D loss: 0.009380] [G loss: 6.861790] time: 0:03:00.583128\n",
      "(30, 64, 64, 3)\n",
      "0.8640127\n",
      "[Epoch 3/5] [Batch 345/360] [D loss: 0.010871] [G loss: 6.793515] time: 0:03:00.699266\n",
      "(30, 64, 64, 3)\n",
      "0.8876419\n",
      "[Epoch 3/5] [Batch 346/360] [D loss: 0.009840] [G loss: 6.921098] time: 0:03:00.815921\n",
      "(30, 64, 64, 3)\n",
      "0.9190306\n",
      "[Epoch 3/5] [Batch 347/360] [D loss: 0.008877] [G loss: 6.529858] time: 0:03:00.930854\n",
      "(30, 64, 64, 3)\n",
      "0.8669891\n",
      "[Epoch 3/5] [Batch 348/360] [D loss: 0.008398] [G loss: 7.133317] time: 0:03:01.048153\n",
      "(30, 64, 64, 3)\n",
      "0.86422014\n",
      "[Epoch 3/5] [Batch 349/360] [D loss: 0.006631] [G loss: 6.932810] time: 0:03:01.162823\n",
      "(30, 64, 64, 3)\n",
      "0.87649965\n",
      "[Epoch 3/5] [Batch 350/360] [D loss: 0.011722] [G loss: 6.557727] time: 0:03:01.274798\n",
      "(30, 64, 64, 3)\n",
      "0.85446614\n",
      "[Epoch 3/5] [Batch 351/360] [D loss: 0.004702] [G loss: 6.419091] time: 0:03:01.389478\n",
      "(30, 64, 64, 3)\n",
      "0.87551975\n",
      "[Epoch 3/5] [Batch 352/360] [D loss: 0.011701] [G loss: 6.221382] time: 0:03:01.509173\n",
      "(30, 64, 64, 3)\n",
      "0.8550152\n",
      "[Epoch 3/5] [Batch 353/360] [D loss: 0.004196] [G loss: 6.869808] time: 0:03:01.627702\n",
      "(30, 64, 64, 3)\n",
      "0.86306137\n",
      "[Epoch 3/5] [Batch 354/360] [D loss: 0.007509] [G loss: 6.817224] time: 0:03:01.755526\n",
      "(30, 64, 64, 3)\n",
      "0.8849527\n",
      "[Epoch 3/5] [Batch 355/360] [D loss: 0.003244] [G loss: 6.174667] time: 0:03:01.868861\n",
      "(30, 64, 64, 3)\n",
      "0.8994034\n",
      "[Epoch 3/5] [Batch 356/360] [D loss: 0.008026] [G loss: 6.568899] time: 0:03:01.984327\n",
      "(30, 64, 64, 3)\n",
      "0.88191557\n",
      "[Epoch 3/5] [Batch 357/360] [D loss: 0.004427] [G loss: 6.563941] time: 0:03:02.101591\n",
      "(30, 64, 64, 3)\n",
      "0.8840761\n",
      "[Epoch 3/5] [Batch 358/360] [D loss: 0.016784] [G loss: 6.444944] time: 0:03:02.216715\n",
      "(30, 64, 64, 3)\n",
      "0.88897485\n",
      "[Epoch 3/5] [Batch 359/360] [D loss: 0.004753] [G loss: 6.661507] time: 0:03:02.328546\n",
      "(30, 64, 64, 3)\n",
      "0.8853326\n",
      "[Epoch 4/5] [Batch 0/360] [D loss: 0.019404] [G loss: 6.656580] time: 0:03:02.445293\n",
      "(30, 64, 64, 3)\n",
      "0.8751507\n",
      "[Epoch 4/5] [Batch 1/360] [D loss: 0.004651] [G loss: 6.875021] time: 0:03:02.559498\n",
      "(30, 64, 64, 3)\n",
      "0.87913305\n",
      "[Epoch 4/5] [Batch 3/360] [D loss: 0.010616] [G loss: 6.579720] time: 0:03:02.693726\n",
      "(30, 64, 64, 3)\n",
      "0.8727786\n",
      "[Epoch 4/5] [Batch 4/360] [D loss: 0.004387] [G loss: 6.552824] time: 0:03:02.811489\n",
      "(30, 64, 64, 3)\n",
      "0.8808283\n",
      "[Epoch 4/5] [Batch 5/360] [D loss: 0.006266] [G loss: 6.516314] time: 0:03:02.938500\n",
      "(30, 64, 64, 3)\n",
      "0.90103096\n",
      "[Epoch 4/5] [Batch 6/360] [D loss: 0.003693] [G loss: 6.761334] time: 0:03:03.052212\n",
      "(30, 64, 64, 3)\n",
      "0.8928843\n",
      "[Epoch 4/5] [Batch 7/360] [D loss: 0.005825] [G loss: 6.546348] time: 0:03:03.170621\n",
      "(30, 64, 64, 3)\n",
      "0.8238396\n",
      "[Epoch 4/5] [Batch 8/360] [D loss: 0.003779] [G loss: 6.440665] time: 0:03:03.285178\n",
      "(30, 64, 64, 3)\n",
      "0.82278347\n",
      "[Epoch 4/5] [Batch 9/360] [D loss: 0.005138] [G loss: 6.656123] time: 0:03:03.399284\n",
      "(30, 64, 64, 3)\n",
      "0.8744955\n",
      "[Epoch 4/5] [Batch 10/360] [D loss: 0.003466] [G loss: 6.433661] time: 0:03:03.514402\n",
      "(30, 64, 64, 3)\n",
      "0.85992664\n",
      "[Epoch 4/5] [Batch 11/360] [D loss: 0.004720] [G loss: 6.130074] time: 0:03:03.635903\n",
      "(30, 64, 64, 3)\n",
      "0.8592674\n",
      "[Epoch 4/5] [Batch 12/360] [D loss: 0.003072] [G loss: 6.738730] time: 0:03:03.750329\n",
      "(30, 64, 64, 3)\n",
      "0.87328845\n",
      "[Epoch 4/5] [Batch 13/360] [D loss: 0.005835] [G loss: 6.215005] time: 0:03:03.868365\n",
      "(30, 64, 64, 3)\n",
      "0.89944386\n",
      "[Epoch 4/5] [Batch 14/360] [D loss: 0.004032] [G loss: 6.359627] time: 0:03:03.981345\n",
      "(30, 64, 64, 3)\n",
      "0.8986433\n",
      "[Epoch 4/5] [Batch 15/360] [D loss: 0.011718] [G loss: 6.364799] time: 0:03:04.108073\n",
      "(30, 64, 64, 3)\n",
      "0.8455823\n",
      "[Epoch 4/5] [Batch 16/360] [D loss: 0.003641] [G loss: 6.762623] time: 0:03:04.226129\n",
      "(30, 64, 64, 3)\n",
      "0.92495626\n",
      "[Epoch 4/5] [Batch 17/360] [D loss: 0.005886] [G loss: 6.429839] time: 0:03:04.353351\n",
      "(30, 64, 64, 3)\n",
      "0.89241314\n",
      "[Epoch 4/5] [Batch 18/360] [D loss: 0.004490] [G loss: 6.676981] time: 0:03:04.468887\n",
      "(30, 64, 64, 3)\n",
      "0.8808474\n",
      "[Epoch 4/5] [Batch 19/360] [D loss: 0.009894] [G loss: 6.298240] time: 0:03:04.586908\n",
      "(30, 64, 64, 3)\n",
      "0.8575649\n",
      "[Epoch 4/5] [Batch 20/360] [D loss: 0.003016] [G loss: 6.965595] time: 0:03:04.700678\n",
      "(30, 64, 64, 3)\n",
      "0.9055126\n",
      "[Epoch 4/5] [Batch 21/360] [D loss: 0.014662] [G loss: 6.675347] time: 0:03:04.833097\n",
      "(30, 64, 64, 3)\n",
      "0.8893016\n",
      "[Epoch 4/5] [Batch 22/360] [D loss: 0.003145] [G loss: 6.975144] time: 0:03:04.949297\n",
      "(30, 64, 64, 3)\n",
      "0.8746076\n",
      "[Epoch 4/5] [Batch 23/360] [D loss: 0.057732] [G loss: 6.818615] time: 0:03:05.065313\n",
      "(30, 64, 64, 3)\n",
      "0.89620906\n",
      "[Epoch 4/5] [Batch 24/360] [D loss: 1.006837] [G loss: 5.746028] time: 0:03:05.177905\n",
      "(30, 64, 64, 3)\n",
      "0.9357899\n",
      "[Epoch 4/5] [Batch 25/360] [D loss: 0.065950] [G loss: 6.233019] time: 0:03:05.294316\n",
      "(30, 64, 64, 3)\n",
      "0.8983\n",
      "[Epoch 4/5] [Batch 26/360] [D loss: 0.227894] [G loss: 6.791264] time: 0:03:05.405441\n",
      "(30, 64, 64, 3)\n",
      "0.88319397\n",
      "[Epoch 4/5] [Batch 27/360] [D loss: 0.172202] [G loss: 6.673491] time: 0:03:05.521754\n",
      "(30, 64, 64, 3)\n",
      "0.88854885\n",
      "[Epoch 4/5] [Batch 28/360] [D loss: 0.138574] [G loss: 6.229259] time: 0:03:05.633613\n",
      "(30, 64, 64, 3)\n",
      "0.8761981\n",
      "[Epoch 4/5] [Batch 29/360] [D loss: 0.113561] [G loss: 6.159707] time: 0:03:05.754702\n",
      "(30, 64, 64, 3)\n",
      "0.8175016\n",
      "[Epoch 4/5] [Batch 30/360] [D loss: 0.216220] [G loss: 6.462547] time: 0:03:05.872555\n",
      "(30, 64, 64, 3)\n",
      "0.87025946\n",
      "[Epoch 4/5] [Batch 31/360] [D loss: 0.219284] [G loss: 6.138725] time: 0:03:05.988811\n",
      "(30, 64, 64, 3)\n",
      "0.9049875\n",
      "[Epoch 4/5] [Batch 32/360] [D loss: 0.109058] [G loss: 6.399427] time: 0:03:06.109727\n",
      "(30, 64, 64, 3)\n",
      "0.8636803\n",
      "[Epoch 4/5] [Batch 33/360] [D loss: 0.108337] [G loss: 6.613752] time: 0:03:06.225268\n",
      "(30, 64, 64, 3)\n",
      "0.9049632\n",
      "[Epoch 4/5] [Batch 34/360] [D loss: 0.116089] [G loss: 6.459769] time: 0:03:06.342407\n",
      "(30, 64, 64, 3)\n",
      "0.88051844\n",
      "[Epoch 4/5] [Batch 35/360] [D loss: 0.058419] [G loss: 7.064021] time: 0:03:06.464464\n",
      "(30, 64, 64, 3)\n",
      "0.8791626\n",
      "[Epoch 4/5] [Batch 36/360] [D loss: 0.036491] [G loss: 6.475296] time: 0:03:06.578670\n",
      "(30, 64, 64, 3)\n",
      "0.89620274\n",
      "[Epoch 4/5] [Batch 37/360] [D loss: 0.037482] [G loss: 6.527167] time: 0:03:06.694061\n",
      "(30, 64, 64, 3)\n",
      "0.9025643\n",
      "[Epoch 4/5] [Batch 38/360] [D loss: 0.068747] [G loss: 7.136003] time: 0:03:06.812954\n",
      "(30, 64, 64, 3)\n",
      "0.8897035\n",
      "[Epoch 4/5] [Batch 39/360] [D loss: 0.190818] [G loss: 7.023802] time: 0:03:06.937637\n",
      "(30, 64, 64, 3)\n",
      "0.9081478\n",
      "[Epoch 4/5] [Batch 40/360] [D loss: 0.294730] [G loss: 6.709269] time: 0:03:07.050163\n",
      "(30, 64, 64, 3)\n",
      "0.8784726\n",
      "[Epoch 4/5] [Batch 41/360] [D loss: 0.219290] [G loss: 6.255474] time: 0:03:07.167108\n",
      "(30, 64, 64, 3)\n",
      "0.8407802\n",
      "[Epoch 4/5] [Batch 42/360] [D loss: 0.089189] [G loss: 6.128902] time: 0:03:07.283888\n",
      "(30, 64, 64, 3)\n",
      "0.8461326\n",
      "[Epoch 4/5] [Batch 43/360] [D loss: 0.033671] [G loss: 6.122099] time: 0:03:07.402638\n",
      "(30, 64, 64, 3)\n",
      "0.86998576\n",
      "[Epoch 4/5] [Batch 44/360] [D loss: 0.016087] [G loss: 6.803493] time: 0:03:07.523395\n",
      "(30, 64, 64, 3)\n",
      "0.84966516\n",
      "[Epoch 4/5] [Batch 45/360] [D loss: 0.028014] [G loss: 6.466828] time: 0:03:07.639308\n",
      "(30, 64, 64, 3)\n",
      "0.87214375\n",
      "[Epoch 4/5] [Batch 46/360] [D loss: 0.056717] [G loss: 6.369611] time: 0:03:07.755570\n",
      "(30, 64, 64, 3)\n",
      "0.87516236\n",
      "[Epoch 4/5] [Batch 47/360] [D loss: 0.119290] [G loss: 6.931531] time: 0:03:07.868768\n",
      "(30, 64, 64, 3)\n",
      "0.9085019\n",
      "[Epoch 4/5] [Batch 48/360] [D loss: 0.179472] [G loss: 6.198952] time: 0:03:07.989163\n",
      "(30, 64, 64, 3)\n",
      "0.8844867\n",
      "[Epoch 4/5] [Batch 49/360] [D loss: 0.178563] [G loss: 6.635468] time: 0:03:08.118459\n",
      "(30, 64, 64, 3)\n",
      "0.8635171\n",
      "[Epoch 4/5] [Batch 50/360] [D loss: 0.054232] [G loss: 6.677479] time: 0:03:08.233841\n",
      "(30, 64, 64, 3)\n",
      "0.90580344\n",
      "[Epoch 4/5] [Batch 51/360] [D loss: 0.010948] [G loss: 6.139828] time: 0:03:08.347396\n",
      "(30, 64, 64, 3)\n",
      "0.9232976\n",
      "[Epoch 4/5] [Batch 52/360] [D loss: 0.024101] [G loss: 5.974792] time: 0:03:08.468153\n",
      "(30, 64, 64, 3)\n",
      "0.89567095\n",
      "[Epoch 4/5] [Batch 53/360] [D loss: 0.011382] [G loss: 6.490439] time: 0:03:08.584851\n",
      "(30, 64, 64, 3)\n",
      "0.90038323\n",
      "[Epoch 4/5] [Batch 54/360] [D loss: 0.011388] [G loss: 6.464193] time: 0:03:08.701356\n",
      "(30, 64, 64, 3)\n",
      "0.8432588\n",
      "[Epoch 4/5] [Batch 55/360] [D loss: 0.012813] [G loss: 6.936094] time: 0:03:08.818440\n",
      "(30, 64, 64, 3)\n",
      "0.88508844\n",
      "[Epoch 4/5] [Batch 56/360] [D loss: 0.011808] [G loss: 6.571435] time: 0:03:08.933387\n",
      "(30, 64, 64, 3)\n",
      "0.88336474\n",
      "[Epoch 4/5] [Batch 57/360] [D loss: 0.014358] [G loss: 6.721580] time: 0:03:09.065396\n",
      "(30, 64, 64, 3)\n",
      "0.8814111\n",
      "[Epoch 4/5] [Batch 58/360] [D loss: 0.013061] [G loss: 6.167789] time: 0:03:09.180365\n",
      "(30, 64, 64, 3)\n",
      "0.89555985\n",
      "[Epoch 4/5] [Batch 59/360] [D loss: 0.012308] [G loss: 6.385791] time: 0:03:09.297474\n",
      "(30, 64, 64, 3)\n",
      "0.8559914\n",
      "[Epoch 4/5] [Batch 60/360] [D loss: 0.010191] [G loss: 6.825600] time: 0:03:09.409895\n",
      "(30, 64, 64, 3)\n",
      "0.8909268\n",
      "[Epoch 4/5] [Batch 61/360] [D loss: 0.018022] [G loss: 6.349007] time: 0:03:09.532298\n",
      "(30, 64, 64, 3)\n",
      "0.87594813\n",
      "[Epoch 4/5] [Batch 62/360] [D loss: 0.012697] [G loss: 7.046169] time: 0:03:09.649348\n",
      "(30, 64, 64, 3)\n",
      "0.864279\n",
      "[Epoch 4/5] [Batch 63/360] [D loss: 0.013060] [G loss: 6.280263] time: 0:03:09.769006\n",
      "(30, 64, 64, 3)\n",
      "0.86942893\n",
      "[Epoch 4/5] [Batch 64/360] [D loss: 0.019540] [G loss: 6.389383] time: 0:03:09.881869\n",
      "(30, 64, 64, 3)\n",
      "0.84432966\n",
      "[Epoch 4/5] [Batch 65/360] [D loss: 0.015647] [G loss: 6.120983] time: 0:03:09.997862\n",
      "(30, 64, 64, 3)\n",
      "0.88824385\n",
      "[Epoch 4/5] [Batch 66/360] [D loss: 0.013806] [G loss: 6.230086] time: 0:03:10.118814\n",
      "(30, 64, 64, 3)\n",
      "0.9171055\n",
      "[Epoch 4/5] [Batch 67/360] [D loss: 0.009307] [G loss: 7.088523] time: 0:03:10.231998\n",
      "(30, 64, 64, 3)\n",
      "0.9108176\n",
      "[Epoch 4/5] [Batch 68/360] [D loss: 0.021748] [G loss: 6.216721] time: 0:03:10.343976\n",
      "(30, 64, 64, 3)\n",
      "0.88814825\n",
      "[Epoch 4/5] [Batch 69/360] [D loss: 0.013741] [G loss: 6.171227] time: 0:03:10.460295\n",
      "(30, 64, 64, 3)\n",
      "0.88086265\n",
      "[Epoch 4/5] [Batch 70/360] [D loss: 0.009967] [G loss: 6.890358] time: 0:03:10.576655\n",
      "(30, 64, 64, 3)\n",
      "0.8372707\n",
      "[Epoch 4/5] [Batch 71/360] [D loss: 0.021532] [G loss: 6.168156] time: 0:03:10.691438\n",
      "(30, 64, 64, 3)\n",
      "0.8972923\n",
      "[Epoch 4/5] [Batch 72/360] [D loss: 0.026578] [G loss: 6.262808] time: 0:03:10.806125\n",
      "(30, 64, 64, 3)\n",
      "0.8769579\n",
      "[Epoch 4/5] [Batch 73/360] [D loss: 0.010122] [G loss: 6.502528] time: 0:03:10.926618\n",
      "(30, 64, 64, 3)\n",
      "0.86313266\n",
      "[Epoch 4/5] [Batch 74/360] [D loss: 0.008882] [G loss: 6.242566] time: 0:03:11.042356\n",
      "(30, 64, 64, 3)\n",
      "0.87944776\n",
      "[Epoch 4/5] [Batch 75/360] [D loss: 0.007961] [G loss: 6.131485] time: 0:03:11.170457\n",
      "(30, 64, 64, 3)\n",
      "0.85281134\n",
      "[Epoch 4/5] [Batch 76/360] [D loss: 0.008388] [G loss: 6.084513] time: 0:03:11.286151\n",
      "(30, 64, 64, 3)\n",
      "0.875496\n",
      "[Epoch 4/5] [Batch 77/360] [D loss: 0.011541] [G loss: 6.378110] time: 0:03:11.400906\n",
      "(30, 64, 64, 3)\n",
      "0.8219176\n",
      "[Epoch 4/5] [Batch 78/360] [D loss: 0.011904] [G loss: 6.463851] time: 0:03:11.517714\n",
      "(30, 64, 64, 3)\n",
      "0.9037543\n",
      "[Epoch 4/5] [Batch 79/360] [D loss: 0.008317] [G loss: 6.264561] time: 0:03:11.639837\n",
      "(30, 64, 64, 3)\n",
      "0.90612584\n",
      "[Epoch 4/5] [Batch 80/360] [D loss: 0.027369] [G loss: 6.090323] time: 0:03:11.755745\n",
      "(30, 64, 64, 3)\n",
      "0.8926192\n",
      "[Epoch 4/5] [Batch 81/360] [D loss: 0.033314] [G loss: 6.080185] time: 0:03:11.877542\n",
      "(30, 64, 64, 3)\n",
      "0.86756355\n",
      "[Epoch 4/5] [Batch 82/360] [D loss: 0.017005] [G loss: 6.076934] time: 0:03:11.995382\n",
      "(30, 64, 64, 3)\n",
      "0.85544175\n",
      "[Epoch 4/5] [Batch 83/360] [D loss: 0.011327] [G loss: 6.771778] time: 0:03:12.109916\n",
      "(30, 64, 64, 3)\n",
      "0.89894056\n",
      "[Epoch 4/5] [Batch 84/360] [D loss: 0.058633] [G loss: 5.983627] time: 0:03:12.228167\n",
      "(30, 64, 64, 3)\n",
      "0.90463954\n",
      "[Epoch 4/5] [Batch 85/360] [D loss: 0.304708] [G loss: 6.745371] time: 0:03:12.346122\n",
      "(30, 64, 64, 3)\n",
      "0.89626116\n",
      "[Epoch 4/5] [Batch 86/360] [D loss: 0.441113] [G loss: 5.948593] time: 0:03:12.458444\n",
      "(30, 64, 64, 3)\n",
      "0.83603686\n",
      "[Epoch 4/5] [Batch 87/360] [D loss: 0.043399] [G loss: 6.103536] time: 0:03:12.573834\n",
      "(30, 64, 64, 3)\n",
      "0.8536296\n",
      "[Epoch 4/5] [Batch 88/360] [D loss: 0.032584] [G loss: 6.843086] time: 0:03:12.691338\n",
      "(30, 64, 64, 3)\n",
      "0.9019503\n",
      "[Epoch 4/5] [Batch 89/360] [D loss: 0.013603] [G loss: 6.021541] time: 0:03:12.805772\n",
      "(30, 64, 64, 3)\n",
      "0.89698935\n",
      "[Epoch 4/5] [Batch 90/360] [D loss: 0.021392] [G loss: 5.749389] time: 0:03:12.919094\n",
      "(30, 64, 64, 3)\n",
      "0.8676898\n",
      "[Epoch 4/5] [Batch 91/360] [D loss: 0.009760] [G loss: 6.563807] time: 0:03:13.042869\n",
      "(30, 64, 64, 3)\n",
      "0.8804653\n",
      "[Epoch 4/5] [Batch 92/360] [D loss: 0.017358] [G loss: 5.817273] time: 0:03:13.154855\n",
      "(30, 64, 64, 3)\n",
      "0.8413108\n",
      "[Epoch 4/5] [Batch 93/360] [D loss: 0.007359] [G loss: 5.909039] time: 0:03:13.273958\n",
      "(30, 64, 64, 3)\n",
      "0.84083843\n",
      "[Epoch 4/5] [Batch 94/360] [D loss: 0.016041] [G loss: 6.058685] time: 0:03:13.386084\n",
      "(30, 64, 64, 3)\n",
      "0.884418\n",
      "[Epoch 4/5] [Batch 95/360] [D loss: 0.008572] [G loss: 6.665300] time: 0:03:13.503255\n",
      "(30, 64, 64, 3)\n",
      "0.84277797\n",
      "[Epoch 4/5] [Batch 96/360] [D loss: 0.023223] [G loss: 5.736501] time: 0:03:13.617841\n",
      "(30, 64, 64, 3)\n",
      "0.8968938\n",
      "[Epoch 4/5] [Batch 97/360] [D loss: 0.005495] [G loss: 5.801424] time: 0:03:13.737555\n",
      "(30, 64, 64, 3)\n",
      "0.9280887\n",
      "[Epoch 4/5] [Batch 98/360] [D loss: 0.023120] [G loss: 5.852920] time: 0:03:13.847829\n",
      "(30, 64, 64, 3)\n",
      "0.8414524\n",
      "[Epoch 4/5] [Batch 99/360] [D loss: 0.007093] [G loss: 5.878965] time: 0:03:13.973559\n",
      "(30, 64, 64, 3)\n",
      "0.9010134\n",
      "[Epoch 4/5] [Batch 100/360] [D loss: 0.021615] [G loss: 5.983043] time: 0:03:14.089413\n",
      "(30, 64, 64, 3)\n",
      "0.88276976\n",
      "[Epoch 4/5] [Batch 101/360] [D loss: 0.007005] [G loss: 6.197284] time: 0:03:14.205932\n",
      "(30, 64, 64, 3)\n",
      "0.8867505\n",
      "[Epoch 4/5] [Batch 102/360] [D loss: 0.019497] [G loss: 5.932801] time: 0:03:14.325038\n",
      "(30, 64, 64, 3)\n",
      "0.8890484\n",
      "[Epoch 4/5] [Batch 103/360] [D loss: 0.006800] [G loss: 6.023153] time: 0:03:14.444927\n",
      "(30, 64, 64, 3)\n",
      "0.8806338\n",
      "[Epoch 4/5] [Batch 104/360] [D loss: 0.056507] [G loss: 5.650928] time: 0:03:14.559033\n",
      "(30, 64, 64, 3)\n",
      "0.8848292\n",
      "[Epoch 4/5] [Batch 105/360] [D loss: 0.318375] [G loss: 6.112837] time: 0:03:14.690314\n",
      "(30, 64, 64, 3)\n",
      "0.878943\n",
      "[Epoch 4/5] [Batch 106/360] [D loss: 0.399485] [G loss: 6.083824] time: 0:03:14.805899\n",
      "(30, 64, 64, 3)\n",
      "0.8646559\n",
      "[Epoch 4/5] [Batch 107/360] [D loss: 0.127553] [G loss: 6.213260] time: 0:03:14.925052\n",
      "(30, 64, 64, 3)\n",
      "0.89147997\n",
      "[Epoch 4/5] [Batch 108/360] [D loss: 0.041802] [G loss: 5.840007] time: 0:03:15.041996\n",
      "(30, 64, 64, 3)\n",
      "0.89139515\n",
      "[Epoch 4/5] [Batch 109/360] [D loss: 0.011153] [G loss: 5.689964] time: 0:03:15.168972\n",
      "(30, 64, 64, 3)\n",
      "0.906631\n",
      "[Epoch 4/5] [Batch 110/360] [D loss: 0.019479] [G loss: 6.078709] time: 0:03:15.282490\n",
      "(30, 64, 64, 3)\n",
      "0.88844115\n",
      "[Epoch 4/5] [Batch 111/360] [D loss: 0.006873] [G loss: 5.873505] time: 0:03:15.403537\n",
      "(30, 64, 64, 3)\n",
      "0.91219074\n",
      "[Epoch 4/5] [Batch 112/360] [D loss: 0.016007] [G loss: 5.954697] time: 0:03:15.519467\n",
      "(30, 64, 64, 3)\n",
      "0.88757557\n",
      "[Epoch 4/5] [Batch 113/360] [D loss: 0.007479] [G loss: 6.356653] time: 0:03:15.635901\n",
      "(30, 64, 64, 3)\n",
      "0.8848343\n",
      "[Epoch 4/5] [Batch 114/360] [D loss: 0.021777] [G loss: 6.358728] time: 0:03:15.748872\n",
      "(30, 64, 64, 3)\n",
      "0.8920892\n",
      "[Epoch 4/5] [Batch 115/360] [D loss: 0.008613] [G loss: 6.248237] time: 0:03:15.884920\n",
      "(30, 64, 64, 3)\n",
      "0.8996603\n",
      "[Epoch 4/5] [Batch 116/360] [D loss: 0.013280] [G loss: 5.807484] time: 0:03:15.998479\n",
      "(30, 64, 64, 3)\n",
      "0.8821017\n",
      "[Epoch 4/5] [Batch 117/360] [D loss: 0.007508] [G loss: 6.384494] time: 0:03:16.112723\n",
      "(30, 64, 64, 3)\n",
      "0.9262224\n",
      "[Epoch 4/5] [Batch 118/360] [D loss: 0.013224] [G loss: 6.209137] time: 0:03:16.228769\n",
      "(30, 64, 64, 3)\n",
      "0.9013357\n",
      "[Epoch 4/5] [Batch 119/360] [D loss: 0.006752] [G loss: 6.203111] time: 0:03:16.344088\n",
      "(30, 64, 64, 3)\n",
      "0.8882265\n",
      "[Epoch 4/5] [Batch 120/360] [D loss: 0.016089] [G loss: 5.835357] time: 0:03:16.462049\n",
      "(30, 64, 64, 3)\n",
      "0.9127057\n",
      "[Epoch 4/5] [Batch 121/360] [D loss: 0.006129] [G loss: 5.891458] time: 0:03:16.578934\n",
      "(30, 64, 64, 3)\n",
      "0.8974363\n",
      "[Epoch 4/5] [Batch 122/360] [D loss: 0.018948] [G loss: 6.048983] time: 0:03:16.692589\n",
      "(30, 64, 64, 3)\n",
      "0.8696658\n",
      "[Epoch 4/5] [Batch 123/360] [D loss: 0.006768] [G loss: 5.969659] time: 0:03:16.809874\n",
      "(30, 64, 64, 3)\n",
      "0.8808387\n",
      "[Epoch 4/5] [Batch 124/360] [D loss: 0.012173] [G loss: 6.067313] time: 0:03:16.929765\n",
      "(30, 64, 64, 3)\n",
      "0.92543405\n",
      "[Epoch 4/5] [Batch 125/360] [D loss: 0.005917] [G loss: 5.868834] time: 0:03:17.046127\n",
      "(30, 64, 64, 3)\n",
      "0.91066074\n",
      "[Epoch 4/5] [Batch 126/360] [D loss: 0.012104] [G loss: 5.852991] time: 0:03:17.160573\n",
      "(30, 64, 64, 3)\n",
      "0.8383687\n",
      "[Epoch 4/5] [Batch 127/360] [D loss: 0.006187] [G loss: 5.787613] time: 0:03:17.278323\n",
      "(30, 64, 64, 3)\n",
      "0.9107755\n",
      "[Epoch 4/5] [Batch 128/360] [D loss: 0.015683] [G loss: 5.823861] time: 0:03:17.390967\n",
      "(30, 64, 64, 3)\n",
      "0.82187337\n",
      "[Epoch 4/5] [Batch 129/360] [D loss: 0.006745] [G loss: 6.377856] time: 0:03:17.516210\n",
      "(30, 64, 64, 3)\n",
      "0.89171267\n",
      "[Epoch 4/5] [Batch 130/360] [D loss: 0.011606] [G loss: 5.921486] time: 0:03:17.634805\n",
      "(30, 64, 64, 3)\n",
      "0.89063054\n",
      "[Epoch 4/5] [Batch 131/360] [D loss: 0.008875] [G loss: 5.745827] time: 0:03:17.755217\n",
      "(30, 64, 64, 3)\n",
      "0.86295\n",
      "[Epoch 4/5] [Batch 132/360] [D loss: 0.007660] [G loss: 6.119432] time: 0:03:17.870043\n",
      "(30, 64, 64, 3)\n",
      "0.8709547\n",
      "[Epoch 4/5] [Batch 133/360] [D loss: 0.007425] [G loss: 6.059068] time: 0:03:17.992789\n",
      "(30, 64, 64, 3)\n",
      "0.84221286\n",
      "[Epoch 4/5] [Batch 134/360] [D loss: 0.018116] [G loss: 6.299576] time: 0:03:18.113796\n",
      "(30, 64, 64, 3)\n",
      "0.8905365\n",
      "[Epoch 4/5] [Batch 135/360] [D loss: 0.009774] [G loss: 6.029606] time: 0:03:18.229205\n",
      "(30, 64, 64, 3)\n",
      "0.8703769\n",
      "[Epoch 4/5] [Batch 136/360] [D loss: 0.008937] [G loss: 5.648802] time: 0:03:18.343012\n",
      "(30, 64, 64, 3)\n",
      "0.8774329\n",
      "[Epoch 4/5] [Batch 137/360] [D loss: 0.007911] [G loss: 6.084816] time: 0:03:18.462099\n",
      "(30, 64, 64, 3)\n",
      "0.8956866\n",
      "[Epoch 4/5] [Batch 138/360] [D loss: 0.008077] [G loss: 5.532193] time: 0:03:18.582510\n",
      "(30, 64, 64, 3)\n",
      "0.91619325\n",
      "[Epoch 4/5] [Batch 139/360] [D loss: 0.006020] [G loss: 5.634650] time: 0:03:18.698758\n",
      "(30, 64, 64, 3)\n",
      "0.9486463\n",
      "[Epoch 4/5] [Batch 140/360] [D loss: 0.009430] [G loss: 5.579955] time: 0:03:18.810605\n",
      "(30, 64, 64, 3)\n",
      "0.9051493\n",
      "[Epoch 4/5] [Batch 141/360] [D loss: 0.006408] [G loss: 5.328688] time: 0:03:18.923259\n",
      "(30, 64, 64, 3)\n",
      "0.88610536\n",
      "[Epoch 4/5] [Batch 142/360] [D loss: 0.020281] [G loss: 5.882415] time: 0:03:19.041391\n",
      "(30, 64, 64, 3)\n",
      "0.8999042\n",
      "[Epoch 4/5] [Batch 143/360] [D loss: 0.007473] [G loss: 5.562938] time: 0:03:19.156144\n",
      "(30, 64, 64, 3)\n",
      "0.84579104\n",
      "[Epoch 4/5] [Batch 144/360] [D loss: 0.006631] [G loss: 6.186037] time: 0:03:19.271261\n",
      "(30, 64, 64, 3)\n",
      "0.8725832\n",
      "[Epoch 4/5] [Batch 145/360] [D loss: 0.006434] [G loss: 5.628982] time: 0:03:19.395235\n",
      "(30, 64, 64, 3)\n",
      "0.8479891\n",
      "[Epoch 4/5] [Batch 146/360] [D loss: 0.016758] [G loss: 5.904471] time: 0:03:19.514867\n",
      "(30, 64, 64, 3)\n",
      "0.90079594\n",
      "[Epoch 4/5] [Batch 147/360] [D loss: 0.005834] [G loss: 5.985129] time: 0:03:19.639167\n",
      "(30, 64, 64, 3)\n",
      "0.8902146\n",
      "[Epoch 4/5] [Batch 148/360] [D loss: 0.020404] [G loss: 5.820479] time: 0:03:19.753548\n",
      "(30, 64, 64, 3)\n",
      "0.88093275\n",
      "[Epoch 4/5] [Batch 149/360] [D loss: 0.007149] [G loss: 5.925987] time: 0:03:19.869322\n",
      "(30, 64, 64, 3)\n",
      "0.8826272\n",
      "[Epoch 4/5] [Batch 150/360] [D loss: 0.019257] [G loss: 5.885252] time: 0:03:19.983542\n",
      "(30, 64, 64, 3)\n",
      "0.87577796\n",
      "[Epoch 4/5] [Batch 151/360] [D loss: 0.007477] [G loss: 5.662446] time: 0:03:20.104886\n",
      "(30, 64, 64, 3)\n",
      "0.92774343\n",
      "[Epoch 4/5] [Batch 152/360] [D loss: 0.036416] [G loss: 6.284642] time: 0:03:20.216375\n",
      "(30, 64, 64, 3)\n",
      "0.940558\n",
      "[Epoch 4/5] [Batch 153/360] [D loss: 0.033799] [G loss: 6.160827] time: 0:03:20.336759\n",
      "(30, 64, 64, 3)\n",
      "0.8933986\n",
      "[Epoch 4/5] [Batch 154/360] [D loss: 0.008985] [G loss: 5.973994] time: 0:03:20.446522\n",
      "(30, 64, 64, 3)\n",
      "0.9103491\n",
      "[Epoch 4/5] [Batch 155/360] [D loss: 0.072504] [G loss: 6.159491] time: 0:03:20.561428\n",
      "(30, 64, 64, 3)\n",
      "0.90007406\n",
      "[Epoch 4/5] [Batch 156/360] [D loss: 0.240845] [G loss: 6.163608] time: 0:03:20.678928\n",
      "(30, 64, 64, 3)\n",
      "0.879893\n",
      "[Epoch 4/5] [Batch 157/360] [D loss: 0.446885] [G loss: 5.454051] time: 0:03:20.792131\n",
      "(30, 64, 64, 3)\n",
      "0.8697484\n",
      "[Epoch 4/5] [Batch 158/360] [D loss: 0.512786] [G loss: 6.204936] time: 0:03:20.907474\n",
      "(30, 64, 64, 3)\n",
      "0.8655262\n",
      "[Epoch 4/5] [Batch 159/360] [D loss: 0.295681] [G loss: 5.898646] time: 0:03:21.024091\n",
      "(30, 64, 64, 3)\n",
      "0.86964124\n",
      "[Epoch 4/5] [Batch 160/360] [D loss: 0.186146] [G loss: 5.538549] time: 0:03:21.142483\n",
      "(30, 64, 64, 3)\n",
      "0.9002374\n",
      "[Epoch 4/5] [Batch 161/360] [D loss: 0.042107] [G loss: 5.715648] time: 0:03:21.257979\n",
      "(30, 64, 64, 3)\n",
      "0.91848564\n",
      "[Epoch 4/5] [Batch 162/360] [D loss: 0.097677] [G loss: 5.663069] time: 0:03:21.369507\n",
      "(30, 64, 64, 3)\n",
      "0.8729124\n",
      "[Epoch 4/5] [Batch 163/360] [D loss: 0.038716] [G loss: 5.371567] time: 0:03:21.488149\n",
      "(30, 64, 64, 3)\n",
      "0.88509446\n",
      "[Epoch 4/5] [Batch 164/360] [D loss: 0.041042] [G loss: 5.548164] time: 0:03:21.602341\n",
      "(30, 64, 64, 3)\n",
      "0.85683614\n",
      "[Epoch 4/5] [Batch 165/360] [D loss: 0.027278] [G loss: 5.828539] time: 0:03:21.720774\n",
      "(30, 64, 64, 3)\n",
      "0.9050705\n",
      "[Epoch 4/5] [Batch 166/360] [D loss: 0.024214] [G loss: 5.750167] time: 0:03:21.838543\n",
      "(30, 64, 64, 3)\n",
      "0.8285942\n",
      "[Epoch 4/5] [Batch 167/360] [D loss: 0.016497] [G loss: 5.831208] time: 0:03:21.959584\n",
      "(30, 64, 64, 3)\n",
      "0.9081895\n",
      "[Epoch 4/5] [Batch 168/360] [D loss: 0.013625] [G loss: 5.547778] time: 0:03:22.073407\n",
      "(30, 64, 64, 3)\n",
      "0.89072615\n",
      "[Epoch 4/5] [Batch 169/360] [D loss: 0.011644] [G loss: 5.970700] time: 0:03:22.192744\n",
      "(30, 64, 64, 3)\n",
      "0.8368766\n",
      "[Epoch 4/5] [Batch 170/360] [D loss: 0.007471] [G loss: 5.485033] time: 0:03:22.306253\n",
      "(30, 64, 64, 3)\n",
      "0.90933967\n",
      "[Epoch 4/5] [Batch 171/360] [D loss: 0.013439] [G loss: 5.324290] time: 0:03:22.425719\n",
      "(30, 64, 64, 3)\n",
      "0.8916943\n",
      "[Epoch 4/5] [Batch 172/360] [D loss: 0.009367] [G loss: 5.677423] time: 0:03:22.541153\n",
      "(30, 64, 64, 3)\n",
      "0.89588356\n",
      "[Epoch 4/5] [Batch 173/360] [D loss: 0.013956] [G loss: 5.873643] time: 0:03:22.653857\n",
      "(30, 64, 64, 3)\n",
      "0.90717936\n",
      "[Epoch 4/5] [Batch 174/360] [D loss: 0.006929] [G loss: 5.296815] time: 0:03:22.772427\n",
      "(30, 64, 64, 3)\n",
      "0.8998082\n",
      "[Epoch 4/5] [Batch 175/360] [D loss: 0.011347] [G loss: 5.704182] time: 0:03:22.884798\n",
      "(30, 64, 64, 3)\n",
      "0.86966926\n",
      "[Epoch 4/5] [Batch 176/360] [D loss: 0.007258] [G loss: 6.297174] time: 0:03:23.000985\n",
      "(30, 64, 64, 3)\n",
      "0.8445137\n",
      "[Epoch 4/5] [Batch 177/360] [D loss: 0.010899] [G loss: 5.501547] time: 0:03:23.117210\n",
      "(30, 64, 64, 3)\n",
      "0.91278523\n",
      "[Epoch 4/5] [Batch 178/360] [D loss: 0.007920] [G loss: 5.424216] time: 0:03:23.238999\n",
      "(30, 64, 64, 3)\n",
      "0.92516345\n",
      "[Epoch 4/5] [Batch 179/360] [D loss: 0.010042] [G loss: 5.840820] time: 0:03:23.352057\n",
      "(30, 64, 64, 3)\n",
      "0.8769872\n",
      "[Epoch 4/5] [Batch 180/360] [D loss: 0.007198] [G loss: 5.552221] time: 0:03:23.465091\n",
      "(30, 64, 64, 3)\n",
      "0.8475675\n",
      "[Epoch 4/5] [Batch 181/360] [D loss: 0.007094] [G loss: 6.159155] time: 0:03:23.579543\n",
      "(30, 64, 64, 3)\n",
      "0.9118373\n",
      "[Epoch 4/5] [Batch 182/360] [D loss: 0.006930] [G loss: 5.855815] time: 0:03:23.694465\n",
      "(30, 64, 64, 3)\n",
      "0.857944\n",
      "[Epoch 4/5] [Batch 183/360] [D loss: 0.007941] [G loss: 6.057137] time: 0:03:23.822753\n",
      "(30, 64, 64, 3)\n",
      "0.88284373\n",
      "[Epoch 4/5] [Batch 184/360] [D loss: 0.007012] [G loss: 5.703825] time: 0:03:23.935619\n",
      "(30, 64, 64, 3)\n",
      "0.8874302\n",
      "[Epoch 4/5] [Batch 185/360] [D loss: 0.008189] [G loss: 5.735879] time: 0:03:24.052547\n",
      "(30, 64, 64, 3)\n",
      "0.85698575\n",
      "[Epoch 4/5] [Batch 186/360] [D loss: 0.007599] [G loss: 6.112648] time: 0:03:24.167218\n",
      "(30, 64, 64, 3)\n",
      "0.86627525\n",
      "[Epoch 4/5] [Batch 187/360] [D loss: 0.008742] [G loss: 5.583182] time: 0:03:24.287058\n",
      "(30, 64, 64, 3)\n",
      "0.87377834\n",
      "[Epoch 4/5] [Batch 188/360] [D loss: 0.007364] [G loss: 6.157691] time: 0:03:24.403991\n",
      "(30, 64, 64, 3)\n",
      "0.8935501\n",
      "[Epoch 4/5] [Batch 189/360] [D loss: 0.008043] [G loss: 5.572134] time: 0:03:24.521200\n",
      "(30, 64, 64, 3)\n",
      "0.914388\n",
      "[Epoch 4/5] [Batch 190/360] [D loss: 0.009020] [G loss: 5.286580] time: 0:03:24.633306\n",
      "(30, 64, 64, 3)\n",
      "0.8914756\n",
      "[Epoch 4/5] [Batch 191/360] [D loss: 0.008241] [G loss: 5.658237] time: 0:03:24.747942\n",
      "(30, 64, 64, 3)\n",
      "0.87263423\n",
      "[Epoch 4/5] [Batch 192/360] [D loss: 0.007281] [G loss: 5.555256] time: 0:03:24.865623\n",
      "(30, 64, 64, 3)\n",
      "0.9430857\n",
      "[Epoch 4/5] [Batch 193/360] [D loss: 0.007879] [G loss: 5.657254] time: 0:03:24.983818\n",
      "(30, 64, 64, 3)\n",
      "0.9184351\n",
      "[Epoch 4/5] [Batch 194/360] [D loss: 0.007746] [G loss: 6.049341] time: 0:03:25.098034\n",
      "(30, 64, 64, 3)\n",
      "0.88506466\n",
      "[Epoch 4/5] [Batch 195/360] [D loss: 0.008870] [G loss: 5.989787] time: 0:03:25.213657\n",
      "(30, 64, 64, 3)\n",
      "0.91933864\n",
      "[Epoch 4/5] [Batch 196/360] [D loss: 0.005063] [G loss: 5.781919] time: 0:03:25.329864\n",
      "(30, 64, 64, 3)\n",
      "0.9169759\n",
      "[Epoch 4/5] [Batch 197/360] [D loss: 0.020421] [G loss: 5.354157] time: 0:03:25.446874\n",
      "(30, 64, 64, 3)\n",
      "0.8730862\n",
      "[Epoch 4/5] [Batch 198/360] [D loss: 0.007458] [G loss: 5.516699] time: 0:03:25.563343\n",
      "(30, 64, 64, 3)\n",
      "0.82690525\n",
      "[Epoch 4/5] [Batch 199/360] [D loss: 0.006636] [G loss: 5.596519] time: 0:03:25.679324\n",
      "(30, 64, 64, 3)\n",
      "0.86698794\n",
      "[Epoch 4/5] [Batch 200/360] [D loss: 0.006808] [G loss: 5.840230] time: 0:03:25.793685\n",
      "(30, 64, 64, 3)\n",
      "0.8151221\n",
      "[Epoch 4/5] [Batch 201/360] [D loss: 0.007864] [G loss: 5.958770] time: 0:03:25.911329\n",
      "(30, 64, 64, 3)\n",
      "0.8569691\n",
      "[Epoch 4/5] [Batch 202/360] [D loss: 0.007104] [G loss: 5.883129] time: 0:03:26.026899\n",
      "(30, 64, 64, 3)\n",
      "0.9455886\n",
      "[Epoch 4/5] [Batch 203/360] [D loss: 0.006383] [G loss: 5.971169] time: 0:03:26.145541\n",
      "(30, 64, 64, 3)\n",
      "0.9054468\n",
      "[Epoch 4/5] [Batch 204/360] [D loss: 0.005466] [G loss: 5.736043] time: 0:03:26.258237\n",
      "(30, 64, 64, 3)\n",
      "0.8584798\n",
      "[Epoch 4/5] [Batch 205/360] [D loss: 0.020562] [G loss: 5.262949] time: 0:03:26.377056\n",
      "(30, 64, 64, 3)\n",
      "0.86082345\n",
      "[Epoch 4/5] [Batch 206/360] [D loss: 0.010505] [G loss: 6.026553] time: 0:03:26.492586\n",
      "(30, 64, 64, 3)\n",
      "0.8653961\n",
      "[Epoch 4/5] [Batch 207/360] [D loss: 0.007081] [G loss: 6.320383] time: 0:03:26.609107\n",
      "(30, 64, 64, 3)\n",
      "0.8613692\n",
      "[Epoch 4/5] [Batch 208/360] [D loss: 0.016214] [G loss: 6.238575] time: 0:03:26.724212\n",
      "(30, 64, 64, 3)\n",
      "0.8894113\n",
      "[Epoch 4/5] [Batch 209/360] [D loss: 0.007381] [G loss: 5.431100] time: 0:03:26.845031\n",
      "(30, 64, 64, 3)\n",
      "0.87330216\n",
      "[Epoch 4/5] [Batch 210/360] [D loss: 0.033027] [G loss: 5.537980] time: 0:03:26.962269\n",
      "(30, 64, 64, 3)\n",
      "0.86975735\n",
      "[Epoch 4/5] [Batch 211/360] [D loss: 0.060042] [G loss: 6.026856] time: 0:03:27.082755\n",
      "(30, 64, 64, 3)\n",
      "0.8907385\n",
      "[Epoch 4/5] [Batch 212/360] [D loss: 0.246478] [G loss: 6.001244] time: 0:03:27.201106\n",
      "(30, 64, 64, 3)\n",
      "0.90365475\n",
      "[Epoch 4/5] [Batch 213/360] [D loss: 0.507861] [G loss: 5.596056] time: 0:03:27.323703\n",
      "(30, 64, 64, 3)\n",
      "0.8772464\n",
      "[Epoch 4/5] [Batch 214/360] [D loss: 0.344318] [G loss: 5.829360] time: 0:03:27.437028\n",
      "(30, 64, 64, 3)\n",
      "0.86148053\n",
      "[Epoch 4/5] [Batch 215/360] [D loss: 0.161926] [G loss: 5.117763] time: 0:03:27.563421\n",
      "(30, 64, 64, 3)\n",
      "0.8727811\n",
      "[Epoch 4/5] [Batch 216/360] [D loss: 0.193936] [G loss: 5.251855] time: 0:03:27.684260\n",
      "(30, 64, 64, 3)\n",
      "0.8612463\n",
      "[Epoch 4/5] [Batch 217/360] [D loss: 0.065973] [G loss: 5.162602] time: 0:03:27.811586\n",
      "(30, 64, 64, 3)\n",
      "0.8858827\n",
      "[Epoch 4/5] [Batch 218/360] [D loss: 0.012576] [G loss: 5.100564] time: 0:03:27.925923\n",
      "(30, 64, 64, 3)\n",
      "0.8857827\n",
      "[Epoch 4/5] [Batch 219/360] [D loss: 0.028503] [G loss: 5.624809] time: 0:03:28.044259\n",
      "(30, 64, 64, 3)\n",
      "0.8850691\n",
      "[Epoch 4/5] [Batch 220/360] [D loss: 0.010463] [G loss: 5.495489] time: 0:03:28.159928\n",
      "(30, 64, 64, 3)\n",
      "0.9085471\n",
      "[Epoch 4/5] [Batch 221/360] [D loss: 0.029107] [G loss: 5.404869] time: 0:03:28.285365\n",
      "(30, 64, 64, 3)\n",
      "0.919306\n",
      "[Epoch 4/5] [Batch 222/360] [D loss: 0.009824] [G loss: 5.101458] time: 0:03:28.397775\n",
      "(30, 64, 64, 3)\n",
      "0.87630624\n",
      "[Epoch 4/5] [Batch 223/360] [D loss: 0.024496] [G loss: 5.547139] time: 0:03:28.519121\n",
      "(30, 64, 64, 3)\n",
      "0.92050433\n",
      "[Epoch 4/5] [Batch 224/360] [D loss: 0.008166] [G loss: 5.273238] time: 0:03:28.635392\n",
      "(30, 64, 64, 3)\n",
      "0.8484831\n",
      "[Epoch 4/5] [Batch 225/360] [D loss: 0.027031] [G loss: 5.194725] time: 0:03:28.749459\n",
      "(30, 64, 64, 3)\n",
      "0.9024813\n",
      "[Epoch 4/5] [Batch 226/360] [D loss: 0.008466] [G loss: 5.575466] time: 0:03:28.865452\n",
      "(30, 64, 64, 3)\n",
      "0.8673298\n",
      "[Epoch 4/5] [Batch 227/360] [D loss: 0.014963] [G loss: 5.918432] time: 0:03:28.987566\n",
      "(30, 64, 64, 3)\n",
      "0.9072473\n",
      "[Epoch 4/5] [Batch 228/360] [D loss: 0.008660] [G loss: 5.394063] time: 0:03:29.105946\n",
      "(30, 64, 64, 3)\n",
      "0.8598194\n",
      "[Epoch 4/5] [Batch 229/360] [D loss: 0.023612] [G loss: 5.534911] time: 0:03:29.224299\n",
      "(30, 64, 64, 3)\n",
      "0.8860564\n",
      "[Epoch 4/5] [Batch 230/360] [D loss: 0.011760] [G loss: 5.629560] time: 0:03:29.340133\n",
      "(30, 64, 64, 3)\n",
      "0.87261385\n",
      "[Epoch 4/5] [Batch 231/360] [D loss: 0.010697] [G loss: 5.688995] time: 0:03:29.456032\n",
      "(30, 64, 64, 3)\n",
      "0.889438\n",
      "[Epoch 4/5] [Batch 232/360] [D loss: 0.007653] [G loss: 5.344756] time: 0:03:29.575386\n",
      "(30, 64, 64, 3)\n",
      "0.91379577\n",
      "[Epoch 4/5] [Batch 233/360] [D loss: 0.008087] [G loss: 5.926251] time: 0:03:29.690573\n",
      "(30, 64, 64, 3)\n",
      "0.85086685\n",
      "[Epoch 4/5] [Batch 234/360] [D loss: 0.013935] [G loss: 6.038442] time: 0:03:29.805338\n",
      "(30, 64, 64, 3)\n",
      "0.8739824\n",
      "[Epoch 4/5] [Batch 235/360] [D loss: 0.007794] [G loss: 5.031193] time: 0:03:29.924606\n",
      "(30, 64, 64, 3)\n",
      "0.853681\n",
      "[Epoch 4/5] [Batch 236/360] [D loss: 0.009434] [G loss: 5.745120] time: 0:03:30.041153\n",
      "(30, 64, 64, 3)\n",
      "0.9029901\n",
      "[Epoch 4/5] [Batch 237/360] [D loss: 0.007224] [G loss: 5.004753] time: 0:03:30.159466\n",
      "(30, 64, 64, 3)\n",
      "0.9040981\n",
      "[Epoch 4/5] [Batch 238/360] [D loss: 0.013176] [G loss: 5.384770] time: 0:03:30.274350\n",
      "(30, 64, 64, 3)\n",
      "0.8927745\n",
      "[Epoch 4/5] [Batch 239/360] [D loss: 0.007131] [G loss: 5.340123] time: 0:03:30.388509\n",
      "(30, 64, 64, 3)\n",
      "0.93832487\n",
      "[Epoch 4/5] [Batch 240/360] [D loss: 0.019698] [G loss: 5.427682] time: 0:03:30.499726\n",
      "(30, 64, 64, 3)\n",
      "0.87843347\n",
      "[Epoch 4/5] [Batch 241/360] [D loss: 0.007258] [G loss: 5.516138] time: 0:03:30.619243\n",
      "(30, 64, 64, 3)\n",
      "0.8921904\n",
      "[Epoch 4/5] [Batch 242/360] [D loss: 0.051679] [G loss: 5.765979] time: 0:03:30.732709\n",
      "(30, 64, 64, 3)\n",
      "0.9407833\n",
      "[Epoch 4/5] [Batch 243/360] [D loss: 0.031417] [G loss: 5.194708] time: 0:03:30.856272\n",
      "(30, 64, 64, 3)\n",
      "0.8746472\n",
      "[Epoch 4/5] [Batch 244/360] [D loss: 0.007359] [G loss: 5.305823] time: 0:03:30.970976\n",
      "(30, 64, 64, 3)\n",
      "0.8249175\n",
      "[Epoch 4/5] [Batch 245/360] [D loss: 0.017358] [G loss: 5.549728] time: 0:03:31.088696\n",
      "(30, 64, 64, 3)\n",
      "0.92634517\n",
      "[Epoch 4/5] [Batch 246/360] [D loss: 0.008049] [G loss: 5.468643] time: 0:03:31.208478\n",
      "(30, 64, 64, 3)\n",
      "0.9011137\n",
      "[Epoch 4/5] [Batch 247/360] [D loss: 0.021969] [G loss: 5.388766] time: 0:03:31.322983\n",
      "(30, 64, 64, 3)\n",
      "0.9233678\n",
      "[Epoch 4/5] [Batch 248/360] [D loss: 0.005927] [G loss: 5.226313] time: 0:03:31.435039\n",
      "(30, 64, 64, 3)\n",
      "0.9253444\n",
      "[Epoch 4/5] [Batch 249/360] [D loss: 0.052761] [G loss: 5.984507] time: 0:03:31.561307\n",
      "(30, 64, 64, 3)\n",
      "0.926518\n",
      "[Epoch 4/5] [Batch 250/360] [D loss: 0.064976] [G loss: 5.445887] time: 0:03:31.678888\n",
      "(30, 64, 64, 3)\n",
      "0.87523776\n",
      "[Epoch 4/5] [Batch 251/360] [D loss: 0.090818] [G loss: 5.606968] time: 0:03:31.792941\n",
      "(30, 64, 64, 3)\n",
      "0.88879824\n",
      "[Epoch 4/5] [Batch 252/360] [D loss: 0.349156] [G loss: 6.160137] time: 0:03:31.908138\n",
      "(30, 64, 64, 3)\n",
      "0.8874108\n",
      "[Epoch 4/5] [Batch 253/360] [D loss: 0.492367] [G loss: 5.929569] time: 0:03:32.023257\n",
      "(30, 64, 64, 3)\n",
      "0.93299866\n",
      "[Epoch 4/5] [Batch 254/360] [D loss: 0.426152] [G loss: 4.669143] time: 0:03:32.139940\n",
      "(30, 64, 64, 3)\n",
      "0.8761346\n",
      "[Epoch 4/5] [Batch 255/360] [D loss: 0.299860] [G loss: 5.005350] time: 0:03:32.261911\n",
      "(30, 64, 64, 3)\n",
      "0.89757043\n",
      "[Epoch 4/5] [Batch 256/360] [D loss: 0.080344] [G loss: 4.720109] time: 0:03:32.378548\n",
      "(30, 64, 64, 3)\n",
      "0.9086959\n",
      "[Epoch 4/5] [Batch 257/360] [D loss: 0.021735] [G loss: 5.575995] time: 0:03:32.490568\n",
      "(30, 64, 64, 3)\n",
      "0.9288509\n",
      "[Epoch 4/5] [Batch 258/360] [D loss: 0.052634] [G loss: 5.836090] time: 0:03:32.606591\n",
      "(30, 64, 64, 3)\n",
      "0.91263556\n",
      "[Epoch 4/5] [Batch 259/360] [D loss: 0.021533] [G loss: 4.822955] time: 0:03:32.726644\n",
      "(30, 64, 64, 3)\n",
      "0.9158794\n",
      "[Epoch 4/5] [Batch 260/360] [D loss: 0.025362] [G loss: 4.939102] time: 0:03:32.838926\n",
      "(30, 64, 64, 3)\n",
      "0.92015904\n",
      "[Epoch 4/5] [Batch 261/360] [D loss: 0.016213] [G loss: 4.740204] time: 0:03:32.963862\n",
      "(30, 64, 64, 3)\n",
      "0.94047785\n",
      "[Epoch 4/5] [Batch 262/360] [D loss: 0.015916] [G loss: 5.240100] time: 0:03:33.077060\n",
      "(30, 64, 64, 3)\n",
      "0.8794262\n",
      "[Epoch 4/5] [Batch 263/360] [D loss: 0.016821] [G loss: 4.670878] time: 0:03:33.189876\n",
      "(30, 64, 64, 3)\n",
      "0.8944633\n",
      "[Epoch 4/5] [Batch 264/360] [D loss: 0.010812] [G loss: 5.519134] time: 0:03:33.310011\n",
      "(30, 64, 64, 3)\n",
      "0.9044025\n",
      "[Epoch 4/5] [Batch 265/360] [D loss: 0.014532] [G loss: 5.201948] time: 0:03:33.426412\n",
      "(30, 64, 64, 3)\n",
      "0.8740657\n",
      "[Epoch 4/5] [Batch 266/360] [D loss: 0.009187] [G loss: 4.899225] time: 0:03:33.542050\n",
      "(30, 64, 64, 3)\n",
      "0.8885226\n",
      "[Epoch 4/5] [Batch 267/360] [D loss: 0.017217] [G loss: 4.792923] time: 0:03:33.667092\n",
      "(30, 64, 64, 3)\n",
      "0.9183895\n",
      "[Epoch 4/5] [Batch 268/360] [D loss: 0.009553] [G loss: 5.454720] time: 0:03:33.786960\n",
      "(30, 64, 64, 3)\n",
      "0.8893037\n",
      "[Epoch 4/5] [Batch 269/360] [D loss: 0.014298] [G loss: 4.836183] time: 0:03:33.911617\n",
      "(30, 64, 64, 3)\n",
      "0.9107769\n",
      "[Epoch 4/5] [Batch 270/360] [D loss: 0.007954] [G loss: 5.820749] time: 0:03:34.031083\n",
      "(30, 64, 64, 3)\n",
      "0.94281006\n",
      "[Epoch 4/5] [Batch 271/360] [D loss: 0.015243] [G loss: 5.211134] time: 0:03:34.146079\n",
      "(30, 64, 64, 3)\n",
      "0.89747316\n",
      "[Epoch 4/5] [Batch 272/360] [D loss: 0.009901] [G loss: 4.863493] time: 0:03:34.257856\n",
      "(30, 64, 64, 3)\n",
      "0.91182643\n",
      "[Epoch 4/5] [Batch 273/360] [D loss: 0.011164] [G loss: 5.366826] time: 0:03:34.375279\n",
      "(30, 64, 64, 3)\n",
      "0.92001534\n",
      "[Epoch 4/5] [Batch 274/360] [D loss: 0.008685] [G loss: 5.450679] time: 0:03:34.489451\n",
      "(30, 64, 64, 3)\n",
      "0.8806114\n",
      "[Epoch 4/5] [Batch 275/360] [D loss: 0.008690] [G loss: 4.595426] time: 0:03:34.604661\n",
      "(30, 64, 64, 3)\n",
      "0.8722682\n",
      "[Epoch 4/5] [Batch 276/360] [D loss: 0.011560] [G loss: 5.402815] time: 0:03:34.721732\n",
      "(30, 64, 64, 3)\n",
      "0.87685746\n",
      "[Epoch 4/5] [Batch 277/360] [D loss: 0.010920] [G loss: 5.067019] time: 0:03:34.842026\n",
      "(30, 64, 64, 3)\n",
      "0.8618247\n",
      "[Epoch 4/5] [Batch 278/360] [D loss: 0.006924] [G loss: 5.923609] time: 0:03:34.959802\n",
      "(30, 64, 64, 3)\n",
      "0.9107094\n",
      "[Epoch 4/5] [Batch 279/360] [D loss: 0.011315] [G loss: 4.612129] time: 0:03:35.078450\n",
      "(30, 64, 64, 3)\n",
      "0.90883493\n",
      "[Epoch 4/5] [Batch 280/360] [D loss: 0.008238] [G loss: 5.280914] time: 0:03:35.193183\n",
      "(30, 64, 64, 3)\n",
      "0.9014762\n",
      "[Epoch 4/5] [Batch 281/360] [D loss: 0.014745] [G loss: 4.895418] time: 0:03:35.313358\n",
      "(30, 64, 64, 3)\n",
      "0.8232215\n",
      "[Epoch 4/5] [Batch 282/360] [D loss: 0.009006] [G loss: 5.143750] time: 0:03:35.436798\n",
      "(30, 64, 64, 3)\n",
      "0.87690705\n",
      "[Epoch 4/5] [Batch 283/360] [D loss: 0.013295] [G loss: 5.164612] time: 0:03:35.558062\n",
      "(30, 64, 64, 3)\n",
      "0.8549363\n",
      "[Epoch 4/5] [Batch 284/360] [D loss: 0.008353] [G loss: 5.910138] time: 0:03:35.675581\n",
      "(30, 64, 64, 3)\n",
      "0.9209524\n",
      "[Epoch 4/5] [Batch 285/360] [D loss: 0.015677] [G loss: 5.621543] time: 0:03:35.793582\n",
      "(30, 64, 64, 3)\n",
      "0.8877039\n",
      "[Epoch 4/5] [Batch 286/360] [D loss: 0.007221] [G loss: 5.054382] time: 0:03:35.918937\n",
      "(30, 64, 64, 3)\n",
      "0.8683491\n",
      "[Epoch 4/5] [Batch 287/360] [D loss: 0.016795] [G loss: 4.826579] time: 0:03:36.045420\n",
      "(30, 64, 64, 3)\n",
      "0.8748053\n",
      "[Epoch 4/5] [Batch 288/360] [D loss: 0.009604] [G loss: 5.376094] time: 0:03:36.165226\n",
      "(30, 64, 64, 3)\n",
      "0.8897528\n",
      "[Epoch 4/5] [Batch 289/360] [D loss: 0.022681] [G loss: 4.936264] time: 0:03:36.283442\n",
      "(30, 64, 64, 3)\n",
      "0.83400613\n",
      "[Epoch 4/5] [Batch 290/360] [D loss: 0.008451] [G loss: 5.364811] time: 0:03:36.398560\n",
      "(30, 64, 64, 3)\n",
      "0.8820599\n",
      "[Epoch 4/5] [Batch 291/360] [D loss: 0.008666] [G loss: 4.787282] time: 0:03:36.518762\n",
      "(30, 64, 64, 3)\n",
      "0.9090069\n",
      "[Epoch 4/5] [Batch 292/360] [D loss: 0.007257] [G loss: 5.527068] time: 0:03:36.631791\n",
      "(30, 64, 64, 3)\n",
      "0.8833874\n",
      "[Epoch 4/5] [Batch 293/360] [D loss: 0.013331] [G loss: 5.575484] time: 0:03:36.746150\n",
      "(30, 64, 64, 3)\n",
      "0.9012273\n",
      "[Epoch 4/5] [Batch 294/360] [D loss: 0.006937] [G loss: 5.382754] time: 0:03:36.861467\n",
      "(30, 64, 64, 3)\n",
      "0.90066725\n",
      "[Epoch 4/5] [Batch 295/360] [D loss: 0.009950] [G loss: 5.054405] time: 0:03:36.979051\n",
      "(30, 64, 64, 3)\n",
      "0.88677174\n",
      "[Epoch 4/5] [Batch 296/360] [D loss: 0.005410] [G loss: 4.942619] time: 0:03:37.215764\n",
      "(30, 64, 64, 3)\n",
      "0.8967442\n",
      "[Epoch 4/5] [Batch 297/360] [D loss: 0.014515] [G loss: 5.131647] time: 0:03:37.334005\n",
      "(30, 64, 64, 3)\n",
      "0.8975663\n",
      "[Epoch 4/5] [Batch 298/360] [D loss: 0.006999] [G loss: 5.087046] time: 0:03:37.450051\n",
      "(30, 64, 64, 3)\n",
      "0.8569529\n",
      "[Epoch 4/5] [Batch 299/360] [D loss: 0.034438] [G loss: 5.546248] time: 0:03:37.568802\n",
      "(30, 64, 64, 3)\n",
      "0.86552256\n",
      "[Epoch 4/5] [Batch 300/360] [D loss: 0.008906] [G loss: 5.796736] time: 0:03:37.689057\n",
      "(30, 64, 64, 3)\n",
      "0.9089585\n",
      "[Epoch 4/5] [Batch 301/360] [D loss: 0.021938] [G loss: 5.015141] time: 0:03:37.807168\n",
      "(30, 64, 64, 3)\n",
      "0.866978\n",
      "[Epoch 4/5] [Batch 302/360] [D loss: 0.009542] [G loss: 5.556988] time: 0:03:37.929478\n",
      "(30, 64, 64, 3)\n",
      "0.90346426\n",
      "[Epoch 4/5] [Batch 303/360] [D loss: 0.275936] [G loss: 5.701402] time: 0:03:38.044624\n",
      "(30, 64, 64, 3)\n",
      "0.85645443\n",
      "[Epoch 4/5] [Batch 304/360] [D loss: 0.546748] [G loss: 5.368443] time: 0:03:38.172207\n",
      "(30, 64, 64, 3)\n",
      "0.8906167\n",
      "[Epoch 4/5] [Batch 305/360] [D loss: 0.480719] [G loss: 4.977761] time: 0:03:38.288010\n",
      "(30, 64, 64, 3)\n",
      "0.8902116\n",
      "[Epoch 4/5] [Batch 306/360] [D loss: 0.357245] [G loss: 4.817715] time: 0:03:38.407499\n",
      "(30, 64, 64, 3)\n",
      "0.83807117\n",
      "[Epoch 4/5] [Batch 307/360] [D loss: 0.286133] [G loss: 5.351213] time: 0:03:38.524504\n",
      "(30, 64, 64, 3)\n",
      "0.8653465\n",
      "[Epoch 4/5] [Batch 308/360] [D loss: 0.245072] [G loss: 4.875674] time: 0:03:38.645640\n",
      "(30, 64, 64, 3)\n",
      "0.90685725\n",
      "[Epoch 4/5] [Batch 309/360] [D loss: 0.236913] [G loss: 4.889581] time: 0:03:38.764703\n",
      "(30, 64, 64, 3)\n",
      "0.92355055\n",
      "[Epoch 4/5] [Batch 310/360] [D loss: 0.056063] [G loss: 4.582141] time: 0:03:38.885115\n",
      "(30, 64, 64, 3)\n",
      "0.90897465\n",
      "[Epoch 4/5] [Batch 311/360] [D loss: 0.195821] [G loss: 4.514614] time: 0:03:39.006792\n",
      "(30, 64, 64, 3)\n",
      "0.9462867\n",
      "[Epoch 4/5] [Batch 312/360] [D loss: 0.201477] [G loss: 5.044760] time: 0:03:39.121908\n",
      "(30, 64, 64, 3)\n",
      "0.90752345\n",
      "[Epoch 4/5] [Batch 313/360] [D loss: 0.119525] [G loss: 4.694885] time: 0:03:39.235828\n",
      "(30, 64, 64, 3)\n",
      "0.94546556\n",
      "[Epoch 4/5] [Batch 314/360] [D loss: 0.058713] [G loss: 4.831498] time: 0:03:39.354554\n",
      "(30, 64, 64, 3)\n",
      "0.9364837\n",
      "[Epoch 4/5] [Batch 315/360] [D loss: 0.026129] [G loss: 4.926643] time: 0:03:39.470067\n",
      "(30, 64, 64, 3)\n",
      "0.8804307\n",
      "[Epoch 4/5] [Batch 316/360] [D loss: 0.067149] [G loss: 4.791556] time: 0:03:39.587616\n",
      "(30, 64, 64, 3)\n",
      "0.8755429\n",
      "[Epoch 4/5] [Batch 317/360] [D loss: 0.132222] [G loss: 5.233349] time: 0:03:39.702207\n",
      "(30, 64, 64, 3)\n",
      "0.92725176\n",
      "[Epoch 4/5] [Batch 318/360] [D loss: 0.235811] [G loss: 4.765616] time: 0:03:39.826587\n",
      "(30, 64, 64, 3)\n",
      "0.89491636\n",
      "[Epoch 4/5] [Batch 319/360] [D loss: 0.327983] [G loss: 5.407614] time: 0:03:39.953570\n",
      "(30, 64, 64, 3)\n",
      "0.9062586\n",
      "[Epoch 4/5] [Batch 320/360] [D loss: 0.068632] [G loss: 5.205745] time: 0:03:40.075224\n",
      "(30, 64, 64, 3)\n",
      "0.8723628\n",
      "[Epoch 4/5] [Batch 321/360] [D loss: 0.039272] [G loss: 5.054304] time: 0:03:40.195483\n",
      "(30, 64, 64, 3)\n",
      "0.8938301\n",
      "[Epoch 4/5] [Batch 322/360] [D loss: 0.066458] [G loss: 4.901536] time: 0:03:40.319187\n",
      "(30, 64, 64, 3)\n",
      "0.8510688\n",
      "[Epoch 4/5] [Batch 323/360] [D loss: 0.017835] [G loss: 4.947755] time: 0:03:40.433785\n",
      "(30, 64, 64, 3)\n",
      "0.86208314\n",
      "[Epoch 4/5] [Batch 324/360] [D loss: 0.029383] [G loss: 4.669653] time: 0:03:40.554964\n",
      "(30, 64, 64, 3)\n",
      "0.8953743\n",
      "[Epoch 4/5] [Batch 325/360] [D loss: 0.016220] [G loss: 5.008967] time: 0:03:40.670869\n",
      "(30, 64, 64, 3)\n",
      "0.8926198\n",
      "[Epoch 4/5] [Batch 326/360] [D loss: 0.017079] [G loss: 4.914211] time: 0:03:40.787037\n",
      "(30, 64, 64, 3)\n",
      "0.9131258\n",
      "[Epoch 4/5] [Batch 327/360] [D loss: 0.013933] [G loss: 5.001394] time: 0:03:40.906681\n",
      "(30, 64, 64, 3)\n",
      "0.8895359\n",
      "[Epoch 4/5] [Batch 328/360] [D loss: 0.015753] [G loss: 5.250759] time: 0:03:41.023355\n",
      "(30, 64, 64, 3)\n",
      "0.9228743\n",
      "[Epoch 4/5] [Batch 329/360] [D loss: 0.018154] [G loss: 4.575005] time: 0:03:41.140930\n",
      "(30, 64, 64, 3)\n",
      "0.87713796\n",
      "[Epoch 4/5] [Batch 330/360] [D loss: 0.015626] [G loss: 5.151299] time: 0:03:41.255035\n",
      "(30, 64, 64, 3)\n",
      "0.8822151\n",
      "[Epoch 4/5] [Batch 331/360] [D loss: 0.021170] [G loss: 5.164422] time: 0:03:41.370401\n",
      "(30, 64, 64, 3)\n",
      "0.8976836\n",
      "[Epoch 4/5] [Batch 332/360] [D loss: 0.011440] [G loss: 5.355334] time: 0:03:41.490622\n",
      "(30, 64, 64, 3)\n",
      "0.91655713\n",
      "[Epoch 4/5] [Batch 333/360] [D loss: 0.031006] [G loss: 4.822036] time: 0:03:41.607670\n",
      "(30, 64, 64, 3)\n",
      "0.9240411\n",
      "[Epoch 4/5] [Batch 334/360] [D loss: 0.024924] [G loss: 5.985628] time: 0:03:41.727176\n",
      "(30, 64, 64, 3)\n",
      "0.9027951\n",
      "[Epoch 4/5] [Batch 335/360] [D loss: 0.014894] [G loss: 5.316610] time: 0:03:41.842308\n",
      "(30, 64, 64, 3)\n",
      "0.9299159\n",
      "[Epoch 4/5] [Batch 336/360] [D loss: 0.015623] [G loss: 5.677139] time: 0:03:41.960613\n",
      "(30, 64, 64, 3)\n",
      "0.85671216\n",
      "[Epoch 4/5] [Batch 337/360] [D loss: 0.011455] [G loss: 5.028859] time: 0:03:42.075006\n",
      "(30, 64, 64, 3)\n",
      "0.8859547\n",
      "[Epoch 4/5] [Batch 338/360] [D loss: 0.015078] [G loss: 5.497006] time: 0:03:42.193346\n",
      "(30, 64, 64, 3)\n",
      "0.9019595\n",
      "[Epoch 4/5] [Batch 339/360] [D loss: 0.008906] [G loss: 5.152572] time: 0:03:42.314310\n",
      "(30, 64, 64, 3)\n",
      "0.90729207\n",
      "[Epoch 4/5] [Batch 340/360] [D loss: 0.028362] [G loss: 4.553089] time: 0:03:42.431636\n",
      "(30, 64, 64, 3)\n",
      "0.8937275\n",
      "[Epoch 4/5] [Batch 341/360] [D loss: 0.014416] [G loss: 5.146461] time: 0:03:42.546457\n",
      "(30, 64, 64, 3)\n",
      "0.87374514\n",
      "[Epoch 4/5] [Batch 342/360] [D loss: 0.009745] [G loss: 5.359621] time: 0:03:42.665414\n",
      "(30, 64, 64, 3)\n",
      "0.86873716\n",
      "[Epoch 4/5] [Batch 343/360] [D loss: 0.015664] [G loss: 5.711179] time: 0:03:42.779223\n",
      "(30, 64, 64, 3)\n",
      "0.85854083\n",
      "[Epoch 4/5] [Batch 344/360] [D loss: 0.015029] [G loss: 5.223254] time: 0:03:42.909600\n",
      "(30, 64, 64, 3)\n",
      "0.85060525\n",
      "[Epoch 4/5] [Batch 345/360] [D loss: 0.008407] [G loss: 5.239084] time: 0:03:43.026377\n",
      "(30, 64, 64, 3)\n",
      "0.8921936\n",
      "[Epoch 4/5] [Batch 346/360] [D loss: 0.016789] [G loss: 4.390391] time: 0:03:43.151200\n",
      "(30, 64, 64, 3)\n",
      "0.8923928\n",
      "[Epoch 4/5] [Batch 347/360] [D loss: 0.010334] [G loss: 5.754057] time: 0:03:43.264721\n",
      "(30, 64, 64, 3)\n",
      "0.8938775\n",
      "[Epoch 4/5] [Batch 348/360] [D loss: 0.050267] [G loss: 5.055963] time: 0:03:43.379802\n",
      "(30, 64, 64, 3)\n",
      "0.93765277\n",
      "[Epoch 4/5] [Batch 349/360] [D loss: 0.304273] [G loss: 5.410676] time: 0:03:43.498027\n",
      "(30, 64, 64, 3)\n",
      "0.8834083\n",
      "[Epoch 4/5] [Batch 350/360] [D loss: 0.481149] [G loss: 4.841208] time: 0:03:43.615565\n",
      "(30, 64, 64, 3)\n",
      "0.8822756\n",
      "[Epoch 4/5] [Batch 351/360] [D loss: 0.407210] [G loss: 5.371778] time: 0:03:43.730091\n",
      "(30, 64, 64, 3)\n",
      "0.91566104\n",
      "[Epoch 4/5] [Batch 352/360] [D loss: 0.341049] [G loss: 4.540269] time: 0:03:43.844238\n",
      "(30, 64, 64, 3)\n",
      "0.8979616\n",
      "[Epoch 4/5] [Batch 353/360] [D loss: 0.274863] [G loss: 4.114736] time: 0:03:43.959516\n",
      "(30, 64, 64, 3)\n",
      "0.829642\n",
      "[Epoch 4/5] [Batch 354/360] [D loss: 0.249371] [G loss: 4.762380] time: 0:03:44.079461\n",
      "(30, 64, 64, 3)\n",
      "0.9169388\n",
      "[Epoch 4/5] [Batch 355/360] [D loss: 0.254781] [G loss: 4.853754] time: 0:03:44.198329\n",
      "(30, 64, 64, 3)\n",
      "0.86451244\n",
      "[Epoch 4/5] [Batch 356/360] [D loss: 0.081535] [G loss: 4.014175] time: 0:03:44.314249\n",
      "(30, 64, 64, 3)\n",
      "0.92798805\n",
      "[Epoch 4/5] [Batch 357/360] [D loss: 0.051772] [G loss: 5.152277] time: 0:03:44.429539\n",
      "(30, 64, 64, 3)\n",
      "0.9014904\n",
      "[Epoch 4/5] [Batch 358/360] [D loss: 0.052390] [G loss: 4.372637] time: 0:03:44.559307\n",
      "(30, 64, 64, 3)\n",
      "0.8742676\n",
      "[Epoch 4/5] [Batch 359/360] [D loss: 0.026571] [G loss: 5.158250] time: 0:03:44.674914\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import img_as_ubyte\n",
    "import numpy as np \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "test_first_imgs, test_last_imgs = next(test_batch_generator)\n",
    "\n",
    "for epoch in range(cfg.NUM_EPOCHS):\n",
    "    steps_per_epoch = (nbr_train_data // cfg.BATCH_SIZE) \n",
    "    for batch_i in range(steps_per_epoch):\n",
    "        first_frames, last_frames= next(train_batch_generator)\n",
    "        if first_frames.shape[0] == cfg.BATCH_SIZE: \n",
    "             \n",
    "            # Condition on the first frame and generate the last frame\n",
    "            fake_last_frames = modelObj.generator.predict(first_frames)\n",
    "            #plt.imshow(fake_last_frames[1])\n",
    "            print(fake_last_frames.shape)\n",
    "            #print(tf.keras.backend.mean(fake_last_frames[0]))\n",
    "            print(np.mean(fake_last_frames[0]))\n",
    "\n",
    "            # Train the discriminator with combined loss  \n",
    "            d_loss_real = modelObj.discriminator.train_on_batch([last_frames, first_frames], valid)\n",
    "            d_loss_fake = modelObj.discriminator.train_on_batch([fake_last_frames, first_frames], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    " \n",
    "            # Train the generator\n",
    "            g_loss = modelObj.combined.train_on_batch([last_frames, first_frames], [valid, last_frames])\n",
    "\n",
    "            elapsed_time = datetime.now() - start_time \n",
    "            print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] time: %s\" % (epoch, cfg.NUM_EPOCHS,\n",
    "                                                                                               batch_i,\n",
    "                                                                                               steps_per_epoch,\n",
    "                                                                                               d_loss[0], \n",
    "                                                                                               g_loss[0],\n",
    "                                                                                               elapsed_time))\n",
    "            # run some tests to check how the generated images evolve during training\n",
    "            test_fake_last_imgs = modelObj.generator.predict(test_first_imgs)\n",
    "            test_img_name = output_log_dir + \"/gen_img_epoc_\" + str(epoch) + \".png\"\n",
    "            merged_img = np.vstack((first_frames[0],last_frames[0],fake_last_frames[0]))\n",
    "            imageio.imwrite(test_img_name, img_as_ubyte(merged_img)) #scipy.misc.imsave(test_img_name, merged_img)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) We can test the model with 100 test data which will be saved as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_i in range(100):\n",
    "    test_first_imgs, test_last_imgs = next(test_batch_generator)\n",
    "    test_fake_last_imgs = modelObj.generator.predict(test_first_imgs) \n",
    "\n",
    "    test_img_name = output_log_dir + \"/gen_img_test_\" + str(batch_i) + \".png\"\n",
    "    merged_img = np.vstack((test_first_imgs[0],test_last_imgs[0],test_fake_last_imgs[0]))\n",
    "    imageio.imwrite(test_img_name, img_as_ubyte(merged_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1)\n",
    "Update the network architecture given in  **build_generator**  and  **build_discriminator**  of the class GANModel. Please note that the current image resolution is set to 32x32 (i.e. IMAGE_WIDTH and IMAGE_HEIGHT values) in the file configGAN.py. \n",
    "This way initial experiements can run faster. Once you implement the inital version of the network, please set the resolution values back to 128x128. Experimental results should be provided for this high resolution images.  \n",
    "\n",
    "**Hint:** As a generator model, you can use the segmentation model implemented in lab03. Do not forget to adapt the input and output shapes of the generator model in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2) \n",
    "Use different **optimization** (e.g. ADAM, SGD, etc) and **regularization** (e.g. data augmentation, dropout) methods to increase the network accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
