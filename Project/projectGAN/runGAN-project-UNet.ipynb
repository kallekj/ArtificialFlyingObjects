{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:40px;\"><center>Exercise V:<br> GANs\n",
    "</center></h1>\n",
    "\n",
    "## Short summary\n",
    "In this exercise, we will design a generative network to generate the last rgb image given the first image. These folder has **three files**: \n",
    "- **configGAN.py:** this involves definitions of all parameters and data paths\n",
    "- **utilsGAN.py:** includes utility functions required to grab and visualize data \n",
    "- **runGAN.ipynb:** contains the script to design, train and test the network \n",
    "\n",
    "Make sure that before running this script, you created an environment and **installed all required libraries** such \n",
    "as keras.\n",
    "\n",
    "## The data\n",
    "There exists also a subfolder called **data** which contains the traning, validation, and testing data each has both RGB input images together with the corresponding ground truth images.\n",
    "\n",
    "\n",
    "## The exercises\n",
    "As for the previous lab all exercises are found below.\n",
    "\n",
    "\n",
    "## The different 'Cells'\n",
    "This notebook contains several cells with python code, together with the markdown cells (like this one) with only text. Each of the cells with python code has a \"header\" markdown cell with information about the code. The table below provides a short overview of the code cells. \n",
    "\n",
    "| #  |  CellName | CellType | Comment |\n",
    "| :--- | :-------- | :-------- | :------- |\n",
    "| 1 | Init | Needed | Sets up the environment|\n",
    "| 2 | Ex | Exercise 1| A class definition of a network model  |\n",
    "| 3 | Loading | Needed | Loading parameters and initializing the model |\n",
    "| 4 | Stats | Needed | Show data distribution | \n",
    "| 5 | Data | Needed | Generating the data batches |\n",
    "| 6 | Debug | Needed | Debugging the data |\n",
    "| 7 | Device | Needed | Selecting CPU/GPU |\n",
    "| 8 | Init | Needed | Sets up the timer and other neccessary components |\n",
    "| 9 | Training | Exercise 1-2 | Training the model   |\n",
    "| 10 | Testing | Exercise 1-2| Testing the  method   |  \n",
    "\n",
    "\n",
    "In order for you to start with the exercise you need to run all cells. It is important that you do this in the correct order, starting from the top and continuing with the next cells. Later when you have started to work with the notebook it may be easier to use the command \"Run All\" found in the \"Cell\" dropdown menu.\n",
    "\n",
    "## Writing the report\n",
    "\n",
    "There is no need to provide any report. However, implemented network architecuture and observed experimental results must be presented as a short presentation in the last lecture, May 28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) We first start with importing all required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from configGAN import *\n",
    "cfg = flying_objects_config()\n",
    "# if cfg.GPU >=0:\n",
    "#     print(\"creating network model using gpu \" + str(cfg.GPU))\n",
    "#     os.environ['CUDA_VISIBLE_DEVICES'] = str(cfg.GPU)\n",
    "# elif cfg.GPU >=-1:\n",
    "#     print(\"creating network model using cpu \")  \n",
    "#     os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from utilsGAN import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "import imageio\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "import pprint\n",
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv3D, Conv2D, Conv1D, Convolution2D, Deconvolution2D, Cropping2D, UpSampling2D\n",
    "from keras.layers import Input, Conv2DTranspose, ConvLSTM2D, TimeDistributed, Embedding\n",
    "from keras.layers.convolutional import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import Concatenate, concatenate, Reshape\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from keras.layers import Input, merge\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout, LeakyReLU, ReLU\n",
    "from keras.losses import BinaryCrossentropy\n",
    "import keras.backend as kb\n",
    "from keras.initializers import RandomNormal\n",
    "import pydot\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Here, we have the network model class definition. In this class, the most important functions are **build_generator()** and **build_discriminator()**. As defined in the exercises section, your task is to update the both network architectures defined in these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$|\\frac{n-k+2p}{s}|+1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wasserstein(): \n",
    "    def wasserstein_loss(real_last_frame, fake_last_frame):\n",
    "        return kb.mean(real_last_frame * fake_last_frame)\n",
    "    return wasserstein_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANModel():\n",
    "    def __init__(self, batch_size=32, inputShape=(64, 64, 3), dropout_prob=0.25): \n",
    "        self.batch_size = batch_size\n",
    "        self.inputShape = inputShape\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.num_classes = cfg.NUM_CLASS\n",
    "            \n",
    "        # Calculate the shape of patches\n",
    "        patch = int(self.inputShape[0] / 2**4)\n",
    "        #self.disc_patch = (patch, patch, 1)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        wass = Wasserstein()\n",
    "        \n",
    "        G_lr = 0.0002\n",
    "        D_lr = G_lr/3\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'], optimizer=Adam(D_lr, 0.5), metrics=['accuracy'])\n",
    " \n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # Input images and their conditioning images\n",
    "        first_frame = Input(shape=self.inputShape)\n",
    "        #labels = Input(shape=self.labelShape)\n",
    "        last_frame = Input(shape=self.inputShape)\n",
    "\n",
    "        # By conditioning on the first frame generate a fake version of the last frame\n",
    "        fake_last_frame = self.generator(first_frame)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        # Discriminators determines validity of fake and condition first image pairs\n",
    "        valid = self.discriminator([fake_last_frame, first_frame])\n",
    "\n",
    "        self.combined = Model(inputs=[last_frame, first_frame], outputs=[valid, fake_last_frame])\n",
    "        self.combined.compile(loss=['binary_crossentropy', 'mae'], # mean absolute errors\n",
    "                              loss_weights=[1, 100],\n",
    "                              optimizer=Adam(G_lr, 0.5))\n",
    "\n",
    "        \n",
    "    def conv2d_block_pooling(self, input_tensor, n_filters, kernel_size=3, batchnorm=True, strides=1, moment=0.99, pooling_func=MaxPooling2D):\n",
    "        # first layer\n",
    "        x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), strides=strides, kernel_initializer=\"he_normal\",\n",
    "                   padding=\"same\")(input_tensor)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization(momentum=moment)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        # second layer\n",
    "        x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), strides=strides, kernel_initializer=\"he_normal\",\n",
    "                   padding=\"same\")(x)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization(momentum=moment)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x_pooled = pooling_func((2, 2), strides=(2, 2))(x)\n",
    "        \n",
    "        return x, x_pooled\n",
    "    \n",
    "    def conv2d_block_pooling_new(self, input_tensor, n_filters, kernel_size=3, batchnorm=True, strides=1, moment=0.99):\n",
    "        # first layer\n",
    "        x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), strides=strides, kernel_initializer=\"he_normal\",\n",
    "                   padding=\"same\")(input_tensor)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization(momentum=moment)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        # second layer\n",
    "        x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), strides=strides, kernel_initializer=\"he_normal\",\n",
    "                   padding=\"same\")(x)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization(momentum=moment)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        x_pooled = Conv2D(filters=n_filters, kernel_size=(2,2), strides=(2,2), kernel_initializer=\"he_normal\")(x)\n",
    "        x_pooled = Activation(\"relu\")(x_pooled)\n",
    "        \n",
    "        return x, x_pooled\n",
    "    \n",
    "    \n",
    "    def upSampling2d_block(self, input_tensor, input_tensor_pooled, n_filters, kernel_size=3, batchnorm=True):\n",
    "        upSampling = UpSampling2D((2, 2))(input_tensor)\n",
    "        concat = concatenate([upSampling, input_tensor_pooled], axis=3)\n",
    "        up = Conv2D(n_filters, (kernel_size, kernel_size), padding='same')(concat)\n",
    "        if batchnorm:\n",
    "            up = BatchNormalization()(up)\n",
    "        up = Activation('relu')(up)\n",
    "        up = Conv2D(n_filters, (kernel_size, kernel_size), padding='same')(up)\n",
    "        if batchnorm:\n",
    "            up = BatchNormalization()(up)\n",
    "        up = Activation('relu')(up)\n",
    "        \n",
    "        return up\n",
    "\n",
    "    def build_generator(self):\n",
    "        input_imgs = Input(shape=self.inputShape)\n",
    "        \n",
    "        batchnorm = True\n",
    "        dropout = True\n",
    "        momentum = 0.8\n",
    "        dropout_prob = 0.2\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        \n",
    "        #128\n",
    "        x1, x_pooled1 = self.conv2d_block_pooling(input_imgs, 32, batchnorm=batchnorm, kernel_size=3, pooling_func=AveragePooling2D)\n",
    "        #64\n",
    "        x2, x_pooled2 = self.conv2d_block_pooling(x_pooled1, 64, batchnorm=batchnorm, kernel_size=3, pooling_func=AveragePooling2D)\n",
    "        # 32\n",
    "        x3, x_pooled3 = self.conv2d_block_pooling(x_pooled2, 128, batchnorm=batchnorm, kernel_size=3, pooling_func=AveragePooling2D)\n",
    "        # 16\n",
    "        #x4, x_pooled4 = self.conv2d_block_pooling(x_pooled3, 256, batchnorm=batchnorm, kernel_size=3, pooling_func=AveragePooling2D)\n",
    "        \n",
    "        \n",
    "        #8\n",
    "        mid = Conv2D(256, (3, 3), padding='same')(x_pooled3)\n",
    "        #if batch_norm:\n",
    "        #    mid = BatchNormalization()(mid)\n",
    "        mid = Activation('relu')(mid)\n",
    "        \n",
    "        #mid = Dense(512)(x_pooled3)\n",
    "        #mid = Activation(LeakyReLU(alpha=0.2))(mid)\n",
    "        \n",
    "        \n",
    "        #up1 = self.upSampling2d_block(mid, x4, 256, kernel_size=3, batchnorm=batchnorm)\n",
    "        up2 = self.upSampling2d_block(mid, x3, 128, kernel_size=3, batchnorm=batchnorm)\n",
    "        up3 = self.upSampling2d_block(up2, x2, 64, kernel_size=3, batchnorm=batchnorm)\n",
    "        up4 = self.upSampling2d_block(up3, x1, 32, kernel_size=3, batchnorm=batchnorm)\n",
    "        \n",
    "        \n",
    "        nbr_img_channels = self.inputShape[2]\n",
    "\n",
    "        outputs = Conv2D(nbr_img_channels, (1, 1), activation='tanh')(up4)\n",
    "        \n",
    "        model = Model(inputs=input_imgs, outputs=outputs, name='Generator')\n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        \n",
    "        # weight initialization\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        \n",
    "        last_img = Input(shape=self.inputShape)\n",
    "        first_img = Input(shape=self.inputShape)\n",
    "        combined_imgs = Concatenate(axis=-1)([last_img, first_img])\n",
    "          \n",
    "        kernel_size=(3,3)\n",
    "        \n",
    "        d1 = Conv2D(32, kernel_size=kernel_size, strides=2, padding='same', kernel_initializer=init)(combined_imgs)\n",
    "        #d1 = BatchNormalization()(d1)\n",
    "        d1 = LeakyReLU(alpha=0.2)(d1) \n",
    "        d1 = Dropout(0.2)(d1)\n",
    "        d2 = Conv2D(64, kernel_size=kernel_size, strides=2, padding='same', kernel_initializer=init)(d1)\n",
    "        #d2 = BatchNormalization()(d2)\n",
    "        d2 = LeakyReLU(alpha=0.2)(d2)\n",
    "        d2 = Dropout(0.2)(d2)\n",
    "        d3 = Conv2D(128, kernel_size=kernel_size, strides=2, padding='same', kernel_initializer=init)(d2)\n",
    "        #d3 = BatchNormalization()(d3)\n",
    "        d3 = LeakyReLU(alpha=0.2)(d3)\n",
    "        d3 = Dropout(0.2)(d3)\n",
    "        d4 = Conv2D(256, kernel_size=kernel_size, strides=2, padding='same', kernel_initializer=init)(d3)\n",
    "        #d4 = BatchNormalization()(d4)\n",
    "        d4 = LeakyReLU(alpha=0.2)(d4)\n",
    "        d4 = Dropout(0.2)(d4)\n",
    "\n",
    "        \n",
    "        validity = Conv2D(1, kernel_size=(4,4), padding='same', kernel_initializer=init, activation=\"sigmoid\")(d4)\n",
    "\n",
    "        model = Model([last_img, first_img], validity)\n",
    "        model.summary()\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) We import the network **hyperparameters** and build a simple network by calling the class introduced in the previous step. Please note that to change the hyperparameters, you just need to change the values in the file called **configPredictor.py.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 6)    0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 16, 16, 32)   1760        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 16, 16, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 16, 16, 32)   0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 8, 8, 64)     18496       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 8, 8, 64)     0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8, 8, 64)     0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 4, 4, 128)    73856       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 4, 4, 128)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 4, 4, 128)    0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 2, 2, 256)    295168      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 2, 2, 256)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2, 2, 256)    0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 2, 2, 1)      4097        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 393,377\n",
      "Trainable params: 393,377\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"Generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   896         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 32)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 32)   9248        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 16, 32)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 64)   18496       average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 64)   256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16, 16, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 8, 8, 64)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 128)    73856       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 8, 8, 128)    512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 8, 8, 128)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 8, 128)    147584      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 8, 8, 128)    512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 8, 8, 128)    0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 4, 4, 128)    0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 4, 4, 256)    295168      average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 4, 4, 256)    0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 8, 8, 256)    0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 384)    0           up_sampling2d[0][0]              \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 8, 128)    442496      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 8, 8, 128)    512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 8, 8, 128)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 128)    147584      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 8, 8, 128)    512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 128)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 128)  0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16, 16, 192)  0           up_sampling2d_1[0][0]            \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 64)   110656      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 64)   256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 64)   36928       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 64)   256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 64)   0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 96)   0           up_sampling2d_2[0][0]            \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 32)   27680       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 3)    99          activation_12[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,360,451\n",
      "Trainable params: 1,358,659\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_shape = (cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH, cfg.IMAGE_CHANNEL)\n",
    "modelObj = GANModel(batch_size=cfg.BATCH_SIZE, inputShape=image_shape,\n",
    "                                 dropout_prob=cfg.DROPOUT_PROB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) We call the utility function **show_statistics** to display the data distribution. This is just for debugging purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "##################### Training Data Statistics #####################\n",
      "######################################################################\n",
      "total image number \t 10817\n",
      "total class number \t 3\n",
      "class square \t 3488 images\n",
      "class circular \t 3626 images\n",
      "class triangle \t 3703 images\n",
      "######################################################################\n",
      "\n",
      "######################################################################\n",
      "##################### Validation Data Statistics #####################\n",
      "######################################################################\n",
      "total image number \t 2241\n",
      "total class number \t 3\n",
      "class circular \t 713 images\n",
      "class square \t 783 images\n",
      "class triangle \t 745 images\n",
      "######################################################################\n",
      "\n",
      "######################################################################\n",
      "##################### Testing Data Statistics #####################\n",
      "######################################################################\n",
      "total image number \t 2220\n",
      "total class number \t 3\n",
      "class circular \t 722 images\n",
      "class square \t 765 images\n",
      "class triangle \t 733 images\n",
      "######################################################################\n"
     ]
    }
   ],
   "source": [
    "#### show how the data looks like\n",
    "show_statistics(cfg.training_data_dir, fineGrained=False, title=\" Training Data Statistics \")\n",
    "show_statistics(cfg.validation_data_dir, fineGrained=False, title=\" Validation Data Statistics \")\n",
    "show_statistics(cfg.testing_data_dir, fineGrained=False, title=\" Testing Data Statistics \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is being augmented!\n"
     ]
    }
   ],
   "source": [
    "# setup data\n",
    "if cfg.DATA_AUGMENTATION:\n",
    "    print(\"Data is being augmented!\")\n",
    "    aug_parameters = ImageDataGenerator(\n",
    "        # zoom_range=0.2, # randomly zoom into images\n",
    "        # rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        #width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        #height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=True)  # randomly flip images\n",
    "else:\n",
    "    print(\"Data will not be augmented!\")\n",
    "    aug_parameters = ImageDataGenerator(\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) We now create batch generators to get small batches from the entire dataset. There is no need to change these functions as they already return **normalized inputs as batches**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batch generators are created!\n"
     ]
    }
   ],
   "source": [
    "nbr_train_data = get_dataset_size(cfg.training_data_dir)\n",
    "nbr_valid_data = get_dataset_size(cfg.validation_data_dir)\n",
    "nbr_test_data = get_dataset_size(cfg.testing_data_dir)\n",
    "train_batch_generator = generate_lastframepredictor_batches(cfg.training_data_dir, image_shape, cfg.BATCH_SIZE)\n",
    "valid_batch_generator = generate_lastframepredictor_batches(cfg.validation_data_dir, image_shape, cfg.BATCH_SIZE)\n",
    "test_batch_generator = generate_lastframepredictor_batches(cfg.testing_data_dir, image_shape, cfg.BATCH_SIZE)\n",
    "aug_train_batch_generator = generate_augmented_lastframe_batches(train_batch_generator, aug_parameters)\n",
    "aug_valid_batch_generator = generate_augmented_lastframe_batches(valid_batch_generator, aug_parameters)\n",
    "print(\"Data batch generators are created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) We can visualize how the data looks like for debugging purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (30, 32, 32, 3) float32 -1.0 1.0\n",
      "train_y (30, 32, 32, 3) float32 0.0 255.0\n",
      "{'BATCH_SIZE': 30,\n",
      " 'DATA_AUGMENTATION': True,\n",
      " 'DEBUG_MODE': True,\n",
      " 'DROPOUT_PROB': 0.5,\n",
      " 'GPU': 0,\n",
      " 'IMAGE_CHANNEL': 3,\n",
      " 'IMAGE_HEIGHT': 32,\n",
      " 'IMAGE_WIDTH': 32,\n",
      " 'LEARNING_RATE': 0.0002,\n",
      " 'LR_DECAY_FACTOR': 0.1,\n",
      " 'NUM_CLASS': 3,\n",
      " 'NUM_EPOCHS': 20,\n",
      " 'PRINT_EVERY': 20,\n",
      " 'SAVE_EVERY': 1,\n",
      " 'SEQUENCE_LENGTH': 10,\n",
      " 'fineGrained': False,\n",
      " 'testing_data_dir': '../../data/FlyingObjectDataset_10K/testing',\n",
      " 'training_data_dir': '../../data/FlyingObjectDataset_10K/training',\n",
      " 'validation_data_dir': '../../data/FlyingObjectDataset_10K/validation'}\n"
     ]
    }
   ],
   "source": [
    "if cfg.DEBUG_MODE:\n",
    "    if cfg.DATA_AUGMENTATION:\n",
    "        t_x, t_y = next(aug_train_batch_generator)\n",
    "    else:\n",
    "        t_x, t_y, _ = next(train_batch_generator)\n",
    "    print('train_x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\n",
    "    print('train_y', t_y.shape, t_y.dtype, t_y.min(), t_y.max()) \n",
    "    #plot_sample_lastframepredictor_data_with_groundtruth(t_x, t_y, t_y)\n",
    "    pprint.pprint (cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Start timer and init matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start, end):\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    if(minutes + hours == 0):\n",
    "        return \"{:05.2f}s\".format(seconds)\n",
    "    elif(minutes > 0 and hours == 0):\n",
    "        return \"{:0>2}:{:05.2f}\".format(int(minutes),seconds)\n",
    "    else:\n",
    "        return \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial loss ground truths\n",
    "valid = np.ones((cfg.BATCH_SIZE,) + modelObj.disc_patch)\n",
    "fake = np.zeros((cfg.BATCH_SIZE,) + modelObj.disc_patch)\n",
    "# log file\n",
    "output_log_dir = \"./logs/{}_retrained_with_40ep\".format(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "if not os.path.exists(output_log_dir):\n",
    "    os.makedirs(output_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) We can now feed the training and validation data to the network. This will train the network for **some epochs**. Note that the epoch number is also predefined in the file called **configGAN.py.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                             | 0/360 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9ec5aa5784db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA_AUGMENTATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mfirst_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_train_batch_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mfirst_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import img_as_ubyte\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "test_first_imgs, test_last_imgs, _ = next(test_batch_generator)\n",
    "    \n",
    "test_fake_last_imgs_per_epoch = []\n",
    "\n",
    "d_loss_data = []\n",
    "d_acc_real_data = []\n",
    "d_acc_fake_data = []\n",
    "d_acc_data = []\n",
    "g_loss_data = []\n",
    "start_time = time.time()\n",
    "for epoch in range(cfg.NUM_EPOCHS):\n",
    "    steps_per_epoch = (nbr_train_data // cfg.BATCH_SIZE) \n",
    "    with tqdm(total=steps_per_epoch,ncols=170) as pbar:\n",
    "        d_loss_batch = []\n",
    "        d_acc_real_batch = []\n",
    "        d_acc_fake_batch = []\n",
    "        d_acc_batch = []\n",
    "        g_loss_batch = []\n",
    "        for batch_i in range(steps_per_epoch):\n",
    "            if cfg.DATA_AUGMENTATION:\n",
    "                first_frames, last_frames = next(aug_train_batch_generator)\n",
    "            else:\n",
    "                first_frames, last_frames, _ = next(train_batch_generator)\n",
    "\n",
    "            if first_frames.shape[0] == cfg.BATCH_SIZE: \n",
    "\n",
    "                # Condition on the first frame and generate the last frame\n",
    "                fake_last_frames = modelObj.generator.predict(first_frames)\n",
    "                \n",
    "                # Train the discriminator with combined loss  \n",
    "                d_loss_real = modelObj.discriminator.train_on_batch([last_frames, first_frames], valid)\n",
    "                d_loss_fake = modelObj.discriminator.train_on_batch([fake_last_frames, first_frames], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                d_loss_batch.append(d_loss[0])\n",
    "                d_acc_real_batch.append(d_loss_real[1])\n",
    "                d_acc_fake_batch.append(d_loss_fake[1])\n",
    "                d_acc_batch.append(d_loss[1])\n",
    "                \n",
    "                # Train the generator\n",
    "                g_loss = modelObj.combined.train_on_batch([last_frames, first_frames], [valid, last_frames])\n",
    "                g_loss_batch.append(g_loss[0])\n",
    "                \n",
    "                elapsed_time = timer(start_time, time.time())\n",
    "                \n",
    "                pbar.set_description(\"[Epoch %d/%d] [D loss: %.3f] [D acc real: %.2f] [D acc fake: %.2f] [D acc: %.2f] [G loss: %.3f] time: %s\" % (epoch+1, cfg.NUM_EPOCHS,\n",
    "                                                                                                   d_loss[0],\n",
    "                                                                                                   d_loss_real[1],\n",
    "                                                                                                   d_loss_fake[1],\n",
    "                                                                                                   d_loss[1],\n",
    "                                                                                                   g_loss[0],\n",
    "                                                                                                   elapsed_time))\n",
    "                pbar.update()\n",
    "        \n",
    "        \n",
    "    d_loss_data.append(np.array(d_loss_batch).mean())\n",
    "    g_loss_data.append(np.array(g_loss_batch).mean())\n",
    "    d_acc_real_data.append(np.array(d_acc_real_batch).mean())\n",
    "    d_acc_fake_data.append(np.array(d_acc_fake_batch).mean())\n",
    "    d_acc_data.append(np.array(d_acc_batch).mean())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # run some tests to check how the generated images evolve during training\n",
    "    fake_last_frames = modelObj.generator.predict(test_first_imgs)\n",
    "    merged_first_img = np.vstack((test_first_imgs[0], test_first_imgs[1], test_first_imgs[2]))\n",
    "    merged_last_img = np.vstack((test_last_imgs[0], test_last_imgs[1], test_last_imgs[2]))\n",
    "    merged_fake_last_img = np.vstack((fake_last_frames[0], fake_last_frames[1], fake_last_frames[2]))\n",
    "    test_fake_last_imgs_per_epoch.append((merged_first_img, merged_last_img, merged_fake_last_img))\n",
    "\n",
    "    #test_img_name = output_log_dir + \"/gen_img_epoc_\" + str(epoch) + \".png\"\n",
    "    #merged_img = np.vstack((first_frames[0],last_frames[0],fake_last_frames[0]))\n",
    "    #imageio.imwrite(test_img_name, img_as_ubyte(merged_img)) #scipy.misc.imsave(test_img_name, merged_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the images generated each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fd540233cb1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#kwargs_write = {'fps':1.0, 'quantizer':'nq'}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_log_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/128x128_bxe-mae-loss-weight-1-10_disc-act-sigmoid-BN_aug_4L-gen_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y%m%d-%H%M%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".gif\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrender_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_fake_last_imgs_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_log_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/generator_model_plot.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_log_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/discriminator_model_plot.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-fd540233cb1d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#kwargs_write = {'fps':1.0, 'quantizer':'nq'}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_log_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/128x128_bxe-mae-loss-weight-1-10_disc-act-sigmoid-BN_aug_4L-gen_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y%m%d-%H%M%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".gif\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrender_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_fake_last_imgs_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_log_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/generator_model_plot.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_log_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/discriminator_model_plot.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-fd540233cb1d>\u001b[0m in \u001b[0;36mrender_images\u001b[0;34m(test_fake_last_imgs_per_epoch, epoch)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mgenerated_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_fake_last_imgs_per_epoch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             ax.text(0.5*(left+right), 0.5*(bottom+top), \"Epoch %d\" % epoch,\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def render_images(test_fake_last_imgs_per_epoch, epoch):\n",
    "    rows = cfg.NUM_EPOCHS\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(8, rows*0.5))\n",
    "\n",
    "    # build a rectangle in axes coords\n",
    "    left, width = .25, .5\n",
    "    bottom, height = .25, .5\n",
    "    right = left + width\n",
    "    top = bottom + height\n",
    "    titles = [\"First Img\", \"Last Img\", \"Fake Last Img\"]\n",
    "\n",
    "    for i, ax in enumerate(axs):\n",
    "        generated_images = test_fake_last_imgs_per_epoch[epoch]\n",
    "        if(i == 0):\n",
    "            ax.text(0.5*(left+right), 0.5*(bottom+top), \"Epoch %d\" % epoch,\n",
    "            horizontalalignment='center',\n",
    "            verticalalignment='center',\n",
    "            fontsize=15)\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.imshow((generated_images[i-1]+1)/2.0)\n",
    "            ax.set_title(titles[i-1])\n",
    "            ax.axis('off')\n",
    "\n",
    "    fig.canvas.draw()       # draw the canvas, cache the renderer\n",
    "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "    return image\n",
    "\n",
    "#kwargs_write = {'fps':1.0, 'quantizer':'nq'}\n",
    "file_path = output_log_dir + '/128x128_bxe-mae-loss-weight-1-10_disc-act-sigmoid-BN_aug_4L-gen_{}'.format(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "imageio.mimsave(file_path + \".gif\", [render_images(test_fake_last_imgs_per_epoch, epoch) for epoch in range(cfg.NUM_EPOCHS)], fps=2)\n",
    "tf.keras.utils.plot_model(modelObj.generator, to_file=output_log_dir+\"/generator_model_plot.png\", show_shapes=True, dpi=64)\n",
    "tf.keras.utils.plot_model(modelObj.discriminator, to_file=output_log_dir + \"/discriminator_model_plot.png\", show_shapes=True, dpi=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) We can test the model with $n$ test data which will be saved as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 5\n",
    "fig, axs = plt.subplots(nrows=rows, ncols=4, figsize=(8, 2*rows))\n",
    "\n",
    "# build a rectangle in axes coords\n",
    "left, width = .25, .5\n",
    "bottom, height = .25, .5\n",
    "right = left + width\n",
    "top = bottom + height\n",
    "\n",
    "validation = False\n",
    "\n",
    "if validation == True:\n",
    "    # ============ VALIDATION ====================\n",
    "    with tqdm(total=steps_per_epoch,ncols=170) as pbar2:\n",
    "\n",
    "        val_d_loss_real = []\n",
    "        val_d_loss_fake = []\n",
    "        val_d_loss = []\n",
    "        val_d_acc_real = []\n",
    "        val_d_acc_fake = []\n",
    "        val_d_acc = []\n",
    "        val_g_loss = []\n",
    "\n",
    "\n",
    "        for batch_i in range(steps_per_epoch):\n",
    "            test_first_imgs, test_last_imgs, _ = next(test_batch_generator)\n",
    "\n",
    "            if test_first_imgs.shape[0] == cfg.BATCH_SIZE: \n",
    "                fake_last_frames = modelObj.generator.predict(test_first_imgs)\n",
    "                d_loss_real = modelObj.discriminator.evaluate([test_last_imgs, test_first_imgs], valid, verbose=0)\n",
    "                d_loss_fake = modelObj.discriminator.evaluate([fake_last_frames, test_first_imgs], fake, verbose=0)\n",
    "\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                val_d_loss.append(d_loss[0])\n",
    "                val_d_acc_real.append(d_loss_real[1])\n",
    "                val_d_acc_fake.append(d_loss_fake[1])\n",
    "                val_d_acc.append(d_loss[1])\n",
    "\n",
    "                g_loss = modelObj.combined.evaluate([test_last_imgs, test_first_imgs], [valid, test_last_imgs], verbose=0)\n",
    "                val_g_loss.append(g_loss[0])\n",
    "\n",
    "                pbar2.set_description(\"Validation: [Epoch %d/%d] [D loss: %.3f] [D acc real: %.2f] [D acc fake: %.2f] [D acc: %.2f] [G loss: %.3f]\" % (epoch+1, cfg.NUM_EPOCHS,\n",
    "                                                                                                   np.sum(val_d_loss)/len(val_d_loss),\n",
    "                                                                                                   np.sum(val_d_acc_real)/len(val_d_acc_real),\n",
    "                                                                                                   np.sum(val_d_acc_fake)/len(val_d_acc_fake),\n",
    "                                                                                                   np.sum(val_d_loss)/len(val_d_loss),\n",
    "                                                                                                   np.sum(val_g_loss)/len(val_g_loss)))\n",
    "                pbar2.update()\n",
    "\n",
    "\n",
    "\n",
    "for ax, batch_i in zip(axs, range(rows)):\n",
    "    test_first_imgs, test_last_imgs, _ = next(test_batch_generator)\n",
    "    #test_img_labels = label_encoder(test_img_labels)\n",
    "    test_fake_last_imgs = modelObj.generator.predict(test_first_imgs) \n",
    "\n",
    "    test_img_name = output_log_dir + \"/gen_img_test_\" + str(batch_i) + \".png\"\n",
    "    #merged_img = np.vstack((test_first_imgs[0],test_last_imgs[0],test_fake_last_imgs[0]))\n",
    "    #imageio.imwrite(test_img_name, img_as_ubyte(merged_img))\n",
    "    ax[0].text(0.5*(left+right), 0.5*(bottom+top), batch_i,\n",
    "    horizontalalignment='center',\n",
    "    verticalalignment='center',\n",
    "    fontsize=15)\n",
    "    ax[0].axis('off')\n",
    "    for i, img in zip(range(1,4), [[\"First Img\", test_first_imgs[0]], [\"Last Img\", test_last_imgs[0]], [\"Fake Last Img\", test_fake_last_imgs[0]]]):\n",
    "        ax[i].imshow((img[1]+1)/2.0)\n",
    "        if(batch_i == 0):\n",
    "            ax[i].set_title(img[0])\n",
    "        ax[i].axis('auto')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(file_path + \"test_images.png\", format=\"png\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n",
    "\n",
    "epochs = np.linspace(1, cfg.NUM_EPOCHS, cfg.NUM_EPOCHS)\n",
    "\n",
    "data_dict = {\"Loss\":{\"Discriminator Mean Loss\":d_loss_data, \"Generator Mean Loss\":g_loss_data}, \"Accuracy\":{\"Discriminator Real Accuracy\":d_acc_real_data, \"Discriminator Fake Accuracy\":d_acc_fake_data, \"Discriminator Accuracy\":d_acc_data}}\n",
    "\n",
    "for i, (ax, prim_key) in enumerate(zip(axs, data_dict)):\n",
    "    if(i == 0):    \n",
    "        # 300 represents number of points to make between T.min and T.max\n",
    "        xnew = np.linspace(1, cfg.NUM_EPOCHS, 100)\n",
    "        for key in data_dict[prim_key]:\n",
    "            spl = make_interp_spline(epochs, data_dict[prim_key][key], k=3)  # type: BSpline\n",
    "            data_smooth = spl(xnew)\n",
    "            ax.plot(xnew, data_smooth, label=key)\n",
    "            ax.legend()\n",
    "                                                          \n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Mean Loss')\n",
    "    else:\n",
    "        # 300 represents number of points to make between T.min and T.max\n",
    "        xnew = np.linspace(1, cfg.NUM_EPOCHS, 100)\n",
    "        for key in data_dict[prim_key]:\n",
    "            spl = make_interp_spline(epochs, data_dict[prim_key][key], k=3)  # type: BSpline\n",
    "            data_smooth = spl(xnew)\n",
    "            ax.plot(xnew, data_smooth, label=key)\n",
    "            ax.legend()\n",
    "                                                          \n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Mean Accuracy')\n",
    "        \n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(file_path + \"_plots.png\", format=\"png\")\n",
    "plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1)\n",
    "Update the network architecture given in  **build_generator**  and  **build_discriminator**  of the class GANModel. Please note that the current image resolution is set to 32x32 (i.e. IMAGE_WIDTH and IMAGE_HEIGHT values) in the file configGAN.py. \n",
    "This way initial experiements can run faster. Once you implement the inital version of the network, please set the resolution values back to 128x128. Experimental results should be provided for this high resolution images.  \n",
    "\n",
    "**Hint:** As a generator model, you can use the segmentation model implemented in lab03. Do not forget to adapt the input and output shapes of the generator model in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2) \n",
    "Use different **optimization** (e.g. ADAM, SGD, etc) and **regularization** (e.g. data augmentation, dropout) methods to increase the network accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "### Name\n",
    "Karl-Johan Djervbrant\n",
    "\n",
    "## Introduction\n",
    "\n",
    "## Methodology\n",
    "To get a baseline on how the new network performs, a test with no modification to the model provided is made over 10 epochs. Thereafter, a model from the CNN lab is adopted to replace the generator to start development.  \n",
    "## Results\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
