{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from skimage import io\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model\n",
    "\n",
    "Here, we have the network model class definition. As defined in the exercises section, your task is to update the network architecture defined in this class such that the network will return the highest accuracy for the given training, validation, and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDNNModel(nn.Module):\n",
    "    '''\n",
    "    Classifier DNN Class\n",
    "    Values:\n",
    "        input_dim: number of channels in the images\n",
    "        hidden_dim: inner dimension for conv layers\n",
    "        linear_dim: dimenstion for the linear layer\n",
    "        n_classes: number of output classes\n",
    "    '''\n",
    "    def __init__(self, input_dim=(128, 128, 3), hidden_dim=25, linear_dim=2048, n_classes=3, **kwargs):\n",
    "        super(ClassifierDNNModel, self).__init__()\n",
    "\n",
    "        (h, w, c) = input_dim\n",
    "        \n",
    "        \n",
    "\n",
    "        # Build our model\n",
    "        self.model = nn.Sequential(\n",
    "            # input is           (c) x h      x w\n",
    "            # output is (hidden_dim) x h / 2  x w / 2\n",
    "            self.make_conv_block(c, hidden_dim, 0),\n",
    "            \n",
    "            # input is  (hidden_dim)     x h / 2  x w / 2\n",
    "            # output is (hidden_dim x 2) x h / 4  x w / 4\n",
    "            self.make_conv_block(hidden_dim, hidden_dim * 2, 0.2),\n",
    "            \n",
    "            # input is  (hidden_dim x 2)     x h / 4  x w / 4\n",
    "            # output is (hidden_dim x 4) x h / 8  x w / 8\n",
    "            self.make_conv_block(hidden_dim * 2, hidden_dim * 4, 0.3),\n",
    "            \n",
    "            # input is  (hidden_dim x 2)     x h / 4  x w / 4\n",
    "            # output is (hidden_dim x 4) x h / 8  x w / 8\n",
    "            #self.make_conv_block(hidden_dim * 4, hidden_dim * 4),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            # dimensions after flatten layer is the image size * dimensions.\n",
    "            self.make_linear_block(hidden_dim * 4 * h // 8 * w // 8, linear_dim),\n",
    "            self.make_linear_block(linear_dim, n_classes, last_layer=True),\n",
    "        )\n",
    "    \n",
    "    def make_conv_block(self, input_channels, output_channels, dropout):\n",
    "        '''\n",
    "        Returns a sequence corresponding to the convolutional layers in our DNN model.\n",
    "        Parameters:\n",
    "            input_channels: number of input channels to this block\n",
    "            output_channels: number of output channels from this block\n",
    "            batch_norm: if batch normalization should be used or not.\n",
    "        '''\n",
    "        \n",
    "        if dropout <= 0:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2)\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.MaxPool2d(2)\n",
    "            )\n",
    "\n",
    "    def make_linear_block(self, input_channels, output_channels, last_layer=False):\n",
    "        '''\n",
    "        Returns a sequence corresponding to the linear layers in our DNN model.\n",
    "        Parameters:\n",
    "            input_channels: number of input channels to this block\n",
    "            output_channels: number of output channels from this block\n",
    "            batch_norm: if batch normalization should be used or not.\n",
    "            last_layer: if this is the final layers in our model\n",
    "        '''\n",
    "        if last_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(input_channels, output_channels, bias=False),\n",
    "                nn.Softmax(dim=1)\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(input_channels, output_channels, bias=False),\n",
    "                nn.BatchNorm1d(output_channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass: Given an image, \n",
    "        returns predicted class.\n",
    "        Parameters:\n",
    "            image: an image tensor with dimension (input_dim)\n",
    "        '''\n",
    "        return self.model(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup network hyperparameters\n",
    "\n",
    "We import the network hyperparameters and build a simple cnn by calling the class introduced in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam = {\n",
    "    'input_dim': (128, 128, 3),\n",
    "    'hidden_dim': 25,\n",
    "    'linear_dim': 2048,\n",
    "    'n_classes': 3,\n",
    "    'device': 'cuda',\n",
    "    'lr': 0.001,\n",
    "    'n_epochs': 10,\n",
    "}\n",
    "\n",
    "model = ClassifierDNNModel(**hparam).to(hparam['device'])\n",
    "model_opt = torch.optim.Adam(model.parameters(), lr=hparam['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL\n",
    "\n",
    "Extract, transform load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtificialFlyingObjectsDataset(Dataset):\n",
    "    '''\n",
    "    Artificial flying objects dataset.\n",
    "    Values:\n",
    "        root_dir: root directory where images can be found.\n",
    "        split: If it should return training, validation, testing split\n",
    "        fineGrained: If we should use the fine grained classes\n",
    "        transforms: List of optional tensorvision transforms.\n",
    "    '''\n",
    "    def __init__(self, root_dir='../data/FlyingObjectDataset_10K', split='training', fineGrained=False, transforms=None):\n",
    "        import glob\n",
    "        self.split = split\n",
    "        self.datadir = root_dir\n",
    "        filenames = glob.glob(f'{root_dir}/{split}/image/*.png')\n",
    "        categories = []\n",
    "        for filename in filenames:\n",
    "            seg = filename.split('_')\n",
    "\n",
    "            if fineGrained:\n",
    "                categories.append(seg[2] + \"_\" + seg[3])\n",
    "            else:\n",
    "                categories.append(seg[2])\n",
    "\n",
    "        self.labels = pd.get_dummies(categories)\n",
    "        self.filenames = list(filenames)\n",
    "        \n",
    "        if transforms:\n",
    "            self.transforms = Compose(transforms.append(ToTensor()))\n",
    "        else:\n",
    "            self.transforms = Compose([ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Length of dataset\n",
    "        '''\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Get individual item.\n",
    "        '''\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = self.filenames[idx]\n",
    "        X = io.imread(img_name)\n",
    "        y = torch.tensor(self.labels.iloc[idx, 0], dtype=torch.long)\n",
    "\n",
    "        X = self.transforms(X)\n",
    "        X = X.clone().detach()\n",
    "        return (X, y)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        head = \"Dataset \" + self.__class__.__name__ + \" : \" + self.split\n",
    "        body = [f\"Number of images: {self.__len__()}\"]\n",
    "        body.append(f\"Categories: {len(set(self.labels))}\")\n",
    "        body.append(f\"Root location: {self.datadir}\")\n",
    "        if hasattr(self, \"transforms\") and self.transforms is not None:\n",
    "            body += [f'Transforms: ' + repr(self.transforms)]\n",
    "        lines = [head] + [\" \" * 4 + line for line in body]\n",
    "        return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add optional augmentation as a list of transforms\n",
    "# See: https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "transform = None\n",
    "\n",
    "train_ds = ArtificialFlyingObjectsDataset(transforms=transform)\n",
    "val_ds = ArtificialFlyingObjectsDataset(split='validation', transforms=transform)\n",
    "test_ds = ArtificialFlyingObjectsDataset(split='testing', transforms=transform)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=True)\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    drop_last=True)\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print statistics\n",
    "\n",
    "Print some statistics about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ArtificialFlyingObjectsDataset : training\n",
      "    Number of images: 10817\n",
      "    Categories: 3\n",
      "    Root location: ../data/FlyingObjectDataset_10K\n",
      "    Transforms: Compose(\n",
      "    ToTensor()\n",
      ")\n",
      "Dataset ArtificialFlyingObjectsDataset : validation\n",
      "    Number of images: 2241\n",
      "    Categories: 3\n",
      "    Root location: ../data/FlyingObjectDataset_10K\n",
      "    Transforms: Compose(\n",
      "    ToTensor()\n",
      ")\n",
      "Dataset ArtificialFlyingObjectsDataset : testing\n",
      "    Number of images: 2220\n",
      "    Categories: 3\n",
      "    Root location: ../data/FlyingObjectDataset_10K\n",
      "    Transforms: Compose(\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(train_ds)\n",
    "print(val_ds)\n",
    "print(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Setup training hparams and execute training of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c34773747644f8fad9ab324be01b7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=338.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy 0.66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e993c8f2d064dadb0adabbf70b16d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=338.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy 0.56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1e303f9c7c48688acd011402b3eca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=338.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy 0.59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d3f56b29d949b8a85d46be31760dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=338.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy 0.59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1843bd8fb1ee449b906d7c0394b4dbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=338.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy 0.44\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bef1ddc8b6e43a4af60cacd2e309feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=338.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy 0.59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80e7a021407447788adc0570755cd60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=338.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy 0.55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a714fd84bea44e5b9234271fbff998f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=338.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy 0.61\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e8bdbc782a40359871ccdb3b7d9e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=338.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy 0.57\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2565161e60d4c789d1330cd89d1be4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train'), FloatProgress(value=0.0, max=338.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy 0.59\n",
      "Test accuracy 0.51\n"
     ]
    }
   ],
   "source": [
    "n_epochs = hparam['n_epochs']\n",
    "cur_step = 0\n",
    "mean_loss = 0\n",
    "mean_accuracy = 0\n",
    "device = hparam['device']\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches\n",
    "\n",
    "    model.train()\n",
    "    for x, y in tqdm(train_dl, desc='Train'):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        ## Update model ##\n",
    "        model_opt.zero_grad()\n",
    "        yh = model(x)\n",
    "        loss = loss_fn(yh, y)\n",
    "        loss.backward()\n",
    "        model_opt.step()\n",
    "\n",
    "    # Do verification each epoch\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_dl:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            yh = model(x)\n",
    "            _, predicted = torch.max(yh.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    print(f'Validation accuracy {correct / total:.2f}')\n",
    "\n",
    "# Calculate final test accuracy\n",
    "model.eval()\n",
    "total = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_dl:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        yh = model(x)\n",
    "        _, predicted = torch.max(yh.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "print(f'Test accuracy {correct / total:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please do all exercises desribed below. Note that all your source code as well as the log folders must be provided as final results **before April 05, 2019.** \n",
    "\n",
    "\n",
    "#### Exercise 1)\n",
    "Update the network architecture given in the function **create_model** of the class ClassifierDNNModel. \n",
    "\n",
    "**Hint:** You can add more convolution, max pooling layers etc. Batch normalization and dropout are other options to be considered. You can also try applying different activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2) \n",
    "Use different **optimization** (e.g. ADAM, SGD, etc) and **regularization** (e.g. data augmentation, dropout) methods to increase the network accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3)\n",
    "In the file **configClassifier.py**, there is a flag named as **cfg.fineGrained** which is set to **False**. This flag defines the classification granularity level. In the default setting, i.e. when it is **False**, there exist 3 class types: **Square**, **Triangle**, and **Circle**. In case of switching this flag to **True**, the class number goes to 15. Repeat previous exercises 1) and 2) after setting this flag to **True** and provide results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint:\n",
    "All network resposes are stored in a **log folder** which is automatically created. To visualize these responses, we can use the tensorboard as follows:\n",
    "- First make sure that there is a new folder created with **a date and time stamp** under folder **logs**\n",
    "- Next, open a terminal and type \n",
    "    > tensorboard --logdir=./logs\n",
    "- Finally, open a web browser and type \n",
    "    > http://localhost:6006\n",
    "- You can have an overview of all accuracies on the tensorboard. For more information about tensorboard, please see https://www.tensorflow.org/guide/summaries_and_tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The report!\n",
    "\n",
    "\n",
    "### Name\n",
    "\n",
    "### Introduction\n",
    "\n",
    "### Answers to questions\n",
    "\n",
    "### Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
